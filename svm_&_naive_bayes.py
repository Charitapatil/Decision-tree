# -*- coding: utf-8 -*-
"""SVM & Naive bayes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FJkk_Fy_po_Hiw-NKEg_ADAXWdwzLkpP
"""

Theoretical

1. What is a Support Vector Machine (SVM)?
A Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks. It is particularly powerful for binary classification problems.

How SVM Works:
Finding the Optimal Hyperplane

SVM aims to find the best decision boundary (hyperplane) that separates different classes in the feature space.
The best hyperplane maximizes the margin, which is the distance between the closest data points (support vectors) of each class.
Support Vectors

These are the data points that lie closest to the hyperplane.
They are crucial in defining the decision boundary.
Kernel Trick

If the data is not linearly separable, SVM can use a kernel function (like polynomial or radial basis function (RBF)) to transform the data into a higher-dimensional space where it becomes linearly separable.
Mathematical Representation
Given training data points
(
𝑥
𝑖
,
𝑦
𝑖
)
(x
i
​
 ,y
i
​
 ), where
𝑦
𝑖
y
i
​
  is the class label (1 or -1), SVM finds a hyperplane:

𝑤
⋅
𝑥
+
𝑏
=
0
w⋅x+b=0
where:

𝑤
w is the weight vector,
𝑥
x is the feature vector,
𝑏
b is the bias term.
The margin is maximized by solving an optimization problem with constraints.

The difference between Hard Margin and Soft Margin SVM lies in how strictly the SVM enforces class separation.

1. Hard Margin SVM
It assumes that the data is perfectly linearly separable (i.e., no overlap between classes).
The goal is to find a hyperplane that strictly separates all points without any misclassification.
Constraint: All data points must be on the correct side of the margin.
Limitation: It is very sensitive to outliers; even one misclassified point can prevent the model from working.
2. Soft Margin SVM
It allows some misclassification by introducing a slack variable (ξ) to handle noisy or overlapping data.
A hyperparameter C controls the trade-off between maximizing the margin and allowing some misclassification:
Large C → Less tolerance for misclassification, leads to a smaller margin.
Small C → More tolerance for misclassification, leads to a larger margin.
Advantage: Works well with real-world data, which often has noise and overlaps.
Key Difference Summary:
Feature	Hard Margin SVM	Soft Margin SVM
Data Assumption	Perfectly separable	Can have overlap/noise
Misclassification	Not allowed	Allowed (controlled by C)
Overfitting Risk	High	Lower
Robustness	Poor with outliers	More robust
Soft Margin SVM is generally preferred in practical scenarios since real-world data is rarely perfectly separable.

Mathematical Intuition Behind SVM
SVM aims to find an optimal hyperplane that separates two classes while maximizing the margin between them. The margin is the distance between the hyperplane and the nearest data points, known as support vectors.

1. The Decision Hyperplane
For a dataset with feature vectors
𝑥
𝑖
∈
𝑅
𝑛
x
i
​
 ∈R
n
  and corresponding labels
𝑦
𝑖
∈
{
−
1
,
1
}
y
i
​
 ∈{−1,1}, a linear classifier can be defined as:

𝑓
(
𝑥
)
=
𝑤
𝑇
𝑥
+
𝑏
f(x)=w
T
 x+b
where:

𝑤
w is the weight vector (normal to the hyperplane).
𝑏
b is the bias term.
𝑥
x is the input feature vector.
The decision boundary is defined by the equation:

𝑤
𝑇
𝑥
+
𝑏
=
0
w
T
 x+b=0
2. Maximizing the Margin
The margin is the distance between the hyperplane and the nearest data points (support vectors). It is given by:

2
∥
𝑤
∥
∥w∥
2
​

SVM maximizes this margin while ensuring correct classification.

3. The Optimization Problem
To maximize the margin, we solve:

min
⁡
𝑤
,
𝑏
1
2
∥
𝑤
∥
2
w,b
min
​

2
1
​
 ∥w∥
2

subject to the constraint:

𝑦
𝑖
(
𝑤
𝑇
𝑥
𝑖
+
𝑏
)
≥
1
,
∀
𝑖
y
i
​
 (w
T
 x
i
​
 +b)≥1,∀i
This ensures that all points are correctly classified and at least at a distance of 1 from the hyperplane.

4. Soft Margin SVM (with Slack Variables)
For non-linearly separable data, we introduce slack variables
𝜉
𝑖
ξ
i
​
  to allow misclassification:

𝑦
𝑖
(
𝑤
𝑇
𝑥
𝑖
+
𝑏
)
≥
1
−
𝜉
𝑖
,
𝜉
𝑖
≥
0
y
i
​
 (w
T
 x
i
​
 +b)≥1−ξ
i
​
 ,ξ
i
​
 ≥0
We modify the objective function to:

min
⁡
𝑤
,
𝑏
1
2
∥
𝑤
∥
2
+
𝐶
∑
𝑖
𝜉
𝑖
w,b
min
​

2
1
​
 ∥w∥
2
 +C
i
∑
​
 ξ
i
​

where C controls the trade-off between margin maximization and misclassification tolerance.

5. Dual Formulation (Kernel Trick for Non-Linear SVM)
Instead of solving in the primal form, we often solve the dual problem using Lagrange multipliers
𝛼
𝑖
α
i
​
 :

max
⁡
𝛼
∑
𝑖
𝛼
𝑖
−
1
2
∑
𝑖
,
𝑗
𝛼
𝑖
𝛼
𝑗
𝑦
𝑖
𝑦
𝑗
𝐾
(
𝑥
𝑖
,
𝑥
𝑗
)
α
max
​

i
∑
​
 α
i
​
 −
2
1
​

i,j
∑
​
 α
i
​
 α
j
​
 y
i
​
 y
j
​
 K(x
i
​
 ,x
j
​
 )
subject to:

∑
𝑖
𝛼
𝑖
𝑦
𝑖
=
0
,
0
≤
𝛼
𝑖
≤
𝐶
i
∑
​
 α
i
​
 y
i
​
 =0,0≤α
i
​
 ≤C
where
𝐾
(
𝑥
𝑖
,
𝑥
𝑗
)
K(x
i
​
 ,x
j
​
 ) is a kernel function that maps data into a higher-dimensional space, allowing SVM to handle non-linearly separable data.

Key Takeaways:
✅ SVM maximizes the margin between classes for better generalization.
✅ Soft margin SVM allows misclassification using slack variables.
✅ Dual formulation & kernel trick help extend SVM to nonlinear classification.

Role of Lagrange Multipliers in SVM
Lagrange multipliers play a crucial role in solving the constrained optimization problem in SVM. They help transform the original problem into a form that is easier to solve, particularly when dealing with non-linear decision boundaries.

1. The Primal Optimization Problem (Hard Margin SVM)
SVM aims to find a hyperplane that maximizes the margin while ensuring correct classification. The optimization problem is:

min
⁡
𝑤
,
𝑏
1
2
∥
𝑤
∥
2
w,b
min
​

2
1
​
 ∥w∥
2

subject to:

𝑦
𝑖
(
𝑤
𝑇
𝑥
𝑖
+
𝑏
)
≥
1
,
∀
𝑖
y
i
​
 (w
T
 x
i
​
 +b)≥1,∀i
This is a constrained optimization problem, which is difficult to solve directly.

2. Introducing Lagrange Multipliers
To handle the constraints, we use Lagrange multipliers
𝛼
𝑖
α
i
​
  (one for each training point) and form the Lagrangian function:

𝐿
(
𝑤
,
𝑏
,
𝛼
)
=
1
2
∥
𝑤
∥
2
−
∑
𝑖
𝛼
𝑖
[
𝑦
𝑖
(
𝑤
𝑇
𝑥
𝑖
+
𝑏
)
−
1
]
L(w,b,α)=
2
1
​
 ∥w∥
2
 −
i
∑
​
 α
i
​
 [y
i
​
 (w
T
 x
i
​
 +b)−1]
where
𝛼
𝑖
≥
0
α
i
​
 ≥0 are the Lagrange multipliers.

3. Converting to the Dual Problem
We take the derivatives of
𝐿
L w.r.t.
𝑤
w and
𝑏
b and set them to zero:

∂
𝐿
∂
𝑤
=
0
⇒
𝑤
=
∑
𝑖
𝛼
𝑖
𝑦
𝑖
𝑥
𝑖
∂w
∂L
​
 =0⇒w=
i
∑
​
 α
i
​
 y
i
​
 x
i
​

∂
𝐿
∂
𝑏
=
0
⇒
∑
𝑖
𝛼
𝑖
𝑦
𝑖
=
0
∂b
∂L
​
 =0⇒
i
∑
​
 α
i
​
 y
i
​
 =0
Substituting these into
𝐿
L, we get the dual form of SVM:

max
⁡
𝛼
∑
𝑖
𝛼
𝑖
−
1
2
∑
𝑖
,
𝑗
𝛼
𝑖
𝛼
𝑗
𝑦
𝑖
𝑦
𝑗
(
𝑥
𝑖
𝑇
𝑥
𝑗
)
α
max
​

i
∑
​
 α
i
​
 −
2
1
​

i,j
∑
​
 α
i
​
 α
j
​
 y
i
​
 y
j
​
 (x
i
T
​
 x
j
​
 )
subject to:

∑
𝑖
𝛼
𝑖
𝑦
𝑖
=
0
,
𝛼
𝑖
≥
0
i
∑
​
 α
i
​
 y
i
​
 =0,α
i
​
 ≥0
4. Importance of Lagrange Multipliers
They transform the problem into the dual form, making it easier to solve, especially when using the kernel trick for non-linear classification.

They determine the support vectors: Only points with
𝛼
𝑖
>
0
α
i
​
 >0 contribute to defining the decision boundary. Other points have
𝛼
𝑖
=
0
α
i
​
 =0 and do not affect the final classifier.

They help control misclassification in the soft-margin SVM, where a penalty term
𝐶
C is introduced:

0
≤
𝛼
𝑖
≤
𝐶
0≤α
i
​
 ≤C
5. Extension to Kernel Trick
In non-linear cases, we replace
𝑥
𝑖
𝑇
𝑥
𝑗
x
i
T
​
 x
j
​
  with a kernel function
𝐾
(
𝑥
𝑖
,
𝑥
𝑗
)
K(x
i
​
 ,x
j
​
 ), allowing SVM to work in a higher-dimensional space without explicitly computing the transformation:

max
⁡
𝛼
∑
𝑖
𝛼
𝑖
−
1
2
∑
𝑖
,
𝑗
𝛼
𝑖
𝛼
𝑗
𝑦
𝑖
𝑦
𝑗
𝐾
(
𝑥
𝑖
,
𝑥
𝑗
)
α
max
​

i
∑
​
 α
i
​
 −
2
1
​

i,j
∑
​
 α
i
​
 α
j
​
 y
i
​
 y
j
​
 K(x
i
​
 ,x
j
​
 )
This makes SVM powerful for non-linear classification.

Support Vectors in SVM
Support Vectors are the data points that lie closest to the decision boundary (hyperplane) and directly influence its position and orientation. These points define the margin of the classifier, and removing them would change the decision boundary.

1. Why Are Support Vectors Important?
They determine the optimal hyperplane: The decision boundary is entirely dependent on these points.
They maximize the margin: SVM tries to position the hyperplane so that the margin (distance between the hyperplane and the closest support vectors) is as large as possible.
Only a few data points influence the decision: Unlike other models like logistic regression, SVM does not rely on all training points—just the support vectors matter.
2. Support Vectors in Hard Margin SVM
In a Hard Margin SVM (where data is perfectly separable), support vectors are the points that exactly touch the margin boundaries. The margin is defined as:

𝑦
𝑖
(
𝑤
𝑇
𝑥
𝑖
+
𝑏
)
=
1
y
i
​
 (w
T
 x
i
​
 +b)=1
Only these points contribute to defining
𝑤
w and
𝑏
b, while other points do not affect the boundary.

3. Support Vectors in Soft Margin SVM
For a Soft Margin SVM (which allows some misclassification), support vectors include:

Points on the margin (
𝑦
𝑖
(
𝑤
𝑇
𝑥
𝑖
+
𝑏
)
=
1
y
i
​
 (w
T
 x
i
​
 +b)=1).
Points inside the margin (
0
<
𝑦
𝑖
(
𝑤
𝑇
𝑥
𝑖
+
𝑏
)
<
1
0<y
i
​
 (w
T
 x
i
​
 +b)<1).
Misclassified points (
𝑦
𝑖
(
𝑤
𝑇
𝑥
𝑖
+
𝑏
)
<
0
y
i
​
 (w
T
 x
i
​
 +b)<0)—though these are penalized using slack variables
𝜉
𝑖
ξ
i
​
 .
The soft margin approach ensures SVM remains robust in cases where classes overlap or have noise.

4. Mathematical Role of Support Vectors
In the dual formulation of SVM, support vectors are the data points with nonzero Lagrange multipliers
𝛼
𝑖
α
i
​
 :

𝛼
𝑖
>
0
⇒
𝑥
𝑖
 is a support vector
α
i
​
 >0⇒x
i
​
  is a support vector
Non-support vectors have
𝛼
𝑖
=
0
α
i
​
 =0, meaning they do not contribute to the final classifier.

The final decision function is given by:

𝑓
(
𝑥
)
=
∑
𝑖
𝛼
𝑖
𝑦
𝑖
𝐾
(
𝑥
𝑖
,
𝑥
)
+
𝑏
f(x)=
i
∑
​
 α
i
​
 y
i
​
 K(x
i
​
 ,x)+b
where only support vectors contribute (since non-support vectors have
𝛼
𝑖
=
0
α
i
​
 =0).

5. Key Takeaways About Support Vectors
✅ They are the most critical data points in SVM—they define the decision boundary.
✅ The model only depends on support vectors, making SVM memory-efficient.
✅ They influence generalization—fewer support vectors often lead to better generalization, while too many can cause overfitting.
✅ They allow the kernel trick, helping SVM classify non-linearly separable data.

Support Vector Classifier (SVC)
A Support Vector Classifier (SVC) is an application of Support Vector Machines (SVM) for classification tasks. It finds an optimal decision boundary (hyperplane) that separates different classes with the largest possible margin while allowing for some misclassification (soft margin).

1. How SVC Works
SVC aims to solve a classification problem by finding a hyperplane:

𝑤
𝑇
𝑥
+
𝑏
=
0
w
T
 x+b=0
where:

𝑤
w is the weight vector (determines the direction of the hyperplane).
𝑏
b is the bias term (shifts the hyperplane).
𝑥
x is the input feature vector.
For a given point
𝑥
𝑖
x
i
​
 , the classification rule is:

𝑦
𝑖
=
sign
(
𝑤
𝑇
𝑥
𝑖
+
𝑏
)
y
i
​
 =sign(w
T
 x
i
​
 +b)
where
𝑦
𝑖
∈
{
+
1
,
−
1
}
y
i
​
 ∈{+1,−1} represents class labels.

2. Hard Margin vs. Soft Margin SVC
Hard Margin SVC: Assumes perfectly separable data and does not allow misclassification.

Soft Margin SVC: Allows some misclassification using slack variables
𝜉
𝑖
ξ
i
​
  to handle noisy or overlapping data. The objective function becomes:

min
⁡
𝑤
,
𝑏
1
2
∥
𝑤
∥
2
+
𝐶
∑
𝑖
𝜉
𝑖
w,b
min
​

2
1
​
 ∥w∥
2
 +C
i
∑
​
 ξ
i
​

where C is a regularization parameter controlling the trade-off between margin size and misclassification.

3. Nonlinear Classification with Kernel SVC
For non-linearly separable data, SVC uses the kernel trick to map data into a higher-dimensional space where it becomes linearly separable. The decision function changes to:

𝑓
(
𝑥
)
=
∑
𝑖
𝛼
𝑖
𝑦
𝑖
𝐾
(
𝑥
𝑖
,
𝑥
)
+
𝑏
f(x)=
i
∑
​
 α
i
​
 y
i
​
 K(x
i
​
 ,x)+b
where
𝐾
(
𝑥
𝑖
,
𝑥
)
K(x
i
​
 ,x) is a kernel function such as:

Linear Kernel:
𝐾
(
𝑥
𝑖
,
𝑥
𝑗
)
=
𝑥
𝑖
𝑇
𝑥
𝑗
K(x
i
​
 ,x
j
​
 )=x
i
T
​
 x
j
​

Polynomial Kernel:
𝐾
(
𝑥
𝑖
,
𝑥
𝑗
)
=
(
𝑥
𝑖
𝑇
𝑥
𝑗
+
𝑐
)
𝑑
K(x
i
​
 ,x
j
​
 )=(x
i
T
​
 x
j
​
 +c)
d

Radial Basis Function (RBF) Kernel:
𝐾
(
𝑥
𝑖
,
𝑥
𝑗
)
=
𝑒
−
𝛾
∥
𝑥
𝑖
−
𝑥
𝑗
∥
2
K(x
i
​
 ,x
j
​
 )=e
−γ∥x
i
​
 −x
j
​
 ∥
2


4. Key Features of SVC
✅ Maximizes the margin between classes for better generalization.
✅ Uses support vectors to define the decision boundary.
✅ Soft margin allows handling noisy or overlapping data.
✅ Can classify non-linearly separable data using kernel functions.

Support Vector Regressor (SVR)
A Support Vector Regressor (SVR) is a regression version of Support Vector Machines (SVM) that predicts continuous values while maintaining robustness against outliers. Instead of classifying data, SVR finds a function that best fits the data while allowing some flexibility within a given margin.

1. How SVR Works
SVR aims to find a function:

𝑓
(
𝑥
)
=
𝑤
𝑇
𝑥
+
𝑏
f(x)=w
T
 x+b
that predicts the output with minimal error, while ensuring that most training points fall within a certain ε-tube around the regression line.

Key Concept: The 𝜀-Insensitive Tube
Unlike traditional regression (which minimizes the squared error), SVR only penalizes predictions that fall outside a margin
𝜖
ϵ.
If a point is inside the
𝜖
ϵ-tube, no penalty is applied (the error is ignored).
If a point is outside the
𝜖
ϵ-tube, a penalty is applied using slack variables
𝜉
ξ.
2. SVR Optimization Problem
The objective is to minimize:

1
2
∥
𝑤
∥
2
2
1
​
 ∥w∥
2

subject to:

𝑦
𝑖
−
(
𝑤
𝑇
𝑥
𝑖
+
𝑏
)
≤
𝜖
+
𝜉
𝑖
y
i
​
 −(w
T
 x
i
​
 +b)≤ϵ+ξ
i
​

(
𝑤
𝑇
𝑥
𝑖
+
𝑏
)
−
𝑦
𝑖
≤
𝜖
+
𝜉
𝑖
∗
(w
T
 x
i
​
 +b)−y
i
​
 ≤ϵ+ξ
i
∗
​

𝜉
𝑖
,
𝜉
𝑖
∗
≥
0
ξ
i
​
 ,ξ
i
∗
​
 ≥0
where:

𝜖
ϵ (epsilon margin) defines the tolerance for error.
𝜉
𝑖
,
𝜉
𝑖
∗
ξ
i
​
 ,ξ
i
∗
​
  are slack variables that allow deviations beyond
𝜖
ϵ.
C is a hyperparameter that controls the trade-off between margin width and penalty for mispredictions.
3. Kernel Trick in SVR (Nonlinear Regression)
For nonlinear relationships, we use the kernel trick to transform input data into a higher-dimensional space:

𝑓
(
𝑥
)
=
∑
𝑖
(
𝛼
𝑖
−
𝛼
𝑖
∗
)
𝐾
(
𝑥
𝑖
,
𝑥
)
+
𝑏
f(x)=
i
∑
​
 (α
i
​
 −α
i
∗
​
 )K(x
i
​
 ,x)+b
where:

𝐾
(
𝑥
𝑖
,
𝑥
)
K(x
i
​
 ,x) is a kernel function (e.g., Linear, Polynomial, or RBF).
𝛼
𝑖
,
𝛼
𝑖
∗
α
i
​
 ,α
i
∗
​
  are Lagrange multipliers from the dual optimization problem.
4. Key Features of SVR
✅ Works well for small datasets and is less sensitive to outliers due to the
𝜖
ϵ-tube.
✅ Controls complexity using C and
𝜖
ϵ (higher C → lower margin, more complexity).
✅ Can model nonlinear relationships using kernels like RBF and Polynomial.
✅ Only support vectors contribute to predictions, making SVR efficient.

The Kernel Trick in SVM
The Kernel Trick is a technique used in Support Vector Machines (SVM) to handle non-linearly separable data by transforming it into a higher-dimensional space where it becomes linearly separable—without explicitly computing the transformation.

1. Why Do We Need the Kernel Trick?
In many cases, data cannot be separated by a straight line in its original space.
Instead of manually transforming the data into a higher-dimensional space, the kernel trick allows us to apply a function that computes the dot product in that space efficiently.
For example, in 2D space, a dataset that is not linearly separable may become separable in 3D space after applying a transformation.

2. How the Kernel Trick Works
In SVM, we solve an optimization problem in its dual form:

max
⁡
𝛼
∑
𝑖
𝛼
𝑖
−
1
2
∑
𝑖
,
𝑗
𝛼
𝑖
𝛼
𝑗
𝑦
𝑖
𝑦
𝑗
𝐾
(
𝑥
𝑖
,
𝑥
𝑗
)
α
max
​

i
∑
​
 α
i
​
 −
2
1
​

i,j
∑
​
 α
i
​
 α
j
​
 y
i
​
 y
j
​
 K(x
i
​
 ,x
j
​
 )
where
𝐾
(
𝑥
𝑖
,
𝑥
𝑗
)
K(x
i
​
 ,x
j
​
 ) is a kernel function that computes an implicit dot product in a higher-dimensional space:

𝐾
(
𝑥
𝑖
,
𝑥
𝑗
)
=
Φ
(
𝑥
𝑖
)
⋅
Φ
(
𝑥
𝑗
)
K(x
i
​
 ,x
j
​
 )=Φ(x
i
​
 )⋅Φ(x
j
​
 )
Instead of explicitly mapping
𝑥
x into a higher-dimensional space
Φ
(
𝑥
)
Φ(x), the kernel trick allows us to compute
𝐾
(
𝑥
𝑖
,
𝑥
𝑗
)
K(x
i
​
 ,x
j
​
 ) directly in the original space, avoiding computational complexity.

3. Common Types of Kernel Functions
Here are some popular kernel functions used in SVM:

1. Linear Kernel
Used when data is already linearly separable:

𝐾
(
𝑥
𝑖
,
𝑥
𝑗
)
=
𝑥
𝑖
𝑇
𝑥
𝑗
K(x
i
​
 ,x
j
​
 )=x
i
T
​
 x
j
​

2. Polynomial Kernel
Useful when there is a polynomial relationship between features:

𝐾
(
𝑥
𝑖
,
𝑥
𝑗
)
=
(
𝑥
𝑖
𝑇
𝑥
𝑗
+
𝑐
)
𝑑
K(x
i
​
 ,x
j
​
 )=(x
i
T
​
 x
j
​
 +c)
d

𝑑
d = degree of the polynomial
𝑐
c = a constant (controls influence of higher-order terms)
Example: If
𝑑
=
2
d=2, the transformation maps a 2D input to a 3D feature space.

3. Radial Basis Function (RBF) Kernel (Most Popular)
Captures complex nonlinear relationships:

𝐾
(
𝑥
𝑖
,
𝑥
𝑗
)
=
𝑒
−
𝛾
∥
𝑥
𝑖
−
𝑥
𝑗
∥
2
K(x
i
​
 ,x
j
​
 )=e
−γ∥x
i
​
 −x
j
​
 ∥
2


𝛾
γ controls how much influence a single training point has.
Small
𝛾
γ → More generalized decision boundary.
Large
𝛾
γ → More localized, high variance model (risk of overfitting).
4. Sigmoid Kernel (Similar to Neural Networks)
𝐾
(
𝑥
𝑖
,
𝑥
𝑗
)
=
tanh
⁡
(
𝛽
𝑥
𝑖
𝑇
𝑥
𝑗
+
𝑐
)
K(x
i
​
 ,x
j
​
 )=tanh(βx
i
T
​
 x
j
​
 +c)
Similar to an artificial neuron activation function.
Used less frequently than RBF or Polynomial.
4. Advantages of the Kernel Trick
✅ Avoids explicit transformation into high-dimensional space, reducing computation.
✅ Enables SVM to handle complex decision boundaries for non-linearly separable data.
✅ Works well with small to medium-sized datasets.
✅ Flexibility: Different kernels can be used depending on data characteristics.

5. When to Use Which Kernel?
Kernel	Use Case
Linear	When data is already linearly separable.
Polynomial	When data has polynomial relationships.
RBF (Gaussian)	When data has complex, non-linear patterns.
Sigmoid	When modeling behavior similar to neural networks.

Comparison of Linear, Polynomial, and RBF Kernels in SVM
The choice of kernel function significantly impacts the performance of Support Vector Machines (SVM). Here’s a comparison of the Linear, Polynomial, and Radial Basis Function (RBF) kernels in terms of their properties, advantages, and use cases.

1. Kernel Formulas & Intuition
Kernel Type	Formula	Intuition
Linear Kernel
𝐾
(
𝑥
𝑖
,
𝑥
𝑗
)
=
𝑥
𝑖
𝑇
𝑥
𝑗
K(x
i
​
 ,x
j
​
 )=x
i
T
​
 x
j
​
 	Directly computes the dot product in the original feature space. Works well when data is already linearly separable.
Polynomial Kernel
𝐾
(
𝑥
𝑖
,
𝑥
𝑗
)
=
(
𝑥
𝑖
𝑇
𝑥
𝑗
+
𝑐
)
𝑑
K(x
i
​
 ,x
j
​
 )=(x
i
T
​
 x
j
​
 +c)
d
 	Maps data to a higher-dimensional space, capturing polynomial relationships between features.
RBF Kernel (Gaussian Kernel)
𝐾
(
𝑥
𝑖
,
𝑥
𝑗
)
=
𝑒
−
𝛾
∥
𝑥
𝑖
−
𝑥
𝑗
∥
2
K(x
i
​
 ,x
j
​
 )=e
−γ∥x
i
​
 −x
j
​
 ∥
2

 	Maps data to an infinite-dimensional space. Useful when decision boundaries are highly nonlinear.
2. Performance Comparison
Feature	Linear Kernel	Polynomial Kernel	RBF Kernel
Computation Speed	✅ Fastest	❌ Slower (depends on degree
𝑑
d)	❌❌ Slowest (especially for large datasets)
Interpretability	✅ Easy to interpret	❌ Harder to interpret	❌❌ Difficult to interpret
When to Use?	When data is linearly separable	When data has polynomial relationships	When data is highly non-linear
Flexibility	❌ Limited	✅ Somewhat flexible	✅✅ Highly flexible
Risk of Overfitting	✅ Low risk	❌ Higher risk (if degree
𝑑
d is too large)	❌❌ Higher risk (if
𝛾
γ is too large)
Best for High-Dimensional Data?	✅ Yes	❌ Not ideal	✅ Yes (performs well in high dimensions)
Hyperparameters to Tune	None	Degree
𝑑
d, constant
𝑐
c
𝛾
γ (controls spread),
𝐶
C (regularization)
3. When to Choose Which Kernel?
✅ Use Linear Kernel when:
The data is linearly separable (can be separated with a straight line).
You have high-dimensional data (e.g., text classification).
You need a faster and more interpretable model.
✅ Use Polynomial Kernel when:
There is a polynomial relationship between features.
A quadratic or cubic decision boundary is expected.
The dataset is not too large, as polynomial kernels can be computationally expensive.
✅ Use RBF Kernel when:
The decision boundary is highly non-linear.
You do not know the exact feature transformation needed.
The dataset is moderate in size (RBF is computationally expensive on very large datasets).
4. Example Decision Boundaries (Visualization)
Imagine you have a dataset with two classes that need to be separated:

Linear Kernel: Separates classes with a straight line.
Polynomial Kernel: Can create curved decision boundaries.
RBF Kernel: Can form complex, flexible decision boundaries.

Effect of the
𝐶
C Parameter in SVM
The
𝐶
C parameter in Support Vector Machines (SVM) is a regularization hyperparameter that controls the trade-off between:

Maximizing the margin (simpler, more generalized model).
Minimizing classification errors (fitting training data more closely).
1. How
𝐶
C Affects the Decision Boundary
Small
𝐶
C (Higher Regularization, Large Margin)

Prioritizes a wider margin.
Allows more misclassifications in training.
Leads to a simpler, more generalized model (good for noisy data).
May underfit if
𝐶
C is too small.
Large
𝐶
C (Lower Regularization, Narrow Margin)

Prioritizes correct classification of training points.
Allows less misclassification.
Leads to a narrower margin.
May overfit if
𝐶
C is too large, capturing noise in data.
2. Mathematical Perspective
SVM’s optimization function:

min
⁡
𝑤
,
𝑏
1
2
∥
𝑤
∥
2
+
𝐶
∑
𝑖
𝜉
𝑖
w,b
min
​

2
1
​
 ∥w∥
2
 +C
i
∑
​
 ξ
i
​

where:

∥
𝑤
∥
2
∥w∥
2
  controls the margin size.

∑
𝜉
𝑖
∑ξ
i
​
  represents the total misclassification error.

𝐶
C balances margin width vs. error minimization.

Small
𝐶
C → Higher importance on
1
2
∥
𝑤
∥
2
2
1
​
 ∥w∥
2
  → Larger margin.

Large
𝐶
C → Higher importance on error term
∑
𝜉
𝑖
∑ξ
i
​
  → Less misclassification.

3. Visual Effect of
𝐶
C
Low
𝐶
C (Generalized Model)	High
𝐶
C (Overfitting Model)
✅ Large margin	❌ Narrow margin
✅ Allows some misclassification	❌ Tries to classify all points correctly
✅ More generalizable to new data	❌ May overfit training data
❌ May underfit if too small	❌ May not generalize well
4. When to Use High or Low
𝐶
C?
✅ Use Small
𝐶
C (High Regularization):

When generalization is more important than perfect training accuracy.
When dealing with noisy data or overlapping classes.
When you want a simpler model with fewer support vectors.
✅ Use Large
𝐶
C (Low Regularization):

When correct classification is more important than generalization.
When data is well-separated and you want a tight margin.
When overfitting is not a concern.
5. Summary of
𝐶
C in SVM
Small
𝐶
C → Wider margin, allows misclassification, better generalization.
Large
𝐶
C → Narrow margin, fewer misclassifications, risk of overfitting.
Choosing
𝐶
C correctly is crucial for balancing bias and variance!

Role of the
𝛾
γ Parameter in RBF Kernel SVM
The
𝛾
γ parameter in Radial Basis Function (RBF) Kernel SVM controls the influence of individual training points on the decision boundary and plays a key role in defining the shape of the decision boundary. It is a hyperparameter that can significantly affect the model's performance.

1. Mathematical Definition of
𝛾
γ
The RBF Kernel function is given by:

𝐾
(
𝑥
𝑖
,
𝑥
𝑗
)
=
𝑒
−
𝛾
∥
𝑥
𝑖
−
𝑥
𝑗
∥
2
K(x
i
​
 ,x
j
​
 )=e
−γ∥x
i
​
 −x
j
​
 ∥
2


where:

∥
𝑥
𝑖
−
𝑥
𝑗
∥
2
∥x
i
​
 −x
j
​
 ∥
2
  is the squared Euclidean distance between two data points
𝑥
𝑖
x
i
​
  and
𝑥
𝑗
x
j
​
 .
𝛾
γ controls how far the influence of a single training point reaches. A larger
𝛾
γ means that each data point's influence is more localized, while a smaller
𝛾
γ means the influence of each point is spread out over a larger region.
2. Effect of
𝛾
γ on the Decision Boundary
Small
𝛾
γ	Large
𝛾
γ
✅ Wider influence: The decision boundary is smoother and more generalized.	❌ Narrow influence: The decision boundary becomes more complex and localized to individual points.
✅ Less sensitive to individual points: The model may underfit.	❌ Highly sensitive to individual points: The model may overfit, capturing noise and outliers.
❌ Can lead to underfitting (too simple model)	✅ Can lead to overfitting (too complex model)
3. Visualizing the Effect of
𝛾
γ
Small
𝛾
γ: The model will have a smooth decision boundary because the influence of each data point extends across a wide area. This is suitable for datasets where the classes are well-separated and can be generalized.
Large
𝛾
γ: The decision boundary becomes more jagged and complex, fitting the training data very closely. This can result in overfitting where the model captures noise or outliers in the data.
4. Intuition Behind
𝛾
γ
Small
𝛾
γ: Data points have a wide reach, so the decision boundary is less influenced by any single point. This creates a smooth and generalizable boundary but might not capture complex patterns in data.
Large
𝛾
γ: Each point has a more localized effect, causing the decision boundary to become more sensitive to individual data points. The model is likely to overfit, especially if there are outliers or noise in the data.
5. Hyperparameter Tuning for
𝛾
γ
The value of
𝛾
γ should be chosen carefully through techniques like cross-validation to balance model complexity and generalization.
Grid Search or Random Search can help to find the optimal value of
𝛾
γ by testing different values and evaluating performance.
6. Summary of
𝛾
γ in RBF Kernel SVM
Small
𝛾
γ: Influences the decision boundary to be smooth and generalizable but risks underfitting.
Large
𝛾
γ: Creates a complex, tight decision boundary that may overfit the data.
The right
𝛾
γ value helps achieve a balance between bias and variance, avoiding both underfitting and overfitting.

Naïve Bayes Classifier
The Naïve Bayes classifier is a probabilistic machine learning algorithm based on Bayes' Theorem. It is used for classification tasks and is particularly popular in text classification problems (e.g., spam detection, sentiment analysis). It is called "Naïve" because it makes a key assumption about the features used for classification, which simplifies the model.

1. Bayes' Theorem
At the core of Naïve Bayes is Bayes' Theorem, which provides a way of calculating the probability of a class
𝐶
C, given the input features
𝑋
=
(
𝑥
1
,
𝑥
2
,
…
,
𝑥
𝑛
)
X=(x
1
​
 ,x
2
​
 ,…,x
n
​
 ):

𝑃
(
𝐶
∣
𝑋
)
=
𝑃
(
𝑋
∣
𝐶
)
𝑃
(
𝐶
)
𝑃
(
𝑋
)
P(C∣X)=
P(X)
P(X∣C)P(C)
​

Where:

𝑃
(
𝐶
∣
𝑋
)
P(C∣X) is the posterior probability, i.e., the probability of class
𝐶
C given the features
𝑋
X.
𝑃
(
𝑋
∣
𝐶
)
P(X∣C) is the likelihood, i.e., the probability of observing features
𝑋
X given class
𝐶
C.
𝑃
(
𝐶
)
P(C) is the prior probability, i.e., the probability of class
𝐶
C before seeing any data.
𝑃
(
𝑋
)
P(X) is the evidence, i.e., the probability of the features
𝑋
X across all classes.
2. The "Naïve" Assumption
The "Naïve" part of the Naïve Bayes classifier refers to the assumption of conditional independence between the features. This means that, given the class label
𝐶
C, the features
𝑥
1
,
𝑥
2
,
…
,
𝑥
𝑛
x
1
​
 ,x
2
​
 ,…,x
n
​
  are assumed to be independent of each other:

𝑃
(
𝑋
∣
𝐶
)
=
𝑃
(
𝑥
1
∣
𝐶
)
𝑃
(
𝑥
2
∣
𝐶
)
…
𝑃
(
𝑥
𝑛
∣
𝐶
)
P(X∣C)=P(x
1
​
 ∣C)P(x
2
​
 ∣C)…P(x
n
​
 ∣C)
This assumption drastically simplifies the computation because instead of modeling the joint probability
𝑃
(
𝑋
∣
𝐶
)
P(X∣C), we multiply the individual probabilities for each feature.

3. The Naïve Bayes Classification Rule
Given the independence assumption, the Naïve Bayes classifier calculates the posterior probability for each class, and the class with the highest posterior is chosen as the predicted label. For each class
𝐶
𝑘
C
k
​
 , the classifier computes:

𝑃
(
𝐶
𝑘
∣
𝑋
)
∝
𝑃
(
𝐶
𝑘
)
∏
𝑖
=
1
𝑛
𝑃
(
𝑥
𝑖
∣
𝐶
𝑘
)
P(C
k
​
 ∣X)∝P(C
k
​
 )
i=1
∏
n
​
 P(x
i
​
 ∣C
k
​
 )
The class
𝐶
𝑘
C
k
​
  that maximizes this posterior probability is the predicted class:

𝐶
^
=
arg
⁡
max
⁡
𝐶
𝑘
𝑃
(
𝐶
𝑘
)
∏
𝑖
=
1
𝑛
𝑃
(
𝑥
𝑖
∣
𝐶
𝑘
)
C
^
 =arg
C
k
​

max
​
 P(C
k
​
 )
i=1
∏
n
​
 P(x
i
​
 ∣C
k
​
 )
4. Why is It Called "Naïve"?
It’s called "Naïve" because of the independence assumption: the model naively assumes that all the features are independent, which is rarely the case in real-world data. Despite this strong assumption, Naïve Bayes often works surprisingly well, especially when the features are not highly correlated (e.g., in text classification, where word occurrences are somewhat independent).

5. Types of Naïve Bayes Classifiers
There are several variants of the Naïve Bayes classifier, depending on the type of data and distribution assumptions:

Gaussian Naïve Bayes: Assumes that the features are continuous and follow a Gaussian (normal) distribution.
Multinomial Naïve Bayes: Suitable for discrete features (e.g., word counts in text classification). It assumes that the features follow a multinomial distribution.
Bernoulli Naïve Bayes: Used when the features are binary (e.g., word presence/absence).
6. Advantages of Naïve Bayes
✅ Simple and efficient: Naïve Bayes is easy to implement and computationally efficient, especially for large datasets.
✅ Works well with high-dimensional data: Especially effective in text classification tasks like spam filtering or sentiment analysis.
✅ Handles missing data: It can handle missing data in some situations by ignoring the missing feature or treating it as having an unknown value.
✅ Requires fewer training data: It performs well even with relatively small training datasets.

7. Disadvantages of Naïve Bayes
❌ Independence assumption: The assumption that features are independent is often unrealistic, especially when features are correlated.
❌ Poor performance on highly correlated features: When features are highly dependent on each other, Naïve Bayes tends to underperform.
❌ Zero probability problem: If a feature's value does not appear in the training data for a particular class, Naïve Bayes assigns a zero probability, leading to issues. This can be mitigated using Laplace smoothing.

8. Applications of Naïve Bayes
Text Classification (e.g., spam filtering, sentiment analysis, topic categorization).
Medical Diagnosis (predicting the presence of diseases based on features like symptoms).
Recommendation Systems (predicting user preferences based on behavior).
Summary
Naïve Bayes is a probabilistic classifier based on Bayes' Theorem with the key assumption that features are conditionally independent given the class label.
It's called "Naïve" because of the naïve assumption that the features do not depend on each other, which often doesn't hold true in real-world data, but the model still works well in many cases.

Bayes' Theorem
Bayes' Theorem is a fundamental concept in probability theory and statistics that describes the relationship between conditional probabilities. It provides a way to update our beliefs about a hypothesis or event based on new evidence. Bayes' Theorem is widely used in various fields such as machine learning, statistics, data science, and artificial intelligence, especially in classification tasks.

1. Bayes' Theorem Formula
Bayes' Theorem is given by the following formula:

𝑃
(
𝐴
∣
𝐵
)
=
𝑃
(
𝐵
∣
𝐴
)
𝑃
(
𝐴
)
𝑃
(
𝐵
)
P(A∣B)=
P(B)
P(B∣A)P(A)
​

Where:

𝑃
(
𝐴
∣
𝐵
)
P(A∣B) is the posterior probability: the probability of hypothesis
𝐴
A given the observed evidence
𝐵
B.
𝑃
(
𝐵
∣
𝐴
)
P(B∣A) is the likelihood: the probability of observing evidence
𝐵
B given that hypothesis
𝐴
A is true.
𝑃
(
𝐴
)
P(A) is the prior probability: the initial probability of hypothesis
𝐴
A before observing the evidence
𝐵
B.
𝑃
(
𝐵
)
P(B) is the evidence: the total probability of observing the evidence
𝐵
B under all possible hypotheses.
2. Intuition Behind Bayes' Theorem
Prior Probability
𝑃
(
𝐴
)
P(A): This is your belief about the hypothesis
𝐴
A before seeing the new data.

Likelihood
𝑃
(
𝐵
∣
𝐴
)
P(B∣A): This is how likely the evidence
𝐵
B is, given that the hypothesis
𝐴
A is true.

Evidence
𝑃
(
𝐵
)
P(B): This is the probability of observing the evidence, considering all possible hypotheses. It normalizes the equation so that the sum of all probabilities equals 1.

Posterior Probability
𝑃
(
𝐴
∣
𝐵
)
P(A∣B): This is the updated probability of the hypothesis
𝐴
A, given the new evidence
𝐵
B.

3. Practical Interpretation of Bayes' Theorem
Bayes' Theorem allows you to update your beliefs about an event after observing new data or evidence. In other words, it tells you how much confidence you should have in a hypothesis after seeing new information.

For example, in medical diagnostics, if a patient shows a certain symptom (evidence
𝐵
B), Bayes' Theorem can be used to update the probability of a specific disease (hypothesis
𝐴
A) based on prior knowledge of how common the disease is (prior
𝑃
(
𝐴
)
P(A)) and how likely the symptom is in patients with the disease (likelihood
𝑃
(
𝐵
∣
𝐴
)
P(B∣A)).

4. An Example: Spam Email Classification
Let’s say you're trying to classify whether an email is spam or not spam based on the occurrence of the word "free" (evidence
𝐵
B):

Hypothesis
𝐴
A: The email is spam.
Prior
𝑃
(
𝐴
)
P(A): The probability that any email is spam, say 30% (0.3).
Likelihood
𝑃
(
𝐵
∣
𝐴
)
P(B∣A): The probability of the word "free" appearing in a spam email, say 70% (0.7).
Evidence
𝑃
(
𝐵
)
P(B): The probability of the word "free" appearing in any email, say 40% (0.4).
Now, you want to calculate the posterior probability that an email is spam, given that it contains the word "free" (i.e.,
𝑃
(
𝐴
∣
𝐵
)
P(A∣B)).

Using Bayes' Theorem:

𝑃
(
Spam
∣
"free"
)
=
𝑃
(
"free"
∣
Spam
)
𝑃
(
Spam
)
𝑃
(
"free"
)
P(Spam∣"free")=
P("free")
P("free"∣Spam)P(Spam)
​

Substituting the values:

𝑃
(
Spam
∣
"free"
)
=
(
0.7
)
(
0.3
)
0.4
=
0.525
P(Spam∣"free")=
0.4
(0.7)(0.3)
​
 =0.525
So, the probability that the email is spam, given that it contains the word "free," is 52.5%.

5. Generalized Form of Bayes' Theorem (for Multiple Hypotheses)
When there are multiple possible hypotheses
𝐴
1
,
𝐴
2
,
…
,
𝐴
𝑛
A
1
​
 ,A
2
​
 ,…,A
n
​
 , Bayes' Theorem can be generalized to:

𝑃
(
𝐴
𝑖
∣
𝐵
)
=
𝑃
(
𝐵
∣
𝐴
𝑖
)
𝑃
(
𝐴
𝑖
)
∑
𝑖
=
1
𝑛
𝑃
(
𝐵
∣
𝐴
𝑖
)
𝑃
(
𝐴
𝑖
)
P(A
i
​
 ∣B)=
∑
i=1
n
​
 P(B∣A
i
​
 )P(A
i
​
 )
P(B∣A
i
​
 )P(A
i
​
 )
​

Where the denominator is the total probability of evidence
𝐵
B across all possible hypotheses.

6. Applications of Bayes' Theorem
Bayes' Theorem is widely used in:

Machine learning (e.g., Naïve Bayes classifier).
Medical diagnosis (updating the probability of diseases based on test results).
Spam filtering (classifying emails based on their content).
Weather prediction (updating the probability of a weather event based on new observations).
Natural language processing (NLP) (for tasks like speech recognition and part-of-speech tagging).
Summary of Bayes' Theorem
Bayes' Theorem helps us update our beliefs about a hypothesis given new evidence.
It is based on prior probability, likelihood, and evidence.
The formula allows us to calculate the posterior probability.
Bayes' Theorem is widely used in classification, decision-making, and predictive modeling.

Differences Between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes
Naïve Bayes is a family of probabilistic classifiers based on Bayes' Theorem with different assumptions about the distribution of the data. The three most common types of Naïve Bayes classifiers are Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes. Each type is suited for different types of data, and they differ mainly in how they model the distribution of features.

1. Gaussian Naïve Bayes (GNB)
Assumption: Features are continuous and follow a Gaussian (normal) distribution.
Feature Type: Works best with continuous features (real-valued data).
How it Works: For each class, it assumes that the features follow a Gaussian distribution and uses the mean and variance of each feature within the class to calculate the likelihood.
Mathematical Formulation
For each feature
𝑥
x, the probability density function of the Gaussian distribution is given by:

𝑃
(
𝑥
𝑖
∣
𝐶
)
=
1
2
𝜋
𝜎
2
𝑒
−
(
𝑥
𝑖
−
𝜇
)
2
2
𝜎
2
P(x
i
​
 ∣C)=
2πσ
2

​

1
​
 e
−
2σ
2

(x
i
​
 −μ)
2

​


Where:

𝜇
μ is the mean of the feature values for class
𝐶
C,
𝜎
2
σ
2
  is the variance of the feature values for class
𝐶
C.
When to Use Gaussian Naïve Bayes?
When the features are continuous and roughly follow a normal (bell-shaped) distribution.
Common applications: medical data, sensor data, and financial data where continuous values are prevalent.
2. Multinomial Naïve Bayes (MNB)
Assumption: Features are discrete and follow a multinomial distribution (i.e., counts of discrete events or occurrences).
Feature Type: Works best with discrete features, especially count-based data (e.g., word frequencies, document-term matrices).
How it Works: It models the probability of observing a set of features as counts, and for each class, it assumes that the features follow a multinomial distribution (a generalization of the binomial distribution for multiple categories).
Mathematical Formulation
For a class
𝐶
C and a set of features
𝑋
=
(
𝑥
1
,
𝑥
2
,
.
.
.
,
𝑥
𝑛
)
X=(x
1
​
 ,x
2
​
 ,...,x
n
​
 ), the likelihood of features
𝑋
X given class
𝐶
C is modeled as:

𝑃
(
𝑋
∣
𝐶
)
=
∏
𝑖
=
1
𝑛
(
𝑛
𝑖
+
𝛼
)
(
𝑁
+
𝛼
𝑛
)
P(X∣C)=
i=1
∏
n
​

(N+αn)
(n
i
​
 +α)
​

Where:

𝑛
𝑖
n
i
​
  is the count of feature
𝑖
i in the document,
𝑁
N is the total number of features in the document,
𝛼
α is a smoothing parameter (usually set to 1 for Laplace smoothing).
When to Use Multinomial Naïve Bayes?
When the features represent counts or frequencies (e.g., word counts in documents).
Common applications: text classification tasks like spam filtering, document categorization, and topic modeling.
3. Bernoulli Naïve Bayes (BNB)
Assumption: Features are binary (either 0 or 1), representing the presence or absence of a feature.
Feature Type: Works best with binary features, where the feature represents the presence or absence of a characteristic (e.g., presence of a word in a document).
How it Works: Bernoulli Naïve Bayes assumes that the features are binary and models the probability of a feature being present (1) or absent (0) in the given class.
Mathematical Formulation
For each feature
𝑥
𝑖
x
i
​
  in the data, the likelihood is calculated as:

𝑃
(
𝑥
𝑖
=
1
∣
𝐶
)
=
𝑛
𝑖
,
1
+
𝛼
𝑛
𝐶
+
𝛼
P(x
i
​
 =1∣C)=
n
C
​
 +α
n
i,1
​
 +α
​

𝑃
(
𝑥
𝑖
=
0
∣
𝐶
)
=
𝑛
𝑖
,
0
+
𝛼
𝑛
𝐶
+
𝛼
P(x
i
​
 =0∣C)=
n
C
​
 +α
n
i,0
​
 +α
​

Where:

𝑛
𝑖
,
1
n
i,1
​
  is the number of times feature
𝑥
𝑖
x
i
​
  is present in class
𝐶
C,
𝑛
𝑖
,
0
n
i,0
​
  is the number of times feature
𝑥
𝑖
x
i
​
  is absent in class
𝐶
C,
𝑛
𝐶
n
C
​
  is the total number of samples in class
𝐶
C,
𝛼
α is a smoothing parameter (usually set to 1 for Laplace smoothing).
When to Use Bernoulli Naïve Bayes?
When the features are binary (0 or 1), indicating whether something exists or not.
Common applications: document classification where the presence or absence of specific words matters (e.g., spam filtering, sentiment analysis).
4. Key Differences
Feature Type	Gaussian Naïve Bayes	Multinomial Naïve Bayes	Bernoulli Naïve Bayes
Data Type	Continuous (real-valued features)	Discrete (count data)	Binary (0 or 1)
Assumption	Features are normally distributed	Features follow a multinomial distribution	Features are binary (presence/absence)
Typical Use Case	Continuous data, e.g., medical data, financial data	Count-based data, e.g., text classification, document term frequency	Binary data, e.g., spam classification, binary text classification
Likelihood Calculation	Based on Gaussian distribution (mean and variance)	Based on word counts or frequency of events	Based on presence/absence of features
5. When to Choose Which Naïve Bayes?
Gaussian Naïve Bayes: Use when your data consists of continuous features that can be approximated as normally distributed.
Multinomial Naïve Bayes: Use when your data consists of discrete counts, such as word counts or event counts, and you want to model occurrences or frequencies.
Bernoulli Naïve Bayes: Use when your data consists of binary features, such as word presence or absence in documents.
Summary
Gaussian Naïve Bayes: Assumes features are continuous and follow a Gaussian distribution. Best for continuous data.
Multinomial Naïve Bayes: Assumes features are discrete counts and follows a multinomial distribution. Best for count data like word frequencies.
Bernoulli Naïve Bayes: Assumes binary features (presence/absence). Best for binary data like document classification.
Each variant of Naïve Bayes is tailored to different types of data and can be selected based on the nature of the problem you are working on.

When to Use Gaussian Naïve Bayes over Other Variants
Gaussian Naïve Bayes (GNB) is a good choice when your features are continuous and approximately follow a Gaussian (normal) distribution. Here's a detailed breakdown of when you should prefer Gaussian Naïve Bayes over other variants (like Multinomial Naïve Bayes or Bernoulli Naïve Bayes):

1. Continuous Features with Gaussian Distribution
Data Type: The most important condition for using Gaussian Naïve Bayes is that your features should be continuous (i.e., they can take any real value).

Distribution Assumption: It works best when these continuous features are approximately normally distributed (bell-shaped curve), though it can still work to some extent with features that are roughly symmetric or unimodal.

Example: If you're classifying data based on features like height, weight, temperature, salary, or other real-valued attributes, and these features are expected to follow a normal distribution, Gaussian Naïve Bayes is a great choice.
2. Assumed Normality of Data
Gaussian Distribution: If you suspect that the distribution of your data for each class is normal, then Gaussian Naïve Bayes is a natural choice. This is because it explicitly assumes that features, conditioned on the class, follow a normal distribution.

Example: In cases like predicting blood pressure levels, where the data for each class (e.g., normal, high, low) might follow a normal distribution, Gaussian Naïve Bayes can provide good performance.
3. When You Have Less Computational Complexity
Simple Computation: Gaussian Naïve Bayes is computationally simpler and less memory-intensive compared to more complex models, especially if you have a small to medium-sized dataset.
Since it only requires mean and variance for each feature (per class), it doesn’t require storing large amounts of data like other methods such as decision trees or support vector machines.

Example: When you're working with datasets where you want quick, easy-to-understand results and you don't want the overhead of training more complex models (like decision trees or SVMs).

4. High-Dimensional Data
Performance on High-Dimensional Data: Even though Gaussian Naïve Bayes assumes normality, it can work fairly well when dealing with high-dimensional data (large numbers of features), provided that those features are continuous and can be reasonably assumed to follow normal distributions.

Example: In medical diagnostics or financial data analysis, where there may be many continuous features but you're looking for a simple, fast classification model.
5. When Features Are Relatively Independent
Conditional Independence: Gaussian Naïve Bayes, like other variants of Naïve Bayes, assumes conditional independence between features given the class label. Although this assumption is often unrealistic in real-world data, the model can still perform well when features are weakly correlated.

Example: In certain sensor-based data (e.g., temperature, humidity, pressure), even if features are not perfectly independent, Gaussian Naïve Bayes can still perform reasonably well compared to more complex models.
6. When You Have Small or Moderate Data
Effective with Limited Data: Like other Naïve Bayes variants, Gaussian Naïve Bayes often works well even with small datasets. This is because it makes strong assumptions (normality, independence) that allow it to generalize well despite having fewer data points.

Example: In domains like early-stage medical research or small-scale surveys, where limited data is available and you need a quick, reliable model.
When NOT to Use Gaussian Naïve Bayes
Non-Normal Data: If your continuous features do not follow a Gaussian distribution and exhibit skewed or multi-modal patterns, Gaussian Naïve Bayes might not perform well. For example, features that have exponential, log-normal, or other non-Gaussian distributions can lead to poor results.

Alternatives: In these cases, Kernel Density Estimation (KDE) or other methods that do not assume a specific distribution may be better, or you could try other Naïve Bayes variants that are more flexible.
Highly Correlated Features: If your features are highly correlated with each other, the assumption of conditional independence made by Naïve Bayes will not hold, and performance may degrade.

Alternatives: You could try using Gaussian Mixture Models (GMMs) or other models that explicitly model correlations between features.
Summary of When to Use Gaussian Naïve Bayes
When features are continuous and roughly follow a normal distribution (Gaussian).
For small to medium-sized datasets with fewer complex relationships between features.
When computational efficiency and simplicity are required.
For high-dimensional data where each feature is treated independently.
When you don’t have large amounts of labeled data and need a model that can generalize effectively.
Example Scenarios
Medical Diagnostics: Predicting a person's health condition (e.g., diabetes, heart disease) based on continuous features like age, weight, blood sugar levels, etc.
Financial Data: Predicting stock market trends based on historical prices or features like interest rates, trading volumes, etc.
Sensor Data: Classifying sensor readings like temperature, humidity, or pressure, where the data is continuous.
Summary
Gaussian Naïve Bayes is best used when your features are continuous and normally distributed, and when you are dealing with smaller or moderately sized datasets. While it may not be ideal for highly skewed or multi-modal data, its simplicity, speed, and efficiency make it a go-to algorithm in many scenarios, especially when performance needs to be balanced with ease of implementation.

Key Assumptions Made by Naïve Bayes
Naïve Bayes is a simple and powerful probabilistic classifier based on Bayes' Theorem. However, to make its computations tractable, Naïve Bayes makes several strong assumptions. The "naïve" part of the name refers to these assumptions, which simplify the model but may not always hold true in real-world data. Here are the key assumptions made by Naïve Bayes:

1. Conditional Independence of Features
Assumption: Naïve Bayes assumes that all features are conditionally independent given the class label.

Explanation: This means that the presence (or value) of one feature does not affect the presence (or value) of any other feature once the class label is known.

Example: In spam email classification, Naïve Bayes assumes that the presence of the word "money" in an email is independent of the presence of the word "free" in the same email, given that the email is either spam or not.
Why it matters: This assumption makes the model computationally efficient, as it allows the calculation of the likelihood of the features given the class as a product of individual feature likelihoods:

𝑃
(
𝑋
∣
𝐶
)
=
𝑃
(
𝑥
1
∣
𝐶
)
×
𝑃
(
𝑥
2
∣
𝐶
)
×
⋯
×
𝑃
(
𝑥
𝑛
∣
𝐶
)
P(X∣C)=P(x
1
​
 ∣C)×P(x
2
​
 ∣C)×⋯×P(x
n
​
 ∣C)
2. Feature Distribution Assumptions
Naïve Bayes also makes assumptions about how the features are distributed within each class:

a) Gaussian Naïve Bayes (for continuous features)
Assumption: Features are normally distributed (i.e., they follow a Gaussian distribution) for each class.

Explanation: The model assumes that the values of continuous features are distributed according to a normal (bell-shaped) curve, which is described by the mean and variance for each class.

Example: If you’re using Naïve Bayes to classify emails as spam or not, and your features are continuous like the frequency of certain words, the model assumes these frequencies follow a Gaussian distribution for each class (spam or not spam).
b) Multinomial Naïve Bayes (for count-based features)
Assumption: Features are discrete and follow a multinomial distribution (suitable for count data, like word counts in text).

Explanation: In Multinomial Naïve Bayes, each feature is treated as a count or frequency of an event (e.g., word occurrence). The model assumes that the occurrence of each word is independent given the class.

Example: If you have the features word frequency, the model assumes that the frequency of each word in a document follows a multinomial distribution (counting the occurrence of words).
c) Bernoulli Naïve Bayes (for binary features)
Assumption: Features are binary (0 or 1), representing the presence or absence of a feature.

Explanation: The model assumes that each feature follows a Bernoulli distribution, which models the probability of a binary outcome (such as whether a word is present or not in a document).

Example: In a binary text classification task, you might have features like "contains the word 'free'" (1 if present, 0 if absent), and the model assumes these features are independent of each other given the class label.
3. Same Prior Probability for All Features (Simplifying Assumption)
Assumption: All features are assumed to have the same prior probability across all classes, meaning that the class distribution for each feature is the same before observing the actual data.
Explanation: This assumption helps reduce complexity in calculating the likelihoods and priors, simplifying the classification process.
4. Ignoring Feature Interactions
Assumption: Naïve Bayes ignores any potential interactions between features. In real-world data, features often interact or have dependencies on each other, but Naïve Bayes does not account for these relationships.

Example: In predicting whether a patient has a disease based on features like blood pressure and cholesterol levels, Naïve Bayes does not take into account the potential interaction between these two features, which might influence the risk of the disease.
5. Independence Between Classes
Assumption: Naïve Bayes assumes that each class is independent of the others and can be treated separately.

Explanation: Once you have the class labels, Naïve Bayes computes the probability of the data belonging to each class independently, which simplifies the calculation and classification.

Example: In spam detection, Naïve Bayes assumes that the presence of a particular word in an email contributes independently to the classification of whether it is spam or not, without considering how the words interact within the context of the entire email.
Why Are These Assumptions Considered "Naïve"?
The assumptions made by Naïve Bayes (particularly conditional independence) are often unrealistic in practice. In real-world data, features are often correlated, and they may not follow a Gaussian or multinomial distribution.
Despite these "naïve" assumptions, Naïve Bayes can still work surprisingly well in many practical scenarios, especially when the data is sparse or when you're working with high-dimensional data (like text classification).
When Do These Assumptions Work Well?
Naïve Bayes tends to perform well under the following conditions:

Independent features: When features are indeed independent or nearly independent, the assumptions of Naïve Bayes hold, leading to effective classification.
High-dimensional data: In cases where there are many features (like text classification), even weak correlations between features can still result in a good performance.
Small datasets: Naïve Bayes can perform well even with smaller datasets, as it makes strong assumptions that help generalize from limited data.
Text classification tasks: In tasks like spam filtering, sentiment analysis, and document classification, where features (e.g., words) can be treated as discrete events and might exhibit independence.
Summary of Assumptions
Conditional Independence: Features are conditionally independent given the class label.
Feature Distribution:
Gaussian distribution for continuous features (Gaussian Naïve Bayes).
Multinomial distribution for count-based features (Multinomial Naïve Bayes).
Bernoulli distribution for binary features (Bernoulli Naïve Bayes).
Same Prior Probability for all features across classes.
No Feature Interactions: Assumes features do not interact with each other.
Independence Between Classes: Assumes each class is independent of the others.

17. What are the advantages and disadvantages of Naïve Bayes?
Advantages and Disadvantages of Naïve Bayes
Naïve Bayes is a popular classification algorithm, especially in tasks like text classification and spam filtering, due to its simplicity, efficiency, and strong performance in many applications. However, like any algorithm, it has its pros and cons. Let's break them down:

Advantages of Naïve Bayes
1. Simple and Efficient
Explanation: Naïve Bayes is one of the simplest classification algorithms, based on probability theory. It is easy to implement and requires minimal computational resources.

Why it matters: Its simplicity allows it to be applied quickly to large datasets, with minimal training time. It also makes the model easy to understand and interpret.

Example: In spam email classification, Naïve Bayes can be trained and used very quickly, even for large email datasets.
2. Fast and Scalable
Explanation: The training process in Naïve Bayes involves estimating probabilities, which is relatively quick compared to more complex algorithms like decision trees or support vector machines.

Why it matters: Naïve Bayes can handle large datasets efficiently. It works particularly well when the number of features (dimensionality) is high, as in text classification tasks.

Example: Naïve Bayes can be applied to a large dataset of product reviews to classify them as positive or negative with minimal computational resources.
3. Works Well with High-Dimensional Data
Explanation: Naïve Bayes works well in high-dimensional spaces (many features) because of its conditional independence assumption. It doesn’t require interactions between features to be explicitly modeled.

Why it matters: In many real-world applications (like document classification), there are thousands or even millions of features (e.g., words). Naïve Bayes is effective at handling this level of complexity.

Example: In spam detection, where each word in an email might be treated as a separate feature, Naïve Bayes can still perform well even with large numbers of unique words.
4. Performs Well with Smaller Datasets
Explanation: Because of its strong assumptions (e.g., conditional independence), Naïve Bayes can work well even with relatively small datasets, as it does not require a large amount of data to learn the relationships between features and classes.

Why it matters: For applications where data is limited (e.g., rare events like medical diagnoses), Naïve Bayes can still provide reasonable performance.

Example: In a medical diagnosis task with limited patient data, Naïve Bayes can often perform well.
5. Robust to Irrelevant Features
Explanation: Naïve Bayes can handle irrelevant features (i.e., features that don't add value for classification) better than many other algorithms. The independence assumption allows it to ignore irrelevant features during the classification process.

Why it matters: Even if some features are not useful, the model's performance may not degrade significantly.

Example: In text classification, certain words might not be useful in determining the topic, but Naïve Bayes still works well.
6. Works Well with Categorical and Continuous Data
Explanation: Depending on the variant (Gaussian Naïve Bayes for continuous features, Multinomial Naïve Bayes for count data, and Bernoulli Naïve Bayes for binary data), Naïve Bayes can handle different types of features.

Why it matters: It’s versatile and can be applied across various domains and datasets with different feature types.

Example: In document classification, text features (words) can be handled by Multinomial Naïve Bayes, while continuous features (e.g., sensor data) can be handled by Gaussian Naïve Bayes.
Disadvantages of Naïve Bayes
1. Strong Assumptions (Conditional Independence)
Explanation: The conditional independence assumption is often unrealistic in real-world data. Many features are correlated, and Naïve Bayes doesn't account for these interactions, which can lead to poor performance in some cases.

Why it matters: If features are highly correlated, Naïve Bayes can perform poorly because it assumes that they are independent given the class.

Example: In a medical diagnosis task where features like blood pressure and cholesterol levels are highly correlated, Naïve Bayes may not capture the relationships between these features, potentially leading to incorrect classifications.
2. Poor Performance with Highly Correlated Features
Explanation: The assumption that features are conditionally independent makes Naïve Bayes less effective when features are strongly correlated with each other.

Why it matters: The model will not be able to correctly model the relationships between correlated features, leading to bias and reduced accuracy.

Example: In financial forecasting, where features like stock price and trading volume might be highly correlated, Naïve Bayes may not capture the underlying relationship effectively.
3. Limited Expressiveness
Explanation: Because of its simplicity, Naïve Bayes is less expressive than more complex models like decision trees or neural networks. It cannot model complex interactions or non-linear relationships between features.

Why it matters: In datasets where complex patterns exist, Naïve Bayes may not perform as well as more sophisticated models.

Example: In image classification, where complex features such as pixel interactions or spatial relationships matter, Naïve Bayes may struggle to capture the necessary patterns.
4. Assumes Class Conditional Distributions are Known
Explanation: Naïve Bayes assumes that the distribution of features is known (e.g., normal distribution for continuous features in Gaussian Naïve Bayes). In practice, the actual distribution of features may differ from the assumed distribution.

Why it matters: If the real distribution deviates significantly from the assumed distribution, Naïve Bayes' performance can degrade.

Example: If the actual distribution of pixel intensities in an image differs from the assumed Gaussian distribution, Gaussian Naïve Bayes may not perform well for image classification.
5. Zero Probability Problem (for Multinomial Naïve Bayes)
Explanation: In Multinomial Naïve Bayes, if a particular feature (e.g., word) does not appear in the training set for a given class, the probability of that feature is set to zero, which can lead to incorrect classification.

Why it matters: This can cause problems, particularly when dealing with sparse data (e.g., rare words in text classification). To mitigate this, smoothing techniques (e.g., Laplace smoothing) are often used.

Example: In text classification, if a word appears in the test set but not in the training set for a given class, the model will assign a zero probability, potentially causing misclassification.
Summary of Advantages and Disadvantages
Advantages	Disadvantages
Simple and easy to implement	Strong assumptions (conditional independence) often unrealistic
Fast and efficient (good for large datasets)	Poor performance when features are highly correlated
Works well with high-dimensional data	Limited expressiveness (cannot model complex interactions)
Performs well with smaller datasets	Assumes known class-conditional distributions (which may not be accurate)
Robust to irrelevant features	Zero probability problem (in Multinomial Naïve Bayes)
Works with both categorical and continuous data
When to Use Naïve Bayes
When features are mostly independent and not highly correlated.
When you need a fast and efficient model, especially for text classification or simple tasks.
When you have a small or medium-sized dataset and don’t need complex decision boundaries.
When you're working with high-dimensional data, such as documents with a large number of features (words).
When interpretability is important, as Naïve Bayes provides clear probabilistic reasoning for predictions.

18. Why is Naïve Bayes a good choice for text classification?
Why is Naïve Bayes a Good Choice for Text Classification?
Naïve Bayes is particularly well-suited for text classification tasks, such as spam filtering, sentiment analysis, and topic classification, due to several reasons that align well with the characteristics of text data. Here’s why Naïve Bayes works well in this domain:

1. High Dimensionality of Text Data
Explanation: In text classification, the feature space (i.e., the set of possible features) is usually high-dimensional, as each word in a document can be treated as a unique feature. A typical text classification task might involve tens of thousands of unique words (features).

Why it matters: Naïve Bayes is computationally efficient even in high-dimensional spaces because it assumes that the features (words) are conditionally independent given the class label. This makes it scalable and fast in processing large numbers of features.

Example: In spam email classification, each word in an email becomes a feature. There could be thousands of unique words in a collection of emails, but Naïve Bayes can still handle this complexity effectively due to its simplicity.
2. Assumption of Conditional Independence
Explanation: Text features are often treated as independent in Naïve Bayes, which aligns well with a bag-of-words model in text classification. This model treats words as independent of each other, ignoring word order but still capturing word frequencies.

Why it matters: Although the independence assumption is often violated in real-world data (e.g., word combinations or phrases), it still works well in text classification because words tend to provide a strong signal for classification on their own.

Example: In sentiment analysis, words like "happy" or "excited" may independently indicate a positive sentiment, even if they occur together in a sentence. Naïve Bayes can capture this independently, despite ignoring their exact positions in the sentence.
3. Discrete Features (Word Counts)
Explanation: Text classification often uses discrete features, such as the frequency of specific words appearing in a document or the presence/absence of certain words. These discrete features are ideal for the Multinomial Naïve Bayes model.

Why it matters: Multinomial Naïve Bayes treats the frequency of occurrences of each word as a discrete count, which works well when you have count-based data, such as word occurrences in a document.

Example: In topic modeling, the frequency of words like "politics" or "economy" can help classify documents into categories such as politics or business.
4. Robust to Irrelevant Features
Explanation: Naïve Bayes can handle irrelevant features (words that do not contribute meaningfully to the classification task). In text classification, many words may not be informative (such as common stopwords like "the," "and," "in"), but Naïve Bayes can still classify correctly.

Why it matters: The model doesn’t require explicit feature selection, and its performance doesn’t degrade significantly due to irrelevant features.

Example: In spam detection, common words that don't contribute much (like "the" or "a") still don't hurt the model much because it focuses on the more informative words, like "free" or "money."
5. Works Well with Limited Data
Explanation: Naïve Bayes works well with smaller datasets, which is often the case in text classification tasks where labeled data might be limited.

Why it matters: It doesn’t require large amounts of training data to make accurate predictions, and it generalizes well even with smaller datasets.

Example: If you're building a sentiment analysis model for a specific set of product reviews with limited labeled data, Naïve Bayes can still perform well.
6. Smoothing to Handle Zero Probabilities
Explanation: Naïve Bayes handles zero probabilities (when a word doesn’t appear in the training set) using smoothing techniques (e.g., Laplace smoothing). This is particularly important in text classification, where new words might appear in the test set that were not present in the training data.

Why it matters: Smoothing ensures that the model doesn’t assign a zero probability to a word, which would otherwise result in a classification failure.

Example: In email spam filtering, if the word "promotion" is missing from the training set but appears in a test email, smoothing ensures that the model still assigns a non-zero probability to this word in the classification process.
7. Clear Probabilistic Interpretation
Explanation: Naïve Bayes provides a probabilistic interpretation of classifications. It calculates the probability that a document belongs to a particular class (e.g., spam or not spam).

Why it matters: The probabilistic output is valuable for understanding how confident the model is in its predictions, which can be useful in applications where uncertainty matters (e.g., when to flag an email as spam).

Example: In document classification, Naïve Bayes provides the probability of a document belonging to a specific topic, which can help in ranking or further decision-making processes.
8. Efficient for Online Learning
Explanation: Naïve Bayes is particularly effective in online learning scenarios, where data arrives sequentially, and the model needs to be updated regularly.

Why it matters: In text classification, such as social media monitoring or real-time content filtering, Naïve Bayes can easily update itself as new data (documents) come in without needing to retrain from scratch.

Example: In real-time spam filtering for emails, Naïve Bayes can continuously learn from incoming emails and adjust its classification model accordingly.
9. Well-Suited for Multi-Class Classification
Explanation: Naïve Bayes naturally supports multi-class classification, where documents can belong to more than one class (e.g., topics or genres).

Why it matters: Text classification often involves multiple classes, and Naïve Bayes can easily handle the problem without needing complex adjustments.

Example: In news article classification, articles may belong to multiple categories like "politics," "sports," and "health," and Naïve Bayes can efficiently manage multi-class scenarios.
Conclusion: Why Naïve Bayes is a Good Choice for Text Classification
Naïve Bayes is a powerful tool for text classification due to its:

Computational efficiency in high-dimensional spaces,
Effectiveness with count-based and discrete features (such as word frequencies),
Ability to handle irrelevant features and small datasets,
Robustness to new or unseen data through smoothing techniques,
Probabilistic output that is easy to interpret and useful for ranking or decision-making.
Despite the strong independence assumptions, Naïve Bayes often performs remarkably well for many practical text classification tasks, especially when computational simplicity and interpretability are important.

Comparison between Support Vector Machine (SVM) and Naïve Bayes for Classification Tasks
Both Support Vector Machine (SVM) and Naïve Bayes (NB) are powerful classification algorithms, but they differ significantly in their approach, performance characteristics, and suitability for different types of problems. Here's a comprehensive comparison based on various aspects:

1. Algorithm Type and Approach
SVM:

Type: Supervised machine learning algorithm, typically used for classification and regression.
Approach: SVM tries to find the optimal hyperplane that maximizes the margin (distance) between the different classes. It focuses on finding the boundary that best separates the data points.
Key Idea: Maximizing the margin between the support vectors (the most important data points that define the boundary) and the decision boundary.
Naïve Bayes:

Type: Probabilistic classifier based on Bayes' Theorem.
Approach: Naïve Bayes computes the posterior probability for each class based on the features (words, values, etc.). It assumes that features are independent (conditional independence) given the class label, which simplifies the calculations.
Key Idea: Classify based on the maximum posterior probability, often assuming a distribution (e.g., Gaussian for continuous data, multinomial for text data).
2. Model Complexity and Assumptions
SVM:
Assumptions: SVM assumes that the classes are separable by a hyperplane (in the case of linear SVM). For non-linear SVM, the kernel trick is used to transform the data into a higher-dimensional space where it can be linearly separable.
Model Complexity: SVMs can model complex, non-linear decision boundaries (with kernels). The complexity increases with non-linear kernels (like RBF) as the transformation of the feature space can be computationally expensive.
Hyperparameter Tuning: SVM requires tuning of parameters like C (penalty for misclassification), gamma (kernel width), and the kernel type (e.g., linear, RBF, polynomial).
Naïve Bayes:
Assumptions: The key assumption in Naïve Bayes is the conditional independence of features given the class label, which is a strong assumption and may not hold true in practice.
Model Complexity: Naïve Bayes is relatively simple and computationally less expensive. It assumes that the features (e.g., words) in the dataset are conditionally independent, which simplifies calculations.
Hyperparameter Tuning: Naïve Bayes requires minimal tuning (mainly the smoothing parameter for handling zero probabilities), making it easier to deploy with minimal setup.
3. Performance in High-Dimensionality
SVM:

High-Dimensional Data: SVM performs well in high-dimensional spaces, especially with text classification tasks, where each word is treated as a feature. The kernel trick makes SVM highly effective in capturing complex relationships in high-dimensional data.
Computation: SVM can be computationally expensive when dealing with large datasets or high-dimensional feature spaces (e.g., many words in text classification). Non-linear kernels like RBF or polynomial can significantly increase computation time.
Naïve Bayes:

High-Dimensional Data: Naïve Bayes is also very effective in high-dimensional spaces. In text classification, for instance, each word is treated as a feature, and Naïve Bayes can efficiently handle large vocabulary sizes.
Computation: Naïve Bayes is computationally efficient and scales well with large datasets and high-dimensional feature spaces, making it ideal for tasks like spam filtering or sentiment analysis.
4. Handling of Feature Correlation
SVM:

Feature Correlation: SVM doesn’t assume feature independence and can handle correlated features quite well. In fact, SVM can model complex feature interactions if you choose an appropriate kernel.
Suitability: SVM works well when there are complex relationships between features, making it useful in applications where features are not independent.
Naïve Bayes:

Feature Correlation: Naïve Bayes assumes that features are independent of one another given the class label. This assumption can be unrealistic, particularly when features are highly correlated.
Suitability: Naïve Bayes may struggle if there are strong correlations between features, leading to poor performance when features are not conditionally independent.
5. Performance with Small Datasets
SVM:

Small Datasets: SVM generally requires more data to produce a reliable model, especially with a non-linear kernel. It can overfit with small datasets, particularly when the C parameter is not properly tuned.
Sensitivity to Overfitting: SVM can overfit the training data, especially in the case of a complex kernel or improper regularization.
Naïve Bayes:

Small Datasets: Naïve Bayes is relatively robust to small datasets and works well even when the amount of training data is limited, thanks to its probabilistic nature and simplicity.
Less Likely to Overfit: The simplicity of Naïve Bayes makes it less prone to overfitting, especially when compared to SVM, which is more prone to overfitting in small datasets without proper regularization.
6. Model Interpretability
SVM:

Interpretability: SVM, especially with non-linear kernels, can be harder to interpret. While the decision boundary can be visualized in lower dimensions (2D/3D), in higher dimensions it becomes much more difficult to understand.
Interpretation: The model itself doesn’t provide direct insights into feature importance, but the support vectors (the data points closest to the decision boundary) provide some interpretability in terms of the decision-making process.
Naïve Bayes:

Interpretability: Naïve Bayes is often more interpretable because it provides a probabilistic view of the classification process. You can easily understand how each feature (e.g., word) contributes to the classification by looking at the probability values.
Feature Contribution: You can also analyze how each feature influences the prediction, which is useful for applications where model transparency is important (e.g., medical diagnoses or credit scoring).
7. Scalability
SVM:

Scalability: SVM may not scale as well to very large datasets, especially when using non-linear kernels. Training time and memory requirements grow significantly with larger datasets.
Computational Complexity: SVM's training complexity is often
𝑂
(
𝑛
2
)
O(n
2
 ) or higher for large datasets, which can become a bottleneck in large-scale problems.
Naïve Bayes:

Scalability: Naïve Bayes is highly scalable to large datasets. Since it computes probabilities independently for each feature, it tends to scale much better than SVM, especially when dealing with very large datasets.
Computational Complexity: Naïve Bayes typically has linear time complexity,
𝑂
(
𝑛
)
O(n), where
𝑛
n is the number of training samples, making it much faster than SVM on large datasets.
8. Suitability for Specific Applications
SVM:

Best for: SVM is preferred when the decision boundaries are complex or non-linear, and when the dataset contains noise or high-dimensional features. It works well in tasks like image recognition, bioinformatics, and text classification with non-linear relationships.
Examples: Image classification, handwriting recognition, and bioinformatics problems like gene classification.
Naïve Bayes:

Best for: Naïve Bayes is often preferred in situations where features are conditionally independent or nearly so, and when the dataset is small to medium-sized. It excels in text classification tasks, particularly for spam detection, sentiment analysis, and document categorization.
Examples: Email spam detection, document classification, and sentiment analysis.
Summary of Comparison
Aspect	SVM	Naïve Bayes
Approach	Maximizes margin between classes	Based on probability (Bayes' theorem)
Complexity	Can handle complex decision boundaries	Simple and computationally efficient
Feature Independence	Can handle correlated features	Assumes features are independent
Performance	Works well with large datasets, but can overfit small ones	Works well with small datasets, less prone to overfitting
Scalability	Can be computationally expensive	Highly scalable and fast, even with large datasets
Interpretability	Harder to interpret, especially with kernels	Easy to interpret, provides probability estimates
Kernel Trick	Effective with non-linear decision boundaries	Does not use kernels (assumes feature independence)
Best Use Case	High-dimensional, complex data (e.g., images, bioinformatics)	Text classification, spam detection, sentiment analysis
Conclusion
Choose SVM when:
You have complex, high-dimensional data.
You can afford the computational cost.
You need to capture complex, non-linear relationships between features.
Choose Naïve Bayes when:
You have a large number of features with low correlation (text data is a common case).
You need a simple, fast, and interpretable model.
You’re working with smaller datasets or need a quick solution for text classification tasks.

20. How does Laplace Smoothing help in Naïve Bayes?
How Does Laplace Smoothing Help in Naïve Bayes?
Laplace Smoothing (also known as Additive Smoothing) is a technique used in Naïve Bayes to handle the issue of zero probabilities when a feature (e.g., a word) in the test data has not been seen during training.

In Naïve Bayes, the classification depends on calculating the probability of each feature (e.g., word) belonging to a particular class. If a word appears in the test set but was never encountered in the training set, the conditional probability of that word given the class would be zero. This is problematic because multiplying probabilities of independent features together (as Naïve Bayes does) can result in a zero probability for the entire class, which would mean an incorrect classification.

Laplace smoothing addresses this issue by ensuring that no probability is ever zero, even for unseen features. It does this by adding a small constant (usually 1) to each count in the probability calculations, thereby avoiding zero probabilities.

How Laplace Smoothing Works in Naïve Bayes:
Let’s break down the concept with an example:

For a given class
𝐶
C, and a feature (e.g., a word
𝑤
w) in a text document, the conditional probability
𝑃
(
𝑤
∣
𝐶
)
P(w∣C) (probability of the word
𝑤
w given class
𝐶
C) is usually calculated as:
𝑃
(
𝑤
∣
𝐶
)
=
count of word
𝑤
 in class
𝐶
total count of all words in class
𝐶
P(w∣C)=
total count of all words in class C
count of word w in class C
​

However, if the word
𝑤
w never appeared in the training data for class
𝐶
C, the count of
𝑤
w in class
𝐶
C would be zero, leading to
𝑃
(
𝑤
∣
𝐶
)
=
0
P(w∣C)=0.
Laplace Smoothing Formula:
Laplace smoothing modifies the above formula to:

𝑃
(
𝑤
∣
𝐶
)
=
count of word
𝑤
 in class
𝐶
+
1
total count of all words in class
𝐶
+
𝑉
P(w∣C)=
total count of all words in class C+V
count of word w in class C+1
​

Where:

𝑉
V is the vocabulary size (the total number of unique words across all classes).
The +1 in the numerator ensures that each word, including previously unseen words, gets a non-zero probability.
Effect of Laplace Smoothing:
For unseen words: The count of unseen words in the numerator becomes 1, and in the denominator, we add the size of the vocabulary
𝑉
V, which ensures that the probability remains non-zero.
For seen words: The formula still works the same as before but with an adjustment due to the smoothing constant (the +1 in the numerator).
Example to Illustrate Laplace Smoothing:
Suppose you have a dataset with the following two classes:

Class A has the words: "cat", "dog", "mouse".
Class B has the words: "dog", "fish", "bird".
Now, imagine you're trying to calculate the probability
𝑃
(
"fish"
∣
𝐴
)
P("fish"∣A) (the probability that the word "fish" appears in Class A). Since "fish" never appeared in Class A, without smoothing,
𝑃
(
"fish"
∣
𝐴
)
=
0
P("fish"∣A)=0.

However, with Laplace smoothing, the probability for "fish" would be:

𝑃
(
"fish"
∣
𝐴
)
=
0
+
1
3
+
5
=
1
8
P("fish"∣A)=
3+5
0+1
​
 =
8
1
​

Where:

0 is the count of "fish" in Class A (since it doesn’t appear).
3 is the total count of words in Class A (i.e., "cat", "dog", "mouse").
5 is the total vocabulary size (i.e., "cat", "dog", "mouse", "fish", "bird").
By adding the smoothing factor, we ensure that even words not seen in the training data can be given a non-zero probability, avoiding the issue of zero probabilities when classifying new data.

Why Laplace Smoothing is Important:
Prevents Zero Probabilities: It ensures that features (words) that don’t appear in the training set but appear in the test set don’t cause the model to fail.

Improves Generalization: Laplace smoothing helps the model generalize better by smoothing out extreme values in probability estimation. This is particularly important in cases where you might encounter rare or unseen features.

Essential for Text Classification: In text classification tasks like spam detection or sentiment analysis, where new words can appear frequently in test data that were never seen during training, Laplace smoothing is crucial for maintaining robust classification performance.

Summary
Laplace Smoothing adds a small constant (usually 1) to all feature counts to ensure no probability is zero.
It is crucial in Naïve Bayes, especially when dealing with features that don’t appear in the training data (e.g., unseen words in text classification tasks).
By avoiding zero probabilities, it helps the classifier make reliable predictions, even when unseen features are encountered.

Practical

1. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy?
# Import necessary libraries
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize the SVM classifier
svm_classifier = SVC(kernel='linear')  # You can change the kernel to 'rbf' for non-linear SVM

# Train the model
svm_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred = svm_classifier.predict(X_test)

# Evaluate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the SVM classifier on the Iris dataset: {accuracy * 100:.2f}%")

# Import necessary libraries
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize the SVM classifier
svm_classifier = SVC(kernel='linear')  # You can change the kernel to 'rbf' for non-linear SVM

# Train the model
svm_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred = svm_classifier.predict(X_test)

# Evaluate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the SVM classifier on the Iris dataset: {accuracy * 100:.2f}%")

2. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies?
# Import necessary libraries
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load the Wine dataset
wine = datasets.load_wine()
X = wine.data
y = wine.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize the SVM classifiers with different kernels
svm_linear = SVC(kernel='linear')
svm_rbf = SVC(kernel='rbf')

# Train the linear kernel classifier
svm_linear.fit(X_train, y_train)

# Train the RBF kernel classifier
svm_rbf.fit(X_train, y_train)

# Make predictions for both classifiers
y_pred_linear = svm_linear.predict(X_test)
y_pred_rbf = svm_rbf.predict(X_test)

# Calculate accuracies
accuracy_linear = accuracy_score(y_test, y_pred_linear)
accuracy_rbf = accuracy_score(y_test, y_pred_rbf)

# Print the results
print(f"Accuracy of SVM with Linear kernel: {accuracy_linear * 100:.2f}%")
print(f"Accuracy of SVM with RBF kernel: {accuracy_rbf * 100:.2f}%")

# Compare the accuracies
if accuracy_linear > accuracy_rbf:
    print("Linear kernel performed better.")
elif accuracy_rbf > accuracy_linear:
    print("RBF kernel performed better.")
else:
    print("Both kernels performed equally well.")

3. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE)?
# Import necessary libraries
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error
import numpy as np

# Load the Boston housing dataset
boston = datasets.load_boston()
X = boston.data
y = boston.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize the SVR regressor with a linear kernel
svr = SVR(kernel='linear')

# Train the model
svr.fit(X_train, y_train)

# Make predictions on the test set
y_pred = svr.predict(X_test)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error (MSE) of the SVR model: {mse:.2f}")

# Optionally, calculate Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)
print(f"Root Mean Squared Error (RMSE) of the SVR model: {rmse:.2f}")

4.Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary?
# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Create a simple 2D classification dataset
X, y = datasets.make_classification(n_samples=100, n_features=2, n_classes=2, random_state=42)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize the features (important for SVM performance)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initialize the SVM classifier with a polynomial kernel
svm_poly = SVC(kernel='poly', degree=3, C=1.0)

# Train the classifier
svm_poly.fit(X_train, y_train)

# Visualize the decision boundary
x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))

# Make predictions for the grid
Z = svm_poly.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot the decision boundary and the training points
plt.contourf(xx, yy, Z, alpha=0.8)
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', marker='o', s=100, cmap=plt.cm.coolwarm)
plt.title("SVM with Polynomial Kernel (degree=3)")
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

5.Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy?
# Import necessary libraries
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Load the Breast Cancer dataset
cancer = datasets.load_breast_cancer()
X = cancer.data
y = cancer.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize the Gaussian Naïve Bayes classifier
gnb = GaussianNB()

# Train the model
gnb.fit(X_train, y_train)

# Make predictions on the test set
y_pred = gnb.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the Gaussian Naïve Bayes classifier: {accuracy * 100:.2f}%")

6.Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20 Newsgroups dataset.
# Import necessary libraries
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

# Load the 20 Newsgroups dataset
newsgroups = datasets.fetch_20newsgroups(subset='all')

# Get the data (text) and labels
X = newsgroups.data
y = newsgroups.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Convert the text data to a bag-of-words representation
vectorizer = CountVectorizer(stop_words='english')
X_train_vectorized = vectorizer.fit_transform(X_train)
X_test_vectorized = vectorizer.transform(X_test)

# Initialize the Multinomial Naïve Bayes classifier
mnb = MultinomialNB()

# Train the model
mnb.fit(X_train_vectorized, y_train)

# Make predictions on the test set
y_pred = mnb.predict(X_test_vectorized)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the Multinomial Naïve Bayes classifier: {accuracy * 100:.2f}%")

7.Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually?
# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Create a simple 2D classification dataset
X, y = datasets.make_classification(n_samples=100, n_features=2, n_classes=2, random_state=42)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize the features (important for SVM performance)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Values of C to experiment with
C_values = [0.01, 0.1, 1, 10]

# Create subplots to visualize the decision boundaries for different C values
fig, axes = plt.subplots(2, 2, figsize=(10, 8))

for i, C in enumerate(C_values):
    # Initialize the SVM classifier with a linear kernel and the current value of C
    svm = SVC(kernel='linear', C=C)

    # Train the classifier
    svm.fit(X_train, y_train)

    # Plot the decision boundary
    ax = axes[i // 2, i % 2]
    x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1
    y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))

    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    ax.contourf(xx, yy, Z, alpha=0.8)
    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', marker='o', s=100, cmap=plt.cm.coolwarm)
    ax.set_title(f"SVM with C = {C}")
    ax.set_xlabel('Feature 1')
    ax.set_ylabel('Feature 2')

# Adjust layout and show the plot
plt.tight_layout()
plt.show()

 8.Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with binary features?
 # Import necessary libraries
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score
from sklearn.datasets import make_classification

# Generate a synthetic binary classification dataset with binary features
X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_classes=2,
                           random_state=42, n_clusters_per_class=1, flip_y=0,
                           class_sep=2, feature_distribution='uniform')

# Ensure the features are binary (0 or 1)
X = np.round(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize the Bernoulli Naïve Bayes classifier
bnb = BernoulliNB()

# Train the model
bnb.fit(X_train, y_train)

# Make predictions on the test set
y_pred = bnb.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the Bernoulli Naïve Bayes classifier: {accuracy * 100:.2f}%")

9.Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data?
# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Use only the first two features for easy visualization
X = X[:, :2]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# --- Unscaled SVM Model ---
# Initialize the SVM classifier with a linear kernel
svm_unscaled = SVC(kernel='linear', random_state=42)

# Train the unscaled model
svm_unscaled.fit(X_train, y_train)

# Make predictions on the test set
y_pred_unscaled = svm_unscaled.predict(X_test)

# Calculate accuracy for unscaled data
accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)

# --- Scaled SVM Model ---
# Standardize the features (important for SVM performance)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Initialize the SVM classifier for scaled data
svm_scaled = SVC(kernel='linear', random_state=42)

# Train the scaled model
svm_scaled.fit(X_train_scaled, y_train)

# Make predictions on the test set
y_pred_scaled = svm_scaled.predict(X_test_scaled)

# Calculate accuracy for scaled data
accuracy_scaled = accuracy_score(y_test, y_pred_scaled)

# Print the results
print(f"Accuracy with unscaled data: {accuracy_unscaled * 100:.2f}%")
print(f"Accuracy with scaled data: {accuracy_scaled * 100:.2f}%")

# --- Visualize decision boundaries ---
def plot_decision_boundary(X, y, model, ax, title):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))

    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    ax.contourf(xx, yy, Z, alpha=0.8)
    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', s=100, cmap=plt.cm.coolwarm)
    ax.set_title(title)
    return scatter

# Plot decision boundaries for both models
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

plot_decision_boundary(X_train, y_train, svm_unscaled, axes[0], "SVM with Unscaled Data")
plot_decision_boundary(X_train_scaled, y_train, svm_scaled, axes[1], "SVM with Scaled Data")

# Display the accuracy
plt.show()

10.Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and after Laplace Smoothing?
# Import necessary libraries
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# --- Gaussian Naïve Bayes without Laplace Smoothing ---
# Initialize the Gaussian Naïve Bayes classifier without Laplace smoothing
gnb_no_smoothing = GaussianNB(var_smoothing=1e-9)  # No smoothing by setting a very small value

# Train the model
gnb_no_smoothing.fit(X_train, y_train)

# Make predictions on the test set
y_pred_no_smoothing = gnb_no_smoothing.predict(X_test)

# Calculate accuracy for the model without Laplace smoothing
accuracy_no_smoothing = accuracy_score(y_test, y_pred_no_smoothing)

# --- Gaussian Naïve Bayes with Laplace Smoothing ---
# Initialize the Gaussian Naïve Bayes classifier with Laplace smoothing (var_smoothing > 0)
gnb_with_smoothing = GaussianNB(var_smoothing=1.0)  # Using Laplace smoothing

# Train the model with smoothing
gnb_with_smoothing.fit(X_train, y_train)

# Make predictions on the test set
y_pred_with_smoothing = gnb_with_smoothing.predict(X_test)

# Calculate accuracy for the model with Laplace smoothing
accuracy_with_smoothing = accuracy_score(y_test, y_pred_with_smoothing)

# Print the results
print(f"Accuracy without Laplace smoothing: {accuracy_no_smoothing * 100:.2f}%")
print(f"Accuracy with Laplace smoothing: {accuracy_with_smoothing * 100:.2f}%")

# Display the predictions for comparison
print("\nPredictions without Laplace Smoothing:")
print(y_pred_no_smoothing[:10])  # Display the first 10 predictions

print("\nPredictions with Laplace Smoothing:")
print(y_pred_with_smoothing[:10])  # Display the first 10 predictions

11.Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C,gamma, kernel)?
# Import necessary libraries
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialize the SVM classifier
svm = SVC()

# Define the parameter grid for GridSearchCV
param_grid = {
    'C': [0.1, 1, 10, 100],           # Regularization parameter
    'gamma': [0.001, 0.01, 0.1, 1],   # Kernel coefficient
    'kernel': ['linear', 'rbf']       # Kernel type
}

# Initialize GridSearchCV with 5-fold cross-validation
grid_search = GridSearchCV(svm, param_grid, cv=5, verbose=1, n_jobs=-1)

# Fit the model with the training data
grid_search.fit(X_train, y_train)

# Print the best parameters found by GridSearchCV
print("Best parameters found by GridSearchCV:", grid_search.best_params_)

# Get the best estimator (SVM model with optimal hyperparameters)
best_svm = grid_search.best_estimator_

# Make predictions on the test set
y_pred = best_svm.predict(X_test)

# Calculate accuracy of the best model
accuracy = accuracy_score(y_test, y_pred)

# Print the accuracy
print(f"Accuracy of the best model: {accuracy * 100:.2f}%")

12.Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and check it improve accuracy?
# Import necessary libraries
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.utils.class_weight import compute_class_weight

# Generate an imbalanced binary classification dataset
X, y = datasets.make_classification(n_samples=1000, n_features=20, n_informative=2, n_classes=2,
                                    weights=[0.9, 0.1], flip_y=0, random_state=42)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# --- SVM Classifier without class weighting ---
# Initialize the SVM classifier without class weighting
svm_no_weight = SVC(kernel='rbf', random_state=42)

# Train the model
svm_no_weight.fit(X_train, y_train)

# Make predictions on the test set
y_pred_no_weight = svm_no_weight.predict(X_test)

# Calculate accuracy for the model without class weighting
accuracy_no_weight = accuracy_score(y_test, y_pred_no_weight)

# --- SVM Classifier with class weighting ---
# Compute class weights to handle the imbalanced dataset
class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)

# Initialize the SVM classifier with class weighting
svm_with_weight = SVC(kernel='rbf', class_weight={0: class_weights[0], 1: class_weights[1]}, random_state=42)

# Train the model with class weights
svm_with_weight.fit(X_train, y_train)

# Make predictions on the test set
y_pred_with_weight = svm_with_weight.predict(X_test)

# Calculate accuracy for the model with class weighting
accuracy_with_weight = accuracy_score(y_test, y_pred_with_weight)

# Print the results
print(f"Accuracy without class weighting: {accuracy_no_weight * 100:.2f}%")
print(f"Accuracy with class weighting: {accuracy_with_weight * 100:.2f}%"

13. Write a Python program to implement a Naïve Bayes classifier for spam detection using email data?
# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Load the SMS Spam Collection dataset
# (Ensure you have the dataset downloaded and placed in the same directory as this script)
# The dataset contains two columns: 'label' (ham or spam) and 'message' (SMS text)
data = pd.read_csv('spam.csv', encoding='latin-1')

# Preprocess the dataset
# Drop the extra columns
data = data[['v1', 'v2']]
data.columns = ['label', 'message']

# Convert the labels to binary (spam = 1, ham = 0)
data['label'] = data['label'].map({'ham': 0, 'spam': 1})

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data['message'], data['label'], test_size=0.3, random_state=42)

# Convert the text messages into feature vectors using CountVectorizer
vectorizer = CountVectorizer(stop_words='english')

X_train_vect = vectorizer.fit_transform(X_train)
X_test_vect = vectorizer.transform(X_test)

# Initialize the Naïve Bayes classifier
nb_classifier = MultinomialNB()

# Train the model on the training data
nb_classifier.fit(X_train_vect, y_train)

# Make predictions on the test data
y_pred = nb_classifier.predict(X_test_vect)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Print the results
print(f"Accuracy of the Naïve Bayes classifier: {accuracy * 100:.2f}%")
print("Confusion Matrix:")
print(conf_matrix)
print("Classification Report:")
print(class_report)

14.Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and compare their accuracy?
# Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

# Load the SMS Spam Collection dataset
# (Ensure you have the dataset downloaded and placed in the same directory as this script)
# The dataset contains two columns: 'label' (ham or spam) and 'message' (SMS text)
data = pd.read_csv('spam.csv', encoding='latin-1')

# Preprocess the dataset
# Drop the extra columns
data = data[['v1', 'v2']]
data.columns = ['label', 'message']

# Convert the labels to binary (spam = 1, ham = 0)
data['label'] = data['label'].map({'ham': 0, 'spam': 1})

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data['message'], data['label'], test_size=0.3, random_state=42)

# Convert the text messages into feature vectors using CountVectorizer
vectorizer = CountVectorizer(stop_words='english')

X_train_vect = vectorizer.fit_transform(X_train)
X_test_vect = vectorizer.transform(X_test)

# --- Train the SVM classifier ---
svm_classifier = SVC(kernel='rbf', random_state=42)
svm_classifier.fit(X_train_vect, y_train)

# Make predictions on the test set
y_pred_svm = svm_classifier.predict(X_test_vect)

# Calculate accuracy for the SVM model
accuracy_svm = accuracy_score(y_test, y_pred_svm)

# --- Train the Naïve Bayes classifier ---
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train_vect, y_train)

# Make predictions on the test set
y_pred_nb = nb_classifier.predict(X_test_vect)

# Calculate accuracy for the Naïve Bayes model
accuracy_nb = accuracy_score(y_test, y_pred_nb)

# Print the results
print(f"Accuracy of the SVM classifier: {accuracy_svm * 100:.2f}%")
print(f"Accuracy of the Naïve Bayes classifier: {accuracy_nb * 100:.2f}%")

 15.Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare results?
 # Import necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.metrics import accuracy_score

# Load the SMS Spam Collection dataset
# (Ensure you have the dataset downloaded and placed in the same directory as this script)
# The dataset contains two columns: 'label' (ham or spam) and 'message' (SMS text)
data = pd.read_csv('spam.csv', encoding='latin-1')

# Preprocess the dataset
# Drop the extra columns
data = data[['v1', 'v2']]
data.columns = ['label', 'message']

# Convert the labels to binary (spam = 1, ham = 0)
data['label'] = data['label'].map({'ham': 0, 'spam': 1})

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data['message'], data['label'], test_size=0.3, random_state=42)

# Convert the text messages into feature vectors using CountVectorizer
vectorizer = CountVectorizer(stop_words='english')

X_train_vect = vectorizer.fit_transform(X_train)
X_test_vect = vectorizer.transform(X_test)

# --- Train the Naïve Bayes classifier without feature selection ---
nb_classifier_no_select = MultinomialNB()
nb_classifier_no_select.fit(X_train_vect, y_train)

# Make predictions on the test set
y_pred_no_select = nb_classifier_no_select.predict(X_test_vect)

# Calculate accuracy for the Naïve Bayes model without feature selection
accuracy_no_select = accuracy_score(y_test, y_pred_no_select)

# --- Perform feature selection with SelectKBest ---
# Use SelectKBest with chi-squared test to select the top 100 features
selector = SelectKBest(chi2, k=100)
X_train_selected = selector.fit_transform(X_train_vect, y_train)
X_test_selected = selector.transform(X_test_vect)

# Train the Naïve Bayes classifier on the selected features
nb_classifier_select = MultinomialNB()
nb_classifier_select.fit(X_train_selected, y_train)

# Make predictions on the test set with the selected features
y_pred_select = nb_classifier_select.predict(X_test_selected)

# Calculate accuracy for the Naïve Bayes model with feature selection
accuracy_select = accuracy_score(y_test, y_pred_select)

# Print the results
print(f"Accuracy without feature selection: {accuracy_no_select * 100:.2f}%")
print(f"Accuracy with feature selection: {accuracy_select * 100:.2f}%")


 16.Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy?
      # Import necessary libraries
import pandas as pd
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier

# Load the Wine dataset
wine_data = load_wine()

# Create a DataFrame for easier handling
X = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)
y = pd.Series(wine_data.target)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# --- One-vs-Rest (OvR) Strategy ---
# Train an SVM classifier with the OvR strategy
svm_ovr = OneVsRestClassifier(SVC(kernel='linear', random_state=42))
svm_ovr.fit(X_train, y_train)

# Make predictions and calculate accuracy for OvR
y_pred_ovr = svm_ovr.predict(X_test)
accuracy_ovr = accuracy_score(y_test, y_pred_ovr)

# --- One-vs-One (OvO) Strategy ---
# Train an SVM classifier with the OvO strategy
svm_ovo = OneVsOneClassifier(SVC(kernel='linear', random_state=42))
svm_ovo.fit(X_train, y_train)

# Make predictions and calculate accuracy for OvO
y_pred_ovo = svm_ovo.predict(X_test)
accuracy_ovo = accuracy_score(y_test, y_pred_ovo)

# Print the results
print(f"Accuracy of SVM with One-vs-Rest (OvR): {accuracy_ovr * 100:.2f}%")
print(f"Accuracy of SVM with One-vs-One (OvO): {accuracy_ovo * 100:.2f}%")


 17. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy?
# Import necessary libraries
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load the Breast Cancer dataset
cancer_data = load_breast_cancer()

# Create a DataFrame for easier handling
X = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)
y = pd.Series(cancer_data.target)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# --- Train the SVM Classifier with a Linear Kernel ---
svm_linear = SVC(kernel='linear', random_state=42)
svm_linear.fit(X_train, y_train)

# Make predictions and calculate accuracy for Linear kernel
y_pred_linear = svm_linear.predict(X_test)
accuracy_linear = accuracy_score(y_test, y_pred_linear)

# --- Train the SVM Classifier with a Polynomial Kernel ---
svm_poly = SVC(kernel='poly', degree=3, random_state=42)
svm_poly.fit(X_train, y_train)

# Make predictions and calculate accuracy for Polynomial kernel
y_pred_poly = svm_poly.predict(X_test)
accuracy_poly = accuracy_score(y_test, y_pred_poly)

# --- Train the SVM Classifier with an RBF Kernel ---
svm_rbf = SVC(kernel='rbf', random_state=42)
svm_rbf.fit(X_train, y_train)

# Make predictions and calculate accuracy for RBF kernel
y_pred_rbf = svm_rbf.predict(X_test)
accuracy_rbf = accuracy_score(y_test, y_pred_rbf)

# Print the results
print(f"Accuracy of SVM with Linear kernel: {accuracy_linear * 100:.2f}%")
print(f"Accuracy of SVM with Polynomial kernel: {accuracy_poly * 100:.2f}%")
print(f"Accuracy of SVM with RBF kernel: {accuracy_rbf * 100:.2f}%")


18. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy?
# Import necessary libraries
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import StratifiedKFold
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load the Breast Cancer dataset
cancer_data = load_breast_cancer()

# Create a DataFrame for easier handling
X = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)
y = pd.Series(cancer_data.target)

# Initialize StratifiedKFold with 5 splits (you can change the number of splits)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Initialize the SVM classifier
svm_classifier = SVC(kernel='linear', random_state=42)

# List to store accuracy for each fold
accuracies = []

# Perform Stratified K-Fold Cross-Validation
for train_index, test_index in skf.split(X, y):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Train the SVM classifier
    svm_classifier.fit(X_train, y_train)

    # Make predictions
    y_pred = svm_classifier.predict(X_test)

    # Compute accuracy and append to the list
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)

# Calculate and print the average accuracy
average_accuracy = sum(accuracies) / len(accuracies)
print(f"Average Accuracy using Stratified K-Fold Cross-Validation: {average_accuracy * 100:.2f}%")

19.Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare performance?
# Import necessary libraries
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
import numpy as np

# Load the Iris dataset
iris_data = load_iris()

# Create a DataFrame for easier handling
X = iris_data.data
y = iris_data.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# --- Train Naïve Bayes Classifier with default priors ---
nb_default = GaussianNB()
nb_default.fit(X_train, y_train)
y_pred_default = nb_default.predict(X_test)
accuracy_default = accuracy_score(y_test, y_pred_default)

# --- Train Naïve Bayes Classifier with custom priors ---
# Define custom priors (e.g., for class 0, 1, 2)
custom_priors = [0.4, 0.3, 0.3]  # Summing to 1.0
nb_custom = GaussianNB(priors=custom_priors)
nb_custom.fit(X_train, y_train)
y_pred_custom = nb_custom.predict(X_test)
accuracy_custom = accuracy_score(y_test, y_pred_custom)

# --- Train Naïve Bayes Classifier with inverted priors ---
# Inverted priors (e.g., for class 0, 1, 2)
inverted_priors = [0.1, 0.3, 0.6]  # Summing to 1.0
nb_inverted = GaussianNB(priors=inverted_priors)
nb_inverted.fit(X_train, y_train)
y_pred_inverted = nb_inverted.predict(X_test)
accuracy_inverted = accuracy_score(y_test, y_pred_inverted)

# Print the results
print(f"Accuracy with default priors: {accuracy_default * 100:.2f}%")
print(f"Accuracy with custom priors: {accuracy_custom * 100:.2f}%")
print(f"Accuracy with inverted priors: {accuracy_inverted * 100:.2f}%")


# Import necessary libraries
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.feature_selection import RFE
from sklearn.metrics import accuracy_score

# Load the Breast Cancer dataset
cancer_data = load_breast_cancer()

# Create a DataFrame for easier handling
X = cancer_data.data
y = cancer_data.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# --- Train SVM Classifier without RFE ---
svm_classifier = SVC(kernel='linear', random_state=42)
svm_classifier.fit(X_train, y_train)
y_pred_no_rfe = svm_classifier.predict(X_test)
accuracy_no_rfe = accuracy_score(y_test, y_pred_no_rfe)

# --- Perform RFE for feature selection ---
# Create the RFE object with the SVM classifier and choose to select the top 10 features
rfe = RFE(estimator=SVC(kernel='linear'), n_features_to_select=10)
X_train_rfe = rfe.fit_transform(X_train, y_train)
X_test_rfe = rfe.transform(X_test)

# Train SVM Classifier with selected features
svm_rfe_classifier = SVC(kernel='linear', random_state=42)
svm_rfe_classifier.fit(X_train_rfe, y_train)
y_pred_rfe = svm_rfe_classifier.predict(X_test_rfe)
accuracy_rfe = accuracy_score(y_test, y_pred_rfe)

# Print the results
print(f"Accuracy without RFE: {accuracy_no_rfe * 100:.2f}%")
print(f"Accuracy with RFE: {accuracy_rfe * 100:.2f}%")



# Import necessary libraries
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import precision_score, recall_score, f1_score

# Load the Breast Cancer dataset
cancer_data = load_breast_cancer()

# Create a DataFrame for easier handling
X = cancer_data.data
y = cancer_data.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train an SVM Classifier (linear kernel)
svm_classifier = SVC(kernel='linear', random_state=42)
svm_classifier.fit(X_train, y_train)

# Make predictions
y_pred = svm_classifier.predict(X_test)

# Calculate Precision, Recall, and F1-Score
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print the results
print(f"Precision: {precision * 100:.2f}%")
print(f"Recall: {recall * 100:.2f}%")
print(f"F1-Score: {f1 * 100:.2f}%")


# Import necessary libraries
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import log_loss
import numpy as np

# Load the Iris dataset
iris_data = load_iris()

# Create a DataFrame for easier handling
X = iris_data.data
y = iris_data.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a Naïve Bayes classifier (Gaussian Naive Bayes)
nb_classifier = GaussianNB()
nb_classifier.fit(X_train, y_train)

# Make predictions (we need probabilities for Log Loss)
y_pred_prob = nb_classifier.predict_proba(X_test)

# Compute the Log Loss (Cross-Entropy Loss)
loss = log_loss(y_test, y_pred_prob)

# Print the Log Loss result
print(f"Log Loss (Cross-Entropy Loss): {loss:.4f}")


# Import necessary libraries
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix

# Load the Iris dataset
iris_data = load_iris()

# Create a DataFrame for easier handling
X = iris_data.data
y = iris_data.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train an SVM Classifier (linear kernel)
svm_classifier = SVC(kernel='linear', random_state=42)
svm_classifier.fit(X_train, y_train)

# Make predictions
y_pred = svm_classifier.predict(X_test)

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Visualize the confusion matrix using seaborn heatmap
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris_data.target_names, yticklabels=iris_data.target_names)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()


# Import necessary libraries
import numpy as np
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error

# Load the California Housing dataset
housing_data = fetch_california_housing()

# Extract features and target variable
X = housing_data.data
y = housing_data.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train an SVM Regressor (SVR) with a radial basis function kernel
svr_model = SVR(kernel='rbf')
svr_model.fit(X_train, y_train)

# Make predictions
y_pred = svr_model.predict(X_test)

# Calculate Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred)

# Print the MAE result
print(f"Mean Absolute Error (MAE): {mae:.4f}")


# Import necessary libraries
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import roc_auc_score, roc_curve
import matplotlib.pyplot as plt

# Load the Breast Cancer dataset
cancer_data = load_breast_cancer()

# Extract features and target variable
X = cancer_data.data
y = cancer_data.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a Naïve Bayes classifier (Gaussian Naïve Bayes)
nb_classifier = GaussianNB()
nb_classifier.fit(X_train, y_train)

# Predict probabilities for the test set (needed for ROC-AUC)
y_pred_prob = nb_classifier.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class

# Compute the ROC-AUC score
roc_auc = roc_auc_score(y_test, y_pred_prob)

# Print the ROC-AUC score
print(f"ROC-AUC Score: {roc_auc:.4f}")

# Plot the ROC curve
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='red', linestyle='--')  # Diagonal line for random classifier
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.show()

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import precision_recall_curve, average_precision_score

# Load the Breast Cancer dataset
cancer_data = load_breast_cancer()

# Extract features and target variable
X = cancer_data.data
y = cancer_data.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train an SVM Classifier with a linear kernel
svm_classifier = SVC(kernel='linear', probability=True, random_state=42)
svm_classifier.fit(X_train, y_train)

# Predict probabilities for the test set (needed for Precision-Recall Curve)
y_pred_prob = svm_classifier.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class

# Compute the Precision-Recall curve
precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)

# Compute the Average Precision Score
avg_precision = average_precision_score(y_test, y_pred_prob)

# Plot the Precision-Recall curve
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, color='blue', labe