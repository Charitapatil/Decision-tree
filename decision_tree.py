# -*- coding: utf-8 -*-
"""Decision Tree.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QBULlNfFQ-yJId07IYL6cks7hnA36m-S
"""

Theoretical

1. What is a Decision Tree, and how does it work?
A Decision Tree is a type of supervised machine learning algorithm used for classification and regression tasks. It works by breaking down a dataset into smaller subsets while simultaneously developing an associated decision tree model. The tree structure consists of nodes, branches, and leaves:

Components of a Decision Tree:
Root Node:

Represents the entire dataset.
Splits into two or more branches based on the most significant feature (the best splitting criterion).
Decision Nodes:

Intermediate nodes where a decision (split) is made based on certain conditions.
Leaf Nodes:

Represent the final output or decision (e.g., a class label for classification tasks or a predicted value for regression tasks).
Branches:

Connect nodes and represent outcomes of a decision (e.g., "Yes" or "No" for binary splits).
How it Works:
Feature Selection:

The algorithm evaluates each feature to determine the "best split" based on a metric like:
Gini Impurity or Entropy (for classification tasks).
Mean Squared Error (MSE) (for regression tasks).
The goal is to minimize impurity or error at each split.
Splitting:

The dataset is split into subsets based on the selected feature and its threshold or condition (e.g., Age > 30).
This process is repeated recursively.
Stopping Criteria:

Splitting stops when a predefined condition is met, such as:
Maximum tree depth is reached.
Minimum number of samples in a node is below a threshold.
No significant gain in impurity reduction.
Prediction:

Once the tree is built, it can classify a new input by traversing the tree from the root to a leaf node, following the conditions at each node.
Example (Classification):
For a dataset predicting whether someone buys a product:

Root Node: "Is Age > 30?"
Yes Branch: "Income > $50K?"
Yes: "Buy" (Leaf)
No: "Don't Buy" (Leaf)
No Branch: "Don't Buy" (Leaf)
For a new person aged 32 with income $60K, the tree predicts "Buy."

Strengths:
Easy to interpret and visualize.
Handles both numerical and categorical data.
Requires little preprocessing (no need for scaling/normalization).
Weaknesses:
Can overfit the data if the tree is too deep.
Sensitive to small variations in the data.
May not perform well compared to more complex algorithms like Random Forests or Gradient Boosted Trees.

2. What are impurity measures in Decision Trees?
Impurity measures in Decision Trees quantify the "messiness" or "disorder" of a node, helping the algorithm determine the best way to split the data. The goal is to reduce impurity as much as possible at each split, creating purer nodes where the data is more homogeneous (e.g., all examples belong to the same class).

Common Impurity Measures:
1. Gini Impurity:
Used in classification tasks.
Measures the probability of incorrectly classifying a randomly chosen sample if it were labeled according to the class distribution in the node.
Formula:

𝐺
𝑖
𝑛
𝑖
=
1
−
∑
𝑖
=
1
𝐶
𝑝
𝑖
2
Gini=1−
i=1
∑
C
​
 p
i
2
​

Where:

𝐶
C = number of classes.
𝑝
𝑖
p
i
​
  = proportion of samples in class
𝑖
i at the node.
Characteristics:

Ranges between 0 (pure node, all samples belong to one class) and a maximum value (impure node, uniform distribution of classes).
Splits that reduce Gini Impurity the most are preferred.
2. Entropy (Information Gain):
Also used in classification tasks, derived from Information Theory.
Measures the uncertainty or randomness in the node's data distribution.
Formula:

𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
=
−
∑
𝑖
=
1
𝐶
𝑝
𝑖
log
⁡
2
(
𝑝
𝑖
)
Entropy=−
i=1
∑
C
​
 p
i
​
 log
2
​
 (p
i
​
 )
Where:

𝑝
𝑖
p
i
​
  = proportion of samples in class
𝑖
i.
Information Gain:

Measures the reduction in entropy after a split.
𝐼
𝑛
𝑓
𝑜
𝑟
𝑚
𝑎
𝑡
𝑖
𝑜
𝑛

𝐺
𝑎
𝑖
𝑛
=
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
𝑝
𝑎
𝑟
𝑒
𝑛
𝑡
−
∑
𝑗
=
1
𝑘
𝑛
𝑗
𝑛
⋅
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
𝑐
ℎ
𝑖
𝑙
𝑑
𝑗
Information Gain=Entropy
parent
​
 −
j=1
∑
k
​

n
n
j
​

​
 ⋅Entropy
child
j
​

​

Where:

𝑛
n = total samples in the parent node.
𝑛
𝑗
n
j
​
  = samples in child node
𝑗
j.
3. Mean Squared Error (MSE):
Used in regression tasks.
Measures the variance of the target variable within a node.
Formula:

𝑀
𝑆
𝐸
=
1
𝑛
∑
𝑖
=
1
𝑛
(
𝑦
𝑖
−
𝑦
ˉ
)
2
MSE=
n
1
​

i=1
∑
n
​
 (y
i
​
 −
y
ˉ
​
 )
2

Where:

𝑦
𝑖
y
i
​
  = target value of the
𝑖
i-th sample.
𝑦
ˉ
y
ˉ
​
  = mean of target values in the node.
𝑛
n = number of samples in the node.
Characteristics:

Splits that minimize the MSE lead to purer nodes with less variance.
4. Variance Reduction:
Another measure for regression tasks, closely related to MSE.
Assesses the reduction in variance between parent and child nodes.
Formula:

𝑉
𝑎
𝑟
𝑖
𝑎
𝑛
𝑐
𝑒

𝑅
𝑒
𝑑
𝑢
𝑐
𝑡
𝑖
𝑜
𝑛
=
𝑉
𝑎
𝑟
𝑖
𝑎
𝑛
𝑐
𝑒
𝑝
𝑎
𝑟
𝑒
𝑛
𝑡
−
∑
𝑗
=
1
𝑘
𝑛
𝑗
𝑛
⋅
𝑉
𝑎
𝑟
𝑖
𝑎
𝑛
𝑐
𝑒
𝑐
ℎ
𝑖
𝑙
𝑑
𝑗
Variance Reduction=Variance
parent
​
 −
j=1
∑
k
​

n
n
j
​

​
 ⋅Variance
child
j
​

​

Choosing the Best Split:
The algorithm evaluates potential splits using these measures.
For classification, Gini Impurity or Entropy are common.
For regression, MSE or Variance Reduction is used.
The split that results in the greatest impurity reduction is chosen.

The mathematical formula for Gini Impurity is:

𝐺
𝑖
𝑛
𝑖
=
1
−
∑
𝑖
=
1
𝐶
𝑝
𝑖
2
Gini=1−
i=1
∑
C
​
 p
i
2
​

Where:

𝐶
C: The total number of classes in the dataset.
𝑝
𝑖
p
i
​
 : The proportion (probability) of samples belonging to class
𝑖
i in the node.
Explanation:
𝑝
𝑖
p
i
​
 : Calculated as
Number of samples in class
𝑖
Total number of samples in the node
Total number of samples in the node
Number of samples in class i
​
 .
𝑝
𝑖
2
p
i
2
​
 : Represents the squared probability of selecting a sample from class
𝑖
i.
∑
𝑖
=
1
𝐶
𝑝
𝑖
2
∑
i=1
C
​
 p
i
2
​
 : The sum of squared probabilities for all classes.
Higher values mean the node is purer (dominated by one class).
1
−
∑
𝑖
=
1
𝐶
𝑝
𝑖
2
1−∑
i=1
C
​
 p
i
2
​
 : Subtracting this sum from 1 gives the Gini Impurity, representing the chance of incorrect classification.
Characteristics of Gini Impurity:
Range:
𝐺
𝑖
𝑛
𝑖
=
0
Gini=0: Perfectly pure node (all samples belong to one class).
𝐺
𝑖
𝑛
𝑖
Gini close to 0.5: Impure node (samples are evenly distributed among classes in a binary split).
𝐺
𝑖
𝑛
𝑖
Gini approaches
1
−
1
𝐶
1−
C
1
​
  for maximum impurity with
𝐶
C classes.
Split Preference: Decision Trees prefer splits that minimize Gini Impurity, as they lead to purer child nodes.

3. What is the mathematical formula for Entropy?
The mathematical formula for Entropy in the context of decision trees is:

𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
=
−
∑
𝑖
=
1
𝐶
𝑝
𝑖
⋅
log
⁡
2
(
𝑝
𝑖
)
Entropy=−
i=1
∑
C
​
 p
i
​
 ⋅log
2
​
 (p
i
​
 )
Where:

𝐶
C: The total number of classes in the dataset.
𝑝
𝑖
p
i
​
 : The proportion (probability) of samples belonging to class
𝑖
i in the node.
Explanation:
𝑝
𝑖
p
i
​
 :

Calculated as
Number of samples in class
𝑖
Total number of samples in the node
Total number of samples in the node
Number of samples in class i
​
 .
𝑝
𝑖
p
i
​
  is a value between 0 and 1.
log
⁡
2
(
𝑝
𝑖
)
log
2
​
 (p
i
​
 ):

Measures the "information content" or uncertainty for class
𝑖
i.
If
𝑝
𝑖
=
1
p
i
​
 =1 (pure class),
log
⁡
2
(
𝑝
𝑖
)
=
0
log
2
​
 (p
i
​
 )=0, meaning no uncertainty.
If
𝑝
𝑖
=
0
p
i
​
 =0, we define
𝑝
𝑖
⋅
log
⁡
2
(
𝑝
𝑖
)
=
0
p
i
​
 ⋅log
2
​
 (p
i
​
 )=0 (using the convention that
0
⋅
log
⁡
(
0
)
=
0
0⋅log(0)=0).
−
∑
𝑖
=
1
𝐶
𝑝
𝑖
⋅
log
⁡
2
(
𝑝
𝑖
)
−∑
i=1
C
​
 p
i
​
 ⋅log
2
​
 (p
i
​
 ):

The negative sign ensures entropy is non-negative.
The sum aggregates the uncertainty for all classes in the node.
Characteristics of Entropy:
Range:

𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
=
0
Entropy=0: The node is pure (all samples belong to a single class).
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
Entropy is maximum when samples are evenly distributed among all
𝐶
C classes.
For binary classification,
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
=
1
Entropy=1 when
𝑝
1
=
𝑝
2
=
0.5
p
1
​
 =p
2
​
 =0.5.
Split Preference:

Decision Trees aim to maximize Information Gain, which is the reduction in Entropy after a split.
Example:
Suppose we have a node with two classes:

Class 1: 4 samples.
Class 2: 6 samples.
Total samples: 10.
𝑝
1
=
4
10
=
0.4
p
1
​
 =
10
4
​
 =0.4,
𝑝
2
=
6
10
=
0.6
p
2
​
 =
10
6
​
 =0.6.
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
=
−
[
0.4
⋅
log
⁡
2
(
0.4
)
+
0.6
⋅
log
⁡
2
(
0.6
)
]
Entropy=−[0.4⋅log
2
​
 (0.4)+0.6⋅log
2
​
 (0.6)].
Using
log
⁡
2
(
0.4
)
≈
−
1.322
log
2
​
 (0.4)≈−1.322 and
log
⁡
2
(
0.6
)
≈
−
0.737
log
2
​
 (0.6)≈−0.737:
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
=
−
[
0.4
⋅
(
−
1.322
)
+
0.6
⋅
(
−
0.737
)
]
Entropy=−[0.4⋅(−1.322)+0.6⋅(−0.737)]
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
=
0.5288
+
0.4422
=
0.971
Entropy=0.5288+0.4422=0.971
The Entropy for this node is approximately 0.971.

Information Gain (IG) is a metric used in Decision Trees to evaluate how well a feature (or split) separates the data into distinct classes. It measures the reduction in uncertainty (or entropy) about the target variable after splitting the data based on a feature.

Mathematical Formula:
𝐼
𝑛
𝑓
𝑜
𝑟
𝑚
𝑎
𝑡
𝑖
𝑜
𝑛

𝐺
𝑎
𝑖
𝑛
=
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
𝑝
𝑎
𝑟
𝑒
𝑛
𝑡
−
∑
𝑗
=
1
𝑘
𝑛
𝑗
𝑛
⋅
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
𝑐
ℎ
𝑖
𝑙
𝑑
𝑗
Information Gain=Entropy
parent
​
 −
j=1
∑
k
​

n
n
j
​

​
 ⋅Entropy
child
j
​

​

Where:

𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
𝑝
𝑎
𝑟
𝑒
𝑛
𝑡
Entropy
parent
​
 : The entropy of the original dataset (before splitting).
𝑘
k: The number of child nodes after the split.
𝑛
n: Total number of samples in the parent node.
𝑛
𝑗
n
j
​
 : Number of samples in child node
𝑗
j.
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
𝑐
ℎ
𝑖
𝑙
𝑑
𝑗
Entropy
child
j
​

​
 : The entropy of child node
𝑗
j.
How It Works:
Before Splitting:

Compute the entropy of the parent node (
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
𝑝
𝑎
𝑟
𝑒
𝑛
𝑡
Entropy
parent
​
 ).
This quantifies the uncertainty in the target variable for the entire dataset.
After Splitting:

For each potential split:
Divide the data into child nodes based on the feature's values.
Compute the entropy for each child node (
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
𝑐
ℎ
𝑖
𝑙
𝑑
𝑗
Entropy
child
j
​

​
 ).
Calculate the weighted average of these entropies, based on the proportion of samples in each child node.
Information Gain:

Subtract the weighted average entropy of the child nodes from the entropy of the parent node.
Higher Information Gain indicates a better split, as it leads to purer child nodes (less uncertainty).
Key Intuition:
A high Information Gain means the split has greatly reduced uncertainty, creating child nodes that are closer to being pure (all samples in a node belong to the same class).
Features with the highest Information Gain are chosen for splitting at each step of building the tree.
Example:
Suppose a dataset has 10 samples:

6
6 belong to Class A.
4
4 belong to Class B.
1. Compute
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
𝑝
𝑎
𝑟
𝑒
𝑛
𝑡
Entropy
parent
​
 :
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
𝑝
𝑎
𝑟
𝑒
𝑛
𝑡
=
−
(
6
10
log
⁡
2
6
10
+
4
10
log
⁡
2
4
10
)
Entropy
parent
​
 =−(
10
6
​
 log
2
​

10
6
​
 +
10
4
​
 log
2
​

10
4
​
 )
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
𝑝
𝑎
𝑟
𝑒
𝑛
𝑡
=
−
(
0.6
⋅
−
0.737
+
0.4
⋅
−
1.322
)
=
0.971
Entropy
parent
​
 =−(0.6⋅−0.737+0.4⋅−1.322)=0.971
2. After Splitting:
Split the dataset into two child nodes based on a feature:

Child Node 1:
4
4 samples (
3
3 Class A,
1
1 Class B).
Child Node 2:
6
6 samples (
3
3 Class A,
3
3 Class B).
Compute
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
𝑐
ℎ
𝑖
𝑙
𝑑
1
Entropy
child
1
​

​
 :
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
𝑐
ℎ
𝑖
𝑙
𝑑
1
=
−
(
3
4
log
⁡
2
3
4
+
1
4
log
⁡
2
1
4
)
=
0.811
Entropy
child
1
​

​
 =−(
4
3
​
 log
2
​

4
3
​
 +
4
1
​
 log
2
​

4
1
​
 )=0.811
Compute
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
𝑐
ℎ
𝑖
𝑙
𝑑
2
Entropy
child
2
​

​
 :
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
𝑐
ℎ
𝑖
𝑙
𝑑
2
=
−
(
3
6
log
⁡
2
3
6
+
3
6
log
⁡
2
3
6
)
=
1.000
Entropy
child
2
​

​
 =−(
6
3
​
 log
2
​

6
3
​
 +
6
3
​
 log
2
​

6
3
​
 )=1.000
3. Compute Weighted Average Entropy of Child Nodes:
𝑊
𝑒
𝑖
𝑔
ℎ
𝑡
𝑒
𝑑

𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
=
4
10
⋅
0.811
+
6
10
⋅
1.000
=
0.911
Weighted Entropy=
10
4
​
 ⋅0.811+
10
6
​
 ⋅1.000=0.911
4. Compute Information Gain:
𝐼
𝑛
𝑓
𝑜
𝑟
𝑚
𝑎
𝑡
𝑖
𝑜
𝑛

𝐺
𝑎
𝑖
𝑛
=
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
𝑝
𝑎
𝑟
𝑒
𝑛
𝑡
−
𝑊
𝑒
𝑖
𝑔
ℎ
𝑡
𝑒
𝑑

𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
=
0.971
−
0.911
=
0.060
Information Gain=Entropy
parent
​
 −Weighted Entropy=0.971−0.911=0.060
Usage in Decision Trees:
At each step of building the tree, the feature with the highest Information Gain is selected to split the data.
This ensures that the tree grows in a way that reduces uncertainty about the target variable most effectively.

Gini Impurity and Entropy are both impurity measures used in Decision Trees to evaluate the quality of a split. While they serve the same purpose (to determine the "purity" of nodes), they differ in how they calculate impurity and in their characteristics.

Key Differences Between Gini Impurity and Entropy
Aspect	Gini Impurity	Entropy
Formula
𝐺
𝑖
𝑛
𝑖
=
1
−
∑
𝑖
=
1
𝐶
𝑝
𝑖
2
Gini=1−∑
i=1
C
​
 p
i
2
​

𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
=
−
∑
𝑖
=
1
𝐶
𝑝
𝑖
log
⁡
2
(
𝑝
𝑖
)
Entropy=−∑
i=1
C
​
 p
i
​
 log
2
​
 (p
i
​
 )
Interpretation	Measures the probability of misclassification if a sample is randomly assigned a class.	Measures the uncertainty or randomness in the class distribution.
Range
0
≤
𝐺
𝑖
𝑛
𝑖
≤
𝐶
−
1
𝐶
0≤Gini≤
C
C−1
​

0
≤
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
≤
log
⁡
2
(
𝐶
)
0≤Entropy≤log
2
​
 (C)
Value at Purity	Gini = 0 (when all samples belong to one class).	Entropy = 0 (when all samples belong to one class).
Value at Maximum Impurity	Gini approaches
1
−
1
𝐶
1−
C
1
​
  when all classes are evenly distributed.	Entropy is maximum (
log
⁡
2
(
𝐶
)
log
2
​
 (C)) when all classes are evenly distributed.
Sensitivity to Class Distribution	Slightly less sensitive to small changes in class probabilities.	More sensitive to small changes in class probabilities.
Computational Complexity	Easier to compute since it avoids logarithmic calculations.	More computationally expensive due to the logarithm.
Use Case Preference	Commonly used in CART (Classification and Regression Trees) due to faster computation.	Commonly used in ID3, C4.5, and C5.0 algorithms for classification tasks.
Detailed Comparison
Behavior with Pure Nodes:

When a node contains samples of only one class, both Gini and Entropy reach their minimum value (0).
Behavior with Impure Nodes:

For a binary classification problem:
Gini Impurity is maximized when both classes are equally likely (
𝑝
1
=
𝑝
2
=
0.5
p
1
​
 =p
2
​
 =0.5), resulting in
𝐺
𝑖
𝑛
𝑖
=
0.5
Gini=0.5.
Entropy is also maximized when both classes are equally likely, resulting in
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
=
1
Entropy=1.
Impact of Class Probability Changes:

Gini Impurity is less sensitive to small changes in class probabilities, making it slightly more robust.
Entropy is more sensitive, meaning it penalizes impurity more heavily, especially when class probabilities are closer to each other.
Computational Efficiency:

Gini Impurity is computationally faster as it avoids logarithmic calculations.
Entropy involves
log
⁡
2
log
2
​
  calculations, which can add computational overhead.
When to Use Gini Impurity vs. Entropy
Gini Impurity:

Often used in practical applications (e.g., Scikit-learn uses Gini as the default criterion for Decision Trees).
Preferred when speed is a priority, especially for large datasets.
Entropy:

Used when interpretability of the "information gain" is more important.
Preferred in cases where understanding the reduction in randomness or uncertainty is critical.
Example Comparison:
Suppose a node contains:

40 samples of Class A.
60 samples of Class B.
Probabilities:
𝑝
𝐴
=
0.4
,
𝑝
𝐵
=
0.6
p
A
​
 =0.4,p
B
​
 =0.6.

Gini Impurity:

𝐺
𝑖
𝑛
𝑖
=
1
−
(
0.4
2
+
0.6
2
)
=
1
−
(
0.16
+
0.36
)
=
0.48
Gini=1−(0.4
2
 +0.6
2
 )=1−(0.16+0.36)=0.48
Entropy:

𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
=
−
(
0.4
⋅
log
⁡
2
(
0.4
)
+
0.6
⋅
log
⁡
2
(
0.6
)
)
Entropy=−(0.4⋅log
2
​
 (0.4)+0.6⋅log
2
​
 (0.6))
𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
=
−
(
0.4
⋅
−
1.322
+
0.6
⋅
−
0.737
)
=
0.5288
+
0.4422
=
0.971
Entropy=−(0.4⋅−1.322+0.6⋅−0.737)=0.5288+0.4422=0.971
Both measures indicate impurity, but the values are on different scales.

The mathematical foundation of Decision Trees lies in recursive partitioning, where the dataset is split into subsets based on conditions that maximize some measure of "purity" (e.g., Gini Impurity, Entropy, or Variance Reduction). Here’s the detailed explanation:

1. Objective of Decision Trees
The goal is to divide the dataset into smaller and smaller subsets (nodes) such that:

For classification: Each node contains samples that mostly belong to a single class.
For regression: Each node minimizes the variance of the target values.
At each step, the tree identifies the feature and threshold (or category) that results in the best split.

2. Steps of Decision Tree Construction
Step 1: Select the Best Feature to Split
The algorithm evaluates all available features and their possible split points (for numerical features) or categories (for categorical features). The "best" split is the one that maximizes the Information Gain or minimizes the Impurity.

For a feature
𝑋
X:

For classification: Use measures like Entropy, Gini Impurity, or Information Gain.
For regression: Use measures like Mean Squared Error (MSE) or Variance Reduction.
Step 2: Splitting the Data
The dataset is divided into two or more child nodes based on the split decision. For example:

For a numerical feature
𝑋
X: Use a threshold
𝑡
t such that
𝑋
<
𝑡
X<t goes to one child node and
𝑋
≥
𝑡
X≥t to another.
For a categorical feature: Assign categories to different child nodes.
Step 3: Stopping Criteria
The recursion stops when one of the following conditions is met:

A node becomes pure (all samples belong to one class for classification).
A maximum tree depth is reached.
A minimum number of samples per node is reached.
The reduction in impurity becomes negligible.
Mathematical Explanation
Classification Trees
For a dataset
𝐷
D with
𝑁
N samples and
𝐶
C classes:

Entropy:

𝐸
𝑛
𝑡
𝑟
𝑜
𝑝
𝑦
(
𝐷
)
=
−
∑
𝑖
=
1
𝐶
𝑝
𝑖
⋅
log
⁡
2
(
𝑝
𝑖
)
Entropy(D)=−
i=1
∑
C
​
 p
i
​
 ⋅log
2
​
 (p
i
​
 )
Where
𝑝
𝑖
p
i
​
  is the proportion of samples in class
𝑖
i.

Gini Impurity:

𝐺
𝑖
𝑛
𝑖
(
𝐷
)
=
1
−
∑
𝑖
=
1
𝐶
𝑝
𝑖
2
Gini(D)=1−
i=1
∑
C
​
 p
i
2
​

Information Gain:

𝐼
𝐺
=
𝐼
𝑚
𝑝
𝑢
𝑟
𝑖
𝑡
𝑦
𝑝
𝑎
𝑟
𝑒
𝑛
𝑡
−
∑
𝑗
=
1
𝑘
𝑛
𝑗
𝑛
⋅
𝐼
𝑚
𝑝
𝑢
𝑟
𝑖
𝑡
𝑦
𝑐
ℎ
𝑖
𝑙
𝑑
𝑗
IG=Impurity
parent
​
 −
j=1
∑
k
​

n
n
j
​

​
 ⋅Impurity
child
j
​

​

Where:

𝐼
𝑚
𝑝
𝑢
𝑟
𝑖
𝑡
𝑦
𝑝
𝑎
𝑟
𝑒
𝑛
𝑡
Impurity
parent
​
 : Impurity before the split.
𝐼
𝑚
𝑝
𝑢
𝑟
𝑖
𝑡
𝑦
𝑐
ℎ
𝑖
𝑙
𝑑
𝑗
Impurity
child
j
​

​
 : Impurity in child node
𝑗
j.
𝑛
n: Total samples in the parent node.
𝑛
𝑗
n
j
​
 : Samples in child node
𝑗
j.
The algorithm selects the split that maximizes
𝐼
𝐺
IG.

Regression Trees
For a dataset
𝐷
D with
𝑁
N samples and target values
{
𝑦
1
,
𝑦
2
,
.
.
.
,
𝑦
𝑁
}
{y
1
​
 ,y
2
​
 ,...,y
N
​
 }:

Mean Squared Error (MSE):

𝑀
𝑆
𝐸
(
𝐷
)
=
1
𝑁
∑
𝑖
=
1
𝑁
(
𝑦
𝑖
−
𝑦
ˉ
)
2
MSE(D)=
N
1
​

i=1
∑
N
​
 (y
i
​
 −
y
ˉ
​
 )
2

Where
𝑦
ˉ
y
ˉ
​
  is the mean of target values in
𝐷
D.

Variance Reduction:

𝑉
𝑎
𝑟
𝑖
𝑎
𝑛
𝑐
𝑒

𝑅
𝑒
𝑑
𝑢
𝑐
𝑡
𝑖
𝑜
𝑛
=
𝑉
𝑎
𝑟
𝑖
𝑎
𝑛
𝑐
𝑒
𝑝
𝑎
𝑟
𝑒
𝑛
𝑡
−
∑
𝑗
=
1
𝑘
𝑛
𝑗
𝑛
⋅
𝑉
𝑎
𝑟
𝑖
𝑎
𝑛
𝑐
𝑒
𝑐
ℎ
𝑖
𝑙
𝑑
𝑗
Variance Reduction=Variance
parent
​
 −
j=1
∑
k
​

n
n
j
​

​
 ⋅Variance
child
j
​

​

The algorithm selects the split that minimizes MSE or maximizes Variance Reduction.

Tree Prediction
Classification: Assign a class label to a new sample by traversing the tree based on feature values until reaching a leaf node. The leaf node predicts the majority class.
Regression: Predict the target value as the average (or weighted average) of the target values in the leaf node.
Mathematical Properties
Recursive Partitioning: Each split divides the feature space into smaller hyperrectangles. For
𝑑
d-dimensional data, the splits form a hierarchical partition of
𝑅
𝑑
R
d
 .

Non-Parametric Nature: Decision Trees are non-parametric, meaning they do not assume any underlying distribution of the data.

Optimization:

The splitting process optimizes a greedy algorithm: it makes the best split at each step without considering future splits.
Computational Complexity:

Splitting: Evaluating all features and thresholds is
𝑂
(
𝑛
⋅
𝑚
)
O(n⋅m), where
𝑛
n is the number of samples and
𝑚
m is the number of features.
Prediction: Traversing a tree is
𝑂
(
tree depth
)
O(tree depth).

Pre-Pruning, also known as early stopping, is a technique used in Decision Trees to limit their growth during construction. The main objective is to prevent the tree from becoming overly complex and overfitting the training data. Pre-pruning imposes constraints or conditions on the tree growth, stopping it from further splitting when certain criteria are met.

How Pre-Pruning Works
During the tree-building process, the algorithm evaluates whether a split should occur at a node. If a pre-defined stopping condition is met, the splitting process halts, and the node becomes a leaf node, predicting the majority class (for classification) or the mean value (for regression) of the samples in that node.

Common Pre-Pruning Criteria
Pre-pruning uses thresholds to decide when to stop splitting. Some common criteria include:

Maximum Depth:

The tree is allowed to grow only up to a specified depth (
𝑚
𝑎
𝑥
_
𝑑
𝑒
𝑝
𝑡
ℎ
max_depth).
Limits the number of splits from the root to any leaf.
Minimum Number of Samples per Split:

Splitting is allowed only if the number of samples in a node is greater than a threshold (
𝑚
𝑖
𝑛
_
𝑠
𝑎
𝑚
𝑝
𝑙
𝑒
𝑠
_
𝑠
𝑝
𝑙
𝑖
𝑡
min_samples_split).
Prevents splitting nodes with too few samples.
Minimum Number of Samples in a Leaf Node:

Ensures that each leaf node contains at least a minimum number of samples (
𝑚
𝑖
𝑛
_
𝑠
𝑎
𝑚
𝑝
𝑙
𝑒
𝑠
_
𝑙
𝑒
𝑎
𝑓
min_samples_leaf).
Minimum Information Gain:

Splitting occurs only if the Information Gain or Reduction in Impurity exceeds a certain threshold (
𝑚
𝑖
𝑛
_
𝑖
𝑚
𝑝
𝑢
𝑟
𝑖
𝑡
𝑦
_
𝑑
𝑒
𝑐
𝑟
𝑒
𝑎
𝑠
𝑒
min_impurity_decrease).
Prevents splits that do not significantly improve the tree.
Maximum Number of Leaf Nodes:

Limits the total number of leaf nodes in the tree (
𝑚
𝑎
𝑥
_
𝑙
𝑒
𝑎
𝑓
_
𝑛
𝑜
𝑑
𝑒
𝑠
max_leaf_nodes).
Maximum Features to Consider:

Restricts the number of features evaluated for splitting at each node (
𝑚
𝑎
𝑥
_
𝑓
𝑒
𝑎
𝑡
𝑢
𝑟
𝑒
𝑠
max_features).
Advantages of Pre-Pruning
Prevents Overfitting:

Stops the tree from growing too complex, which reduces the likelihood of overfitting to training data.
Reduces Computational Cost:

Stops unnecessary splits early, reducing both training time and model size.
Simplifies the Model:

Results in smaller, more interpretable trees.
Disadvantages of Pre-Pruning
Risk of Underfitting:

By halting the growth too early, the tree might not capture the full complexity of the data.
Important splits might be missed, reducing the model's accuracy.
Hard to Tune:

Finding the optimal pre-pruning parameters (e.g.,
𝑚
𝑖
𝑛
_
𝑠
𝑎
𝑚
𝑝
𝑙
𝑒
𝑠
_
𝑠
𝑝
𝑙
𝑖
𝑡
min_samples_split,
𝑚
𝑎
𝑥
_
𝑑
𝑒
𝑝
𝑡
ℎ
max_depth) can require extensive experimentation and validation.
Example of Pre-Pruning
Let’s say we have a Decision Tree for classifying whether a song is "Popular" or "Not Popular" based on features like Duration and Artist Popularity.

Dataset Size: 500 samples.

Pre-Pruning Conditions:

𝑚
𝑎
𝑥
_
𝑑
𝑒
𝑝
𝑡
ℎ
=
5
max_depth=5: The tree can only grow up to 5 levels.
𝑚
𝑖
𝑛
_
𝑠
𝑎
𝑚
𝑝
𝑙
𝑒
𝑠
_
𝑙
𝑒
𝑎
𝑓
=
10
min_samples_leaf=10: Each leaf must have at least 10 samples.
During tree growth:

At a node with only 8 samples, the split is not allowed because it violates
𝑚
𝑖
𝑛
_
𝑠
𝑎
𝑚
𝑝
𝑙
𝑒
𝑠
_
𝑙
𝑒
𝑎
𝑓
min_samples_leaf.
If the tree reaches a depth of 5, no further splitting occurs regardless of the data.
Comparison with Post-Pruning
Aspect	Pre-Pruning	Post-Pruning
Timing	Stops tree growth during construction.	Removes unnecessary branches after full tree growth.
Objective	Prevents overfitting early.	Simplifies an overfitted tree.
Computational Cost	Lower, as tree growth is restricted.	Higher, as a fully grown tree is pruned.
Risk	May underfit the data by halting too early.	May overfit the training data before pruning.

Post-Pruning, also called Pruning or Reduced Error Pruning, is a technique used in Decision Trees to simplify the model by removing unnecessary branches after the tree has been fully grown. The primary goal is to reduce overfitting and improve the tree's generalization ability on unseen data.

How Post-Pruning Works
Grow the Tree Fully:

A Decision Tree is first grown to its maximum depth, potentially leading to a very complex and overfitted tree.
Prune Back:

Starting from the leaf nodes, branches (subtrees) are removed or replaced if doing so results in improved performance or does not significantly degrade it.
Criteria for Pruning:

A subtree or branch is pruned if:
It does not improve validation accuracy.
It does not sufficiently reduce impurity (e.g., Gini, Entropy).
It does not improve some other evaluation metric (e.g., cross-validation performance).
Leaf Node Replacement:

When a branch is pruned, the node becomes a leaf, predicting the majority class (classification) or the average value (regression) of the samples it contains.
Types of Post-Pruning Techniques
Cost Complexity Pruning (CCP):

Prunes branches based on a trade-off between tree complexity (number of leaf nodes) and performance.
A cost complexity parameter
𝛼
α controls how much penalty is applied for tree size:
𝑅
𝛼
(
𝑇
)
=
𝑅
(
𝑇
)
+
𝛼
⋅
∣
𝑇
∣
R
α
​
 (T)=R(T)+α⋅∣T∣
Where:
𝑅
(
𝑇
)
R(T): Empirical error of tree
𝑇
T (e.g., misclassification error or MSE).
∣
𝑇
∣
∣T∣: Number of leaf nodes in tree
𝑇
T.
𝛼
α: Regularization parameter (higher
𝛼
α leads to more pruning).
Reduced Error Pruning:

Directly evaluates the impact of removing branches on a validation set.
A branch is pruned if removing it does not reduce the accuracy on the validation set.
Minimum Error Pruning:

Prunes the tree to minimize the classification error (for classification tasks) or prediction error (for regression tasks).
Advantages of Post-Pruning
Improves Generalization:

Reduces overfitting by simplifying the tree and removing noise-induced branches.
Better Control Over Complexity:

Allows the tree to fully explore the data initially and then removes unnecessary complexity.
Flexibility:

Works with any fully grown tree and can be combined with various metrics for evaluation.
Disadvantages of Post-Pruning
Computational Overhead:

Requires growing the tree fully first, which can be computationally expensive, especially for large datasets.
Dependency on Validation Data:

Pruning decisions often rely on a validation set, reducing the effective size of the training data.
Example of Post-Pruning
Step-by-Step Explanation
Step 1: Fully grow a Decision Tree, splitting the data until all leaf nodes are pure or the minimum node size is reached.
Step 2: Use a validation set to evaluate the performance of the tree.
Step 3: Start pruning:
Begin with leaf nodes and prune branches if merging them improves or does not degrade validation accuracy.
Step 4: Repeat pruning until the tree's performance stops improving or reaches the desired complexity.
Comparison Between Pre-Pruning and Post-Pruning
Aspect	Pre-Pruning	Post-Pruning
Timing	Stops tree growth early during construction.	Simplifies a fully grown tree by removing branches.
Overfitting	Prevents overfitting during construction.	Removes overfitting after full growth.
Underfitting Risk	Higher risk, as stopping early might miss important splits.	Lower risk, as the tree is initially fully grown.
Computation Cost	Lower, as it avoids growing a large tree.	Higher, as it requires building a full tree first.

Pre-Pruning and Post-Pruning are both methods to control the growth of a Decision Tree and prevent overfitting, but they differ in their timing, approach, and impact on the tree. Here's a detailed comparison:

Key Differences
Aspect	Pre-Pruning	Post-Pruning
Timing	Applied during the construction of the tree.	Applied after the tree is fully grown.
Approach	Stops the tree growth early by imposing constraints.	Prunes branches from a fully grown tree.
Control Criteria	Uses conditions like maximum depth, minimum samples, or minimum impurity improvement to stop splits.	Uses validation data or cost-complexity criteria to prune subtrees.
Complexity of Tree	Produces a smaller tree directly by limiting growth.	Produces a larger initial tree, then simplifies it.
Overfitting Risk	Reduces the risk of overfitting early but may underfit if stopped too soon.	Allows the tree to overfit first, then reduces overfitting.
Computational Cost	Lower, as unnecessary splits are avoided upfront.	Higher, as the tree must first be fully constructed.
Dependency on Data	Does not require a fully grown tree or additional pruning logic.	Relies on validation data or specific pruning algorithms.
Ease of Interpretation	Often easier to control as constraints are applied during growth.	Pruned trees are simplified post-construction, requiring additional steps.
Advantages and Disadvantages
Feature	Pre-Pruning	Post-Pruning
Advantages	- Reduces training time and model size.
- Prevents overfitting early.
- Simple to implement.	- Produces a tree that fully explores the data.
- More robust and flexible.
- Better generalization due to validation-based pruning.
Disadvantages	- Can lead to underfitting if the constraints are too strict.
- May miss important splits.	- Higher computational cost due to growing and then pruning.
- Requires additional data (e.g., validation set).
When to Use Each
Pre-Pruning:

Use when computational efficiency is critical, or when you want to restrict tree complexity upfront (e.g., real-time systems).
Useful for datasets where overfitting is less of a concern or when the structure of the tree needs to be simple.
Post-Pruning:

Use when you want a more accurate model and are okay with additional computational cost.
Suitable when you have validation data to evaluate and prune the tree for better generalization.
Example Scenario
Pre-Pruning:
You set
𝑚
𝑎
𝑥
_
𝑑
𝑒
𝑝
𝑡
ℎ
=
5
max_depth=5 and
𝑚
𝑖
𝑛
_
𝑠
𝑎
𝑚
𝑝
𝑙
𝑒
𝑠
_
𝑠
𝑝
𝑙
𝑖
𝑡
=
10
min_samples_split=10 while building a Decision Tree. The tree stops growing at depth 5 or when a node has fewer than 10 samples, ensuring that the tree is simple and computationally efficient.
Post-Pruning:
You grow the tree fully with no constraints. Afterward, you use validation data and prune unnecessary branches that do not improve accuracy, creating a simpler and better-generalized model.
Conclusion
Pre-Pruning: Stops unnecessary splits early, but risks underfitting.
Post-Pruning: Optimizes a fully grown tree by removing redundant complexity, but is computationally expensive.

A Decision Tree Regressor is a type of Decision Tree model used for regression tasks, where the goal is to predict a continuous numeric value (e.g., price, temperature, sales, etc.). Unlike a Decision Tree for classification, which assigns samples to discrete categories, a Decision Tree Regressor predicts the target as a numeric value based on the input features.

How a Decision Tree Regressor Works
Recursive Splitting:

The dataset is recursively split into subsets based on input features to minimize a specific loss function (e.g., mean squared error or mean absolute error).
At each node, the algorithm identifies the feature and split point that best reduces the impurity (e.g., variance or error) of the target variable.
Leaf Nodes:

Once splitting stops (based on stopping criteria such as tree depth, minimum samples per split, or minimum impurity), the remaining data points in a leaf node are used to compute the predicted value.
The predicted value is typically the mean (or median) of the target values in the leaf node.
Prediction:

For a new input sample, the tree is traversed from the root to a leaf based on the feature values of the sample.
The prediction for the sample is the value of the leaf node reached.
Splitting Criteria in Decision Tree Regression
Instead of using metrics like Gini Impurity or Entropy (used in classification trees), regression trees use metrics to minimize the variability of the target values in a subset. Common metrics include:

Mean Squared Error (MSE):

Measures the variance within the subsets:
𝑀
𝑆
𝐸
=
1
𝑁
∑
𝑖
=
1
𝑁
(
𝑦
𝑖
−
𝑦
ˉ
)
2
MSE=
N
1
​

i=1
∑
N
​
 (y
i
​
 −
y
ˉ
​
 )
2

Where
𝑦
𝑖
y
i
​
  are the target values,
𝑦
ˉ
y
ˉ
​
  is the mean target value in the subset, and
𝑁
N is the number of samples.
Mean Absolute Error (MAE):

Measures the average absolute deviation:
𝑀
𝐴
𝐸
=
1
𝑁
∑
𝑖
=
1
𝑁
∣
𝑦
𝑖
−
𝑦
ˉ
∣
MAE=
N
1
​

i=1
∑
N
​
 ∣y
i
​
 −
y
ˉ
​
 ∣
Reduction in Variance:

The algorithm chooses splits that maximize the reduction in variance of the target values after splitting.
Stopping Criteria
To avoid overfitting, Decision Tree Regressors rely on constraints such as:

Maximum Depth (
𝑚
𝑎
𝑥
_
𝑑
𝑒
𝑝
𝑡
ℎ
max_depth):

Limits the depth of the tree.
Minimum Samples per Split (
𝑚
𝑖
𝑛
_
𝑠
𝑎
𝑚
𝑝
𝑙
𝑒
𝑠
_
𝑠
𝑝
𝑙
𝑖
𝑡
min_samples_split):

Requires a minimum number of samples to consider a split.
Minimum Samples per Leaf (
𝑚
𝑖
𝑛
_
𝑠
𝑎
𝑚
𝑝
𝑙
𝑒
𝑠
_
𝑙
𝑒
𝑎
𝑓
min_samples_leaf):

Ensures leaf nodes have at least a minimum number of samples.
Minimum Impurity Decrease (
𝑚
𝑖
𝑛
_
𝑖
𝑚
𝑝
𝑢
𝑟
𝑖
𝑡
𝑦
_
𝑑
𝑒
𝑐
𝑟
𝑒
𝑎
𝑠
𝑒
min_impurity_decrease):

Splits occur only if the reduction in impurity exceeds a threshold.
Advantages of Decision Tree Regressors
Interpretable:

Easy to understand and visualize, as decisions are made in a hierarchical structure.
Non-linear Relationships:

Can model complex, non-linear relationships between features and the target variable.
No Feature Scaling Required:

Works without the need for normalization or standardization of input features.
Handles Mixed Data:

Can work with both numerical and categorical features.
Disadvantages of Decision Tree Regressors
Overfitting:

Fully grown trees tend to overfit the training data unless pruned or constrained.
Instability:

Small changes in the data can lead to entirely different splits and trees.
Lack of Smooth Predictions:

Predictions are constant within regions, leading to abrupt changes at split boundaries.
Bias Toward Features with Many Values:

Features with a high number of unique values may dominate splits.

Advantages and Disadvantages of Decision Trees
Advantages
Simplicity and Interpretability:

Decision Trees are easy to understand and interpret, even for non-technical users.
The hierarchical structure clearly shows how decisions are made, making it simple to visualize.
No Need for Feature Scaling:

Decision Trees do not require normalization or standardization of data, as they are not affected by the magnitude of feature values.
Handles Mixed Data:

Decision Trees can handle both numerical and categorical data without any special preprocessing.
Non-linear Relationships:

They can model non-linear and complex relationships between features and the target variable.
Feature Importance:

Decision Trees can identify the most significant features used in decision-making, helping with feature selection.
Works Well with Small Data:

Suitable for small to medium-sized datasets and can achieve good results without extensive computation.
Versatility:

Can be used for both classification and regression tasks.
No Assumptions About Data:

Unlike linear models, Decision Trees make no assumptions about the distribution of the data or the relationship between input features.
Disadvantages
Overfitting:

Decision Trees tend to overfit the training data, especially when grown to full depth without pruning.
Instability:

Small changes in the training data can result in completely different trees due to the greedy splitting algorithm.
Bias Toward Features with Many Values:

Features with more unique values (e.g., continuous variables) can dominate the splits, potentially overlooking categorical features.
Lack of Generalization:

Fully grown trees often do not generalize well to unseen data unless pruned or constrained (e.g., by limiting depth).
Greedy Algorithm Limitations:

Splits are determined using a greedy algorithm, which may not result in the globally optimal tree.
Limited Prediction Smoothness:

For regression tasks, Decision Trees predict constant values within split regions, leading to abrupt changes and less smooth predictions.
Computational Cost for Large Trees:

Training a fully grown tree on a large dataset can be computationally expensive.
Curse of Dimensionality:

Performance can degrade when the dataset has many irrelevant features or high dimensionality, as the tree might split on unimportant features.
Summary Table
Advantages	Disadvantages
Simple to understand and interpret	Prone to overfitting without pruning
No need for feature scaling	Unstable to small changes in data
Handles both numerical and categorical data	Bias toward features with many unique values
Can model non-linear relationships	Greedy algorithm may not find global optimum
Identifies feature importance	Predictions are not smooth in regression
Works well with small datasets	Computationally expensive for large trees
When to Use Decision Trees
Use Decision Trees when:
Interpretability is important.
You have mixed types of data (categorical and numerical).
The dataset is small to medium in size.
You want a quick, baseline model for classification or regression.
Alternative Models
If Decision Trees' disadvantages (e.g., overfitting, instability) are problematic, consider:

Random Forests:
Reduces overfitting and instability by averaging multiple Decision Trees.
Gradient Boosting Machines (GBMs):
Builds trees sequentially to optimize performance and reduce error.
Linear Models:
Suitable for simpler tasks where relationships are linear.
Support Vector Machines (SVMs):
Good for high-dimensional data but requires feature scaling.

Decision Trees have mechanisms to handle missing values during both training and prediction phases. Here's how they handle missing data effectively:

1. During Training
a) Ignore Missing Values in Splitting:
When evaluating a potential split at a node, the Decision Tree can ignore data points with missing values for the splitting feature.
Splitting criteria (e.g., Gini Impurity or Entropy for classification, or variance reduction for regression) are calculated only on samples with non-missing values for the feature being evaluated.
b) Surrogate Splits:
A surrogate split is a backup splitting rule used when the primary splitting feature has missing values.
The algorithm identifies alternative features (surrogates) that are most correlated with the primary split and uses them to assign samples with missing values to one of the child nodes.
This technique ensures that samples with missing values are still assigned a path in the tree.
c) Imputation:
Some implementations may impute missing values during training using strategies such as:
Mean or median imputation (for numerical data).
Mode imputation (for categorical data).
Custom imputation methods, such as using domain knowledge.
2. During Prediction
a) Follow the Majority Path:
When encountering a missing value for a splitting feature during prediction, the algorithm can follow the branch with the majority of the training samples from that node.
This is a simple and commonly used heuristic.
b) Weighted Distribution:
Another approach is to distribute the prediction across all possible branches, weighted by the proportion of samples that went down each branch during training.
This approach avoids making a hard decision and considers all potential outcomes.
c) Surrogate Splits:
Similar to training, surrogate splits can be used during prediction. If the primary feature's value is missing, the decision is made based on the surrogate feature.
Example Handling in Scikit-learn
Scikit-learn's DecisionTreeClassifier and DecisionTreeRegressor do not natively handle missing values (e.g., NaNs). Instead, you need to preprocess the data to handle missing values, for instance:

Impute missing values:

python
Copy
Edit
from sklearn.impute import SimpleImputer
from sklearn.tree import DecisionTreeClassifier

# Example dataset with missing values
X = [[1, 2], [3, None], [7, 6], [None, 5]]
y = [0, 1, 0, 1]

# Impute missing values with mean
imputer = SimpleImputer(strategy="mean")
X_imputed = imputer.fit_transform(X)

# Train a Decision Tree
clf = DecisionTreeClassifier()
clf.fit(X_imputed, y)
Use libraries with native missing value support:

Libraries like XGBoost, LightGBM, and CatBoost have built-in mechanisms to handle missing values without imputation.
Advantages of Decision Trees in Handling Missing Values
Flexibility:

Decision Trees can effectively handle missing values through surrogate splits or ignoring missing data points for splitting.
No Need for Extensive Preprocessing:

Some implementations can work directly with datasets containing missing values.
Localized Handling:

Missing values are handled node by node, reducing the impact on the overall model.
Limitations
Dependent on Implementation:

Not all Decision Tree implementations (e.g., in Scikit-learn) natively support missing values.
Preprocessing is required in such cases.
Accuracy Trade-off:

Ignoring missing values or relying on surrogate splits can lead to slight reductions in accuracy, especially if a large proportion of data is missing.

Decision Trees can handle categorical features naturally, as they don't rely on numerical properties like distance or scale. Here's how Decision Trees deal with categorical data:

1. Splitting on Categorical Features
When a feature is categorical (e.g., "Color" with values like "Red," "Green," "Blue"), the Decision Tree splits based on categories rather than numerical thresholds.

a) Binary Splitting (Two Groups)
The Decision Tree splits the feature into two groups:
For example, if the "Color" feature has values
{
𝑅
𝑒
𝑑
,
𝐺
𝑟
𝑒
𝑒
𝑛
,
𝐵
𝑙
𝑢
𝑒
}
{Red,Green,Blue}, the algorithm may decide on a split like:
Group 1:
{
𝑅
𝑒
𝑑
,
𝐺
𝑟
𝑒
𝑒
𝑛
}
{Red,Green}
Group 2:
{
𝐵
𝑙
𝑢
𝑒
}
{Blue}
The algorithm determines the best grouping by calculating impurity (e.g., Gini Impurity, Entropy) for all possible splits and selecting the one that minimizes impurity.
b) Multiway Splitting (One Group Per Category)
The tree can also split the data into multiple branches, one for each unique category.
For example:
If "Color" has three categories (
{
𝑅
𝑒
𝑑
,
𝐺
𝑟
𝑒
𝑒
𝑛
,
𝐵
𝑙
𝑢
𝑒
}
{Red,Green,Blue}), the node splits into three child nodes:
One for
𝐶
𝑜
𝑙
𝑜
𝑟
=
𝑅
𝑒
𝑑
Color=Red
One for
𝐶
𝑜
𝑙
𝑜
𝑟
=
𝐺
𝑟
𝑒
𝑒
𝑛
Color=Green
One for
𝐶
𝑜
𝑙
𝑜
𝑟
=
𝐵
𝑙
𝑢
𝑒
Color=Blue
Multiway splits are common in Decision Tree implementations like Scikit-learn.
2. Encoding Categorical Features
Some Decision Tree implementations (e.g., Scikit-learn) require categorical features to be encoded numerically before training. Encoding methods include:

a) One-Hot Encoding
Converts each category into a separate binary feature (e.g., "Color" with values "Red," "Green," "Blue" becomes three features:
𝐶
𝑜
𝑙
𝑜
𝑟
_
𝑅
𝑒
𝑑
,
𝐶
𝑜
𝑙
𝑜
𝑟
_
𝐺
𝑟
𝑒
𝑒
𝑛
,
𝐶
𝑜
𝑙
𝑜
𝑟
_
𝐵
𝑙
𝑢
𝑒
Color_Red,Color_Green,Color_Blue).
Example:
Original Feature: "Color"
→
[Red, Green, Blue]
Original Feature: "Color"→[Red, Green, Blue]
One-Hot Encoded:
→
[
1
,
0
,
0
]

(
Red
)
,

[
0
,
1
,
0
]

(
Green
)
,

[
0
,
0
,
1
]

(
Blue
)
One-Hot Encoded:→[1,0,0] (Red), [0,1,0] (Green), [0,0,1] (Blue)
b) Label Encoding
Assigns a unique integer to each category (e.g., "Red" → 0, "Green" → 1, "Blue" → 2).
Example:
Original Feature: "Color"
→
[Red, Green, Blue]
Original Feature: "Color"→[Red, Green, Blue]
Label Encoded:
→
[
0
,
1
,
2
]
Label Encoded:→[0,1,2]
Note: This approach can introduce unintended ordinal relationships between categories (e.g.,
𝐵
𝑙
𝑢
𝑒
>
𝐺
𝑟
𝑒
𝑒
𝑛
>
𝑅
𝑒
𝑑
Blue>Green>Red), which may not align with the actual data.
c) Native Support for Categorical Features
Some libraries like CatBoost and LightGBM can handle categorical features natively without requiring encoding.
3. Handling Mixed Features
Decision Trees can handle datasets with both categorical and numerical features. Each feature is evaluated independently during splitting.

Decision Trees are widely used across industries because of their interpretability and versatility. Below are some real-world applications of Decision Trees:

1. Healthcare
Disease Diagnosis:

Predict whether a patient has a certain disease based on symptoms, medical history, or test results.
Example: Classifying whether a tumor is malignant or benign.
Treatment Recommendations:

Suggest personalized treatment plans based on patient characteristics and medical conditions.
Risk Assessment:

Estimate the likelihood of a patient developing a chronic condition (e.g., diabetes, heart disease).
2. Finance
Credit Scoring:

Assess the creditworthiness of an individual based on factors like income, debt, and repayment history.
Fraud Detection:

Identify suspicious transactions or patterns that indicate fraudulent activity.
Loan Approval:

Determine whether a loan application should be approved based on financial metrics and applicant profiles.
Investment Decisions:

Predict stock price trends or portfolio risks.
3. Marketing and Customer Analytics
Customer Segmentation:

Group customers based on purchasing behavior, preferences, or demographics to create targeted marketing campaigns.
Churn Prediction:

Identify customers who are likely to stop using a service and implement retention strategies.
Recommendation Systems:

Suggest products or services to customers based on past behavior and preferences.
Sales Forecasting:

Predict future sales for products using historical data.
4. E-commerce
Dynamic Pricing:

Predict optimal product pricing based on demand, competition, and customer behavior.
Product Categorization:

Classify products into categories based on features and descriptions.
Personalized Shopping Experiences:

Create user-specific recommendations or discounts.
5. Manufacturing
Quality Control:

Predict whether a product will meet quality standards based on production parameters.
Predictive Maintenance:

Estimate when equipment is likely to fail and schedule maintenance accordingly.
Supply Chain Optimization:

Predict demand for raw materials or finished goods to minimize waste and maximize efficiency.
6. Education
Student Performance Prediction:

Predict whether a student will pass or fail based on factors like attendance, test scores, and engagement.
Personalized Learning:

Recommend tailored learning paths based on a student's strengths and weaknesses.
Dropout Risk Assessment:

Identify students at risk of dropping out and suggest intervention strategies.
7. Retail
Inventory Management:

Predict demand for products to optimize stock levels and reduce waste.
Customer Behavior Analysis:

Understand purchasing patterns to optimize product placement and marketing efforts.
Store Location Analysis:

Determine the best locations for new stores based on demographic and geographic data.
8. Energy and Utilities
Energy Consumption Forecasting:

Predict energy usage for efficient grid management.
Fault Detection:

Identify faults in power grids or equipment.
Renewable Energy Optimization:

Predict the efficiency of solar panels or wind turbines based on environmental data.
9. Agriculture
Crop Yield Prediction:

Forecast crop yields based on weather conditions, soil quality, and planting techniques.
Pest and Disease Detection:

Classify whether crops are affected by specific pests or diseases.
Precision Farming:

Recommend optimal planting and irrigation strategies for higher productivity.
10. Transportation
Traffic Prediction:

Predict traffic patterns based on historical data and real-time inputs.
Route Optimization:

Recommend the best routes for deliveries or personal travel.
Autonomous Vehicles:

Make decisions like obstacle detection and lane changes.
11. Entertainment
Recommendation Systems:

Suggest movies, music, or TV shows based on user preferences (e.g., Spotify, Netflix).
Content Categorization:

Classify content into genres or themes for better organization.
12. Government and Public Policy
Policy Impact Analysis:

Predict the effects of new policies based on historical data.
Fraud Detection in Tax Systems:

Identify fraudulent tax filings or benefit claims.
Crime Prediction and Prevention:

Analyze crime patterns to allocate resources effectively.
13. Environmental Science
Weather Prediction:

Predict temperature, rainfall, or storms using environmental data.
Wildlife Conservation:

Classify species or predict habitats under threat based on ecological data.
Pollution Monitoring:

Predict pollution levels and recommend mitigation strategies.
14. Human Resources
Employee Attrition Prediction:

Identify employees likely to leave the organization based on performance reviews and engagement levels.
Hiring Decisions:

Evaluate candidates for specific roles based on past data and interview performance.

Practical

1. Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy?
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Labels

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Calculate the accuracy of the model
accuracy = accuracy_score(y_test, y_pred)

print(f"Model Accuracy: {accuracy:.2f}")

2. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances?
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Labels

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Decision Tree Classifier using Gini Impurity
clf = DecisionTreeClassifier(criterion='gini', random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Print the feature importances
print("Feature Importances:")
for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):
    print(f"{feature_name}: {importance:.4f}")

3.Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy?
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Labels

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Decision Tree Classifier using Entropy as the splitting criterion
clf = DecisionTreeClassifier(criterion='entropy', random_state=42)

# Train the model
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Calculate and print the model accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")

4.Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE)?
from sklearn.datasets import fetch_california_housing
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Load the California Housing dataset
housing = fetch_california_housing()
X = housing.data  # Features
y = housing.target  # Labels (median house value)

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Decision Tree Regressor
regressor = DecisionTreeRegressor(random_state=42)

# Train the model
regressor.fit(X_train, y_train)

# Make predictions on the test set
y_pred = regressor.predict(X_test)

# Calculate the Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error (MSE): {mse:.2f}")

5. Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz?
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, export_graphviz
import graphviz

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Labels

# Train a Decision Tree Classifier
clf = DecisionTreeClassifier(criterion='gini', random_state=42)
clf.fit(X, y)

# Export the Decision Tree to DOT format
dot_data = export_graphviz(
    clf,
    out_file=None,  # Output as a string
    feature_names=iris.feature_names,
    class_names=iris.target_names,
    filled=True,  # Color nodes based on the class they represent
    rounded=True,  # Rounded corners for better readability
    special_characters=True
)

# Visualize the Decision Tree using Graphviz
graph = graphviz.Source(dot_data)
graph.render("decision_tree")  # Save as a PDF file
graph.view()  # Open the visualization in your default viewer

6.Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its
accuracy with a fully grown tree?
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Labels

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Decision Tree with a maximum depth of 3
clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)
clf_limited.fit(X_train, y_train)

# Train a fully grown Decision Tree
clf_full = DecisionTreeClassifier(random_state=42)
clf_full.fit(X_train, y_train)

# Make predictions with both models
y_pred_limited = clf_limited.predict(X_test)
y_pred_full = clf_full.predict(X_test)

# Calculate accuracy for both models
accuracy_limited = accuracy_score(y_test, y_pred_limited)
accuracy_full = accuracy_score(y_test, y_pred_full)

# Print the accuracy results
print(f"Accuracy with max_depth=3: {accuracy_limited:.2f}")
print(f"Accuracy with fully grown tree: {accuracy_full:.2f}")

7.Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its
accuracy with a default tree?
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Labels

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Decision Tree Classifier with min_samples_split=5
clf_custom = DecisionTreeClassifier(min_samples_split=5, random_state=42)
clf_custom.fit(X_train, y_train)

# Train a default Decision Tree Classifier
clf_default = DecisionTreeClassifier(random_state=42)
clf_default.fit(X_train, y_train)

# Make predictions with both models
y_pred_custom = clf_custom.predict(X_test)
y_pred_default = clf_default.predict(X_test)

# Calculate accuracy for both models
accuracy_custom = accuracy_score(y_test, y_pred_custom)
accuracy_default = accuracy_score(y_test, y_pred_default)

# Print the accuracy results
print(f"Accuracy with min_samples_split=5: {accuracy_custom:.2f}")
print(f"Accuracy with default tree: {accuracy_default:.2f}")

8.Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its
accuracy with unscaled data?
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Labels

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Decision Tree Classifier without feature scaling
clf_unscaled = DecisionTreeClassifier(random_state=42)
clf_unscaled.fit(X_train, y_train)
y_pred_unscaled = clf_unscaled.predict(X_test)
accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)

# Apply feature scaling (standardization)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train a Decision Tree Classifier with scaled features
clf_scaled = DecisionTreeClassifier(random_state=42)
clf_scaled.fit(X_train_scaled, y_train)
y_pred_scaled = clf_scaled.predict(X_test_scaled)
accuracy_scaled = accuracy_score(y_test, y_pred_scaled)

# Print accuracy comparison
print(f"Accuracy without scaling: {accuracy_unscaled:.2f}")
print(f"Accuracy with scaling: {accuracy_scaled:.2f}")

9.Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass
classification?
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Labels

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Decision Tree Classifier
base_classifier = DecisionTreeClassifier(random_state=42)

# Apply One-vs-Rest (OvR) strategy
ovr_classifier = OneVsRestClassifier(base_classifier)
ovr_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = ovr_classifier.predict(X_test)

# Calculat

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Labels

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Display feature importance scores
print("Feature Importance Scores:")
for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):
    print(f"{feature_name}: {importance:.4f}")

11.Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance
with an unrestricted tree?
from sklearn.datasets import fetch_california_housing
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Load the California Housing dataset
data = fetch_california_housing()
X = data.data  # Features
y = data.target  # Labels

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Decision Tree Regressor with max_depth=5
reg_limited = DecisionTreeRegressor(max_depth=5, random_state=42)
reg_limited.fit(X_train, y_train)

# Train an unrestricted Decision Tree Regressor
reg_full = DecisionTreeRegressor(random_state=42)
reg_full.fit(X_train, y_train)

# Make predictions on the test set
y_pred_limited = reg_limited.predict(X_test)
y_pred_full = reg_full.predict(X_test)

# Evaluate performance using Mean Squared Error
mse_limited = mean_squared_error(y_test, y_pred_limited)
mse_full = mean_squar

12.Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and
visualize its effect on accuracy?
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Labels

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Decision Tree Classifier without pruning to get effective alphas for pruning
clf = DecisionTreeClassifier(random_state=42)
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas  # Effective alphas
impurities = path.impurities  # Total impurity at each alpha

# Train and evaluate a tree for each alpha
train_accuracies = []
test_accuracies = []

for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    train_accuracies.append(clf.score(X_train, y_train))
    test_accuracies.append(clf.score(X_test, y_test))

# Plot the effect of CCP on accuracy
plt.figure(figsize=(10, 6))
plt.plot(ccp_alphas, train_accuracies, label="Train Accuracy", marker="o")
plt.plot(ccp_alphas, test_accuracies, label="Test Accuracy", marker="o")
plt.xlabel("CCP Alpha")
plt.ylabel("Accuracy")
plt.title("Effect of Cost Complexity Pruning (CCP) on Accuracy")
plt.legend()
plt.grid()
plt.show()

13.Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,
Recall, and F1-Score?
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, precision_score, recall_score, f1_score

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Labels

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Calculate Precision, Recall, and F1-Score
precision = precision_score(y_test, y_pred, average="weighted")
recall = recall_score(y_test, y_pred, average="weighted")
f1 = f1_score(y_test, y_pred, average="weighted")

# Display the results
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Score: {f1:.2f}")

# Detailed Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Labels

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)

# Visualize the

14.Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values
for max_depth and min_samples_split.
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import classification_report

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features
y = iris.target  # Labels

# Split the dataset into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for GridSearchCV
param_grid = {
    "max_depth": [2, 3, 4, 5, 6, None],
    "min_samples_split": [2, 5, 10, 20],
}

# Initialize a Decision Tree Classifier
clf = DecisionTreeClassifier(random_state=42)

# Perform GridSearchCV
grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, scoring="accuracy", cv=5, verbose=1, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best parameters and the best estimator
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# Evaluate the best model on the test set
y_pred = best_model.predict(X_test)
print(f"Best Parameters: {best_params}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=iris.target_names))