{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gKYJte2ij8v"
      },
      "outputs": [],
      "source": [
        "Theoretical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1. What is a Decision Tree, and how does it work?\n",
        "A Decision Tree is a type of supervised machine learning algorithm used for classification and regression tasks. It works by breaking down a dataset into smaller subsets while simultaneously developing an associated decision tree model. The tree structure consists of nodes, branches, and leaves:\n",
        "\n",
        "Components of a Decision Tree:\n",
        "Root Node:\n",
        "\n",
        "Represents the entire dataset.\n",
        "Splits into two or more branches based on the most significant feature (the best splitting criterion).\n",
        "Decision Nodes:\n",
        "\n",
        "Intermediate nodes where a decision (split) is made based on certain conditions.\n",
        "Leaf Nodes:\n",
        "\n",
        "Represent the final output or decision (e.g., a class label for classification tasks or a predicted value for regression tasks).\n",
        "Branches:\n",
        "\n",
        "Connect nodes and represent outcomes of a decision (e.g., \"Yes\" or \"No\" for binary splits).\n",
        "How it Works:\n",
        "Feature Selection:\n",
        "\n",
        "The algorithm evaluates each feature to determine the \"best split\" based on a metric like:\n",
        "Gini Impurity or Entropy (for classification tasks).\n",
        "Mean Squared Error (MSE) (for regression tasks).\n",
        "The goal is to minimize impurity or error at each split.\n",
        "Splitting:\n",
        "\n",
        "The dataset is split into subsets based on the selected feature and its threshold or condition (e.g., Age > 30).\n",
        "This process is repeated recursively.\n",
        "Stopping Criteria:\n",
        "\n",
        "Splitting stops when a predefined condition is met, such as:\n",
        "Maximum tree depth is reached.\n",
        "Minimum number of samples in a node is below a threshold.\n",
        "No significant gain in impurity reduction.\n",
        "Prediction:\n",
        "\n",
        "Once the tree is built, it can classify a new input by traversing the tree from the root to a leaf node, following the conditions at each node.\n",
        "Example (Classification):\n",
        "For a dataset predicting whether someone buys a product:\n",
        "\n",
        "Root Node: \"Is Age > 30?\"\n",
        "Yes Branch: \"Income > $50K?\"\n",
        "Yes: \"Buy\" (Leaf)\n",
        "No: \"Don't Buy\" (Leaf)\n",
        "No Branch: \"Don't Buy\" (Leaf)\n",
        "For a new person aged 32 with income $60K, the tree predicts \"Buy.\"\n",
        "\n",
        "Strengths:\n",
        "Easy to interpret and visualize.\n",
        "Handles both numerical and categorical data.\n",
        "Requires little preprocessing (no need for scaling/normalization).\n",
        "Weaknesses:\n",
        "Can overfit the data if the tree is too deep.\n",
        "Sensitive to small variations in the data.\n",
        "May not perform well compared to more complex algorithms like Random Forests or Gradient Boosted Trees."
      ],
      "metadata": {
        "id": "nmjMfkXji_-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2. What are impurity measures in Decision Trees?\n",
        "Impurity measures in Decision Trees quantify the \"messiness\" or \"disorder\" of a node, helping the algorithm determine the best way to split the data. The goal is to reduce impurity as much as possible at each split, creating purer nodes where the data is more homogeneous (e.g., all examples belong to the same class).\n",
        "\n",
        "Common Impurity Measures:\n",
        "1. Gini Impurity:\n",
        "Used in classification tasks.\n",
        "Measures the probability of incorrectly classifying a randomly chosen sample if it were labeled according to the class distribution in the node.\n",
        "Formula:\n",
        "\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝐶\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini=1−\n",
        "i=1\n",
        "∑\n",
        "C\n",
        "​\n",
        " p\n",
        "i\n",
        "2\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝐶\n",
        "C = number of classes.\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        "  = proportion of samples in class\n",
        "𝑖\n",
        "i at the node.\n",
        "Characteristics:\n",
        "\n",
        "Ranges between 0 (pure node, all samples belong to one class) and a maximum value (impure node, uniform distribution of classes).\n",
        "Splits that reduce Gini Impurity the most are preferred.\n",
        "2. Entropy (Information Gain):\n",
        "Also used in classification tasks, derived from Information Theory.\n",
        "Measures the uncertainty or randomness in the node's data distribution.\n",
        "Formula:\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝐶\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "Entropy=−\n",
        "i=1\n",
        "∑\n",
        "C\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )\n",
        "Where:\n",
        "\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        "  = proportion of samples in class\n",
        "𝑖\n",
        "i.\n",
        "Information Gain:\n",
        "\n",
        "Measures the reduction in entropy after a split.\n",
        "𝐼\n",
        "𝑛\n",
        "𝑓\n",
        "𝑜\n",
        "𝑟\n",
        "𝑚\n",
        "𝑎\n",
        "𝑡\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "\n",
        "𝐺\n",
        "𝑎\n",
        "𝑖\n",
        "𝑛\n",
        "=\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "−\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "𝑛\n",
        "𝑗\n",
        "𝑛\n",
        "⋅\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑗\n",
        "Information Gain=Entropy\n",
        "parent\n",
        "​\n",
        " −\n",
        "j=1\n",
        "∑\n",
        "k\n",
        "​\n",
        "\n",
        "n\n",
        "n\n",
        "j\n",
        "​\n",
        "\n",
        "​\n",
        " ⋅Entropy\n",
        "child\n",
        "j\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑛\n",
        "n = total samples in the parent node.\n",
        "𝑛\n",
        "𝑗\n",
        "n\n",
        "j\n",
        "​\n",
        "  = samples in child node\n",
        "𝑗\n",
        "j.\n",
        "3. Mean Squared Error (MSE):\n",
        "Used in regression tasks.\n",
        "Measures the variance of the target variable within a node.\n",
        "Formula:\n",
        "\n",
        "𝑀\n",
        "𝑆\n",
        "𝐸\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        "−\n",
        "𝑦\n",
        "ˉ\n",
        ")\n",
        "2\n",
        "MSE=\n",
        "n\n",
        "1\n",
        "​\n",
        "\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " (y\n",
        "i\n",
        "​\n",
        " −\n",
        "y\n",
        "ˉ\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑦\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        "  = target value of the\n",
        "𝑖\n",
        "i-th sample.\n",
        "𝑦\n",
        "ˉ\n",
        "y\n",
        "ˉ\n",
        "​\n",
        "  = mean of target values in the node.\n",
        "𝑛\n",
        "n = number of samples in the node.\n",
        "Characteristics:\n",
        "\n",
        "Splits that minimize the MSE lead to purer nodes with less variance.\n",
        "4. Variance Reduction:\n",
        "Another measure for regression tasks, closely related to MSE.\n",
        "Assesses the reduction in variance between parent and child nodes.\n",
        "Formula:\n",
        "\n",
        "𝑉\n",
        "𝑎\n",
        "𝑟\n",
        "𝑖\n",
        "𝑎\n",
        "𝑛\n",
        "𝑐\n",
        "𝑒\n",
        "\n",
        "𝑅\n",
        "𝑒\n",
        "𝑑\n",
        "𝑢\n",
        "𝑐\n",
        "𝑡\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "=\n",
        "𝑉\n",
        "𝑎\n",
        "𝑟\n",
        "𝑖\n",
        "𝑎\n",
        "𝑛\n",
        "𝑐\n",
        "𝑒\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "−\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "𝑛\n",
        "𝑗\n",
        "𝑛\n",
        "⋅\n",
        "𝑉\n",
        "𝑎\n",
        "𝑟\n",
        "𝑖\n",
        "𝑎\n",
        "𝑛\n",
        "𝑐\n",
        "𝑒\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑗\n",
        "Variance Reduction=Variance\n",
        "parent\n",
        "​\n",
        " −\n",
        "j=1\n",
        "∑\n",
        "k\n",
        "​\n",
        "\n",
        "n\n",
        "n\n",
        "j\n",
        "​\n",
        "\n",
        "​\n",
        " ⋅Variance\n",
        "child\n",
        "j\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "Choosing the Best Split:\n",
        "The algorithm evaluates potential splits using these measures.\n",
        "For classification, Gini Impurity or Entropy are common.\n",
        "For regression, MSE or Variance Reduction is used.\n",
        "The split that results in the greatest impurity reduction is chosen."
      ],
      "metadata": {
        "id": "J28TXXXCjULg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The mathematical formula for Gini Impurity is:\n",
        "\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝐶\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini=1−\n",
        "i=1\n",
        "∑\n",
        "C\n",
        "​\n",
        " p\n",
        "i\n",
        "2\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝐶\n",
        "C: The total number of classes in the dataset.\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        " : The proportion (probability) of samples belonging to class\n",
        "𝑖\n",
        "i in the node.\n",
        "Explanation:\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        " : Calculated as\n",
        "Number of samples in class\n",
        "𝑖\n",
        "Total number of samples in the node\n",
        "Total number of samples in the node\n",
        "Number of samples in class i\n",
        "​\n",
        " .\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "p\n",
        "i\n",
        "2\n",
        "​\n",
        " : Represents the squared probability of selecting a sample from class\n",
        "𝑖\n",
        "i.\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝐶\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "∑\n",
        "i=1\n",
        "C\n",
        "​\n",
        " p\n",
        "i\n",
        "2\n",
        "​\n",
        " : The sum of squared probabilities for all classes.\n",
        "Higher values mean the node is purer (dominated by one class).\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝐶\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "1−∑\n",
        "i=1\n",
        "C\n",
        "​\n",
        " p\n",
        "i\n",
        "2\n",
        "​\n",
        " : Subtracting this sum from 1 gives the Gini Impurity, representing the chance of incorrect classification.\n",
        "Characteristics of Gini Impurity:\n",
        "Range:\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "0\n",
        "Gini=0: Perfectly pure node (all samples belong to one class).\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "Gini close to 0.5: Impure node (samples are evenly distributed among classes in a binary split).\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "Gini approaches\n",
        "1\n",
        "−\n",
        "1\n",
        "𝐶\n",
        "1−\n",
        "C\n",
        "1\n",
        "​\n",
        "  for maximum impurity with\n",
        "𝐶\n",
        "C classes.\n",
        "Split Preference: Decision Trees prefer splits that minimize Gini Impurity, as they lead to purer child nodes."
      ],
      "metadata": {
        "id": "QLOPaPLIj4mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3. What is the mathematical formula for Entropy?\n",
        "The mathematical formula for Entropy in the context of decision trees is:\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝐶\n",
        "𝑝\n",
        "𝑖\n",
        "⋅\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "Entropy=−\n",
        "i=1\n",
        "∑\n",
        "C\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        " ⋅log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )\n",
        "Where:\n",
        "\n",
        "𝐶\n",
        "C: The total number of classes in the dataset.\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        " : The proportion (probability) of samples belonging to class\n",
        "𝑖\n",
        "i in the node.\n",
        "Explanation:\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        " :\n",
        "\n",
        "Calculated as\n",
        "Number of samples in class\n",
        "𝑖\n",
        "Total number of samples in the node\n",
        "Total number of samples in the node\n",
        "Number of samples in class i\n",
        "​\n",
        " .\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        "  is a value between 0 and 1.\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " ):\n",
        "\n",
        "Measures the \"information content\" or uncertainty for class\n",
        "𝑖\n",
        "i.\n",
        "If\n",
        "𝑝\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "p\n",
        "i\n",
        "​\n",
        " =1 (pure class),\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "=\n",
        "0\n",
        "log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )=0, meaning no uncertainty.\n",
        "If\n",
        "𝑝\n",
        "𝑖\n",
        "=\n",
        "0\n",
        "p\n",
        "i\n",
        "​\n",
        " =0, we define\n",
        "𝑝\n",
        "𝑖\n",
        "⋅\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "=\n",
        "0\n",
        "p\n",
        "i\n",
        "​\n",
        " ⋅log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )=0 (using the convention that\n",
        "0\n",
        "⋅\n",
        "log\n",
        "⁡\n",
        "(\n",
        "0\n",
        ")\n",
        "=\n",
        "0\n",
        "0⋅log(0)=0).\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝐶\n",
        "𝑝\n",
        "𝑖\n",
        "⋅\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "−∑\n",
        "i=1\n",
        "C\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        " ⋅log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " ):\n",
        "\n",
        "The negative sign ensures entropy is non-negative.\n",
        "The sum aggregates the uncertainty for all classes in the node.\n",
        "Characteristics of Entropy:\n",
        "Range:\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "0\n",
        "Entropy=0: The node is pure (all samples belong to a single class).\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "Entropy is maximum when samples are evenly distributed among all\n",
        "𝐶\n",
        "C classes.\n",
        "For binary classification,\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "Entropy=1 when\n",
        "𝑝\n",
        "1\n",
        "=\n",
        "𝑝\n",
        "2\n",
        "=\n",
        "0.5\n",
        "p\n",
        "1\n",
        "​\n",
        " =p\n",
        "2\n",
        "​\n",
        " =0.5.\n",
        "Split Preference:\n",
        "\n",
        "Decision Trees aim to maximize Information Gain, which is the reduction in Entropy after a split.\n",
        "Example:\n",
        "Suppose we have a node with two classes:\n",
        "\n",
        "Class 1: 4 samples.\n",
        "Class 2: 6 samples.\n",
        "Total samples: 10.\n",
        "𝑝\n",
        "1\n",
        "=\n",
        "4\n",
        "10\n",
        "=\n",
        "0.4\n",
        "p\n",
        "1\n",
        "​\n",
        " =\n",
        "10\n",
        "4\n",
        "​\n",
        " =0.4,\n",
        "𝑝\n",
        "2\n",
        "=\n",
        "6\n",
        "10\n",
        "=\n",
        "0.6\n",
        "p\n",
        "2\n",
        "​\n",
        " =\n",
        "10\n",
        "6\n",
        "​\n",
        " =0.6.\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "[\n",
        "0.4\n",
        "⋅\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "0.4\n",
        ")\n",
        "+\n",
        "0.6\n",
        "⋅\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "0.6\n",
        ")\n",
        "]\n",
        "Entropy=−[0.4⋅log\n",
        "2\n",
        "​\n",
        " (0.4)+0.6⋅log\n",
        "2\n",
        "​\n",
        " (0.6)].\n",
        "Using\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "0.4\n",
        ")\n",
        "≈\n",
        "−\n",
        "1.322\n",
        "log\n",
        "2\n",
        "​\n",
        " (0.4)≈−1.322 and\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "0.6\n",
        ")\n",
        "≈\n",
        "−\n",
        "0.737\n",
        "log\n",
        "2\n",
        "​\n",
        " (0.6)≈−0.737:\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "[\n",
        "0.4\n",
        "⋅\n",
        "(\n",
        "−\n",
        "1.322\n",
        ")\n",
        "+\n",
        "0.6\n",
        "⋅\n",
        "(\n",
        "−\n",
        "0.737\n",
        ")\n",
        "]\n",
        "Entropy=−[0.4⋅(−1.322)+0.6⋅(−0.737)]\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "0.5288\n",
        "+\n",
        "0.4422\n",
        "=\n",
        "0.971\n",
        "Entropy=0.5288+0.4422=0.971\n",
        "The Entropy for this node is approximately 0.971."
      ],
      "metadata": {
        "id": "vfsr61NgkEgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Information Gain (IG) is a metric used in Decision Trees to evaluate how well a feature (or split) separates the data into distinct classes. It measures the reduction in uncertainty (or entropy) about the target variable after splitting the data based on a feature.\n",
        "\n",
        "Mathematical Formula:\n",
        "𝐼\n",
        "𝑛\n",
        "𝑓\n",
        "𝑜\n",
        "𝑟\n",
        "𝑚\n",
        "𝑎\n",
        "𝑡\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "\n",
        "𝐺\n",
        "𝑎\n",
        "𝑖\n",
        "𝑛\n",
        "=\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "−\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "𝑛\n",
        "𝑗\n",
        "𝑛\n",
        "⋅\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑗\n",
        "Information Gain=Entropy\n",
        "parent\n",
        "​\n",
        " −\n",
        "j=1\n",
        "∑\n",
        "k\n",
        "​\n",
        "\n",
        "n\n",
        "n\n",
        "j\n",
        "​\n",
        "\n",
        "​\n",
        " ⋅Entropy\n",
        "child\n",
        "j\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "Entropy\n",
        "parent\n",
        "​\n",
        " : The entropy of the original dataset (before splitting).\n",
        "𝑘\n",
        "k: The number of child nodes after the split.\n",
        "𝑛\n",
        "n: Total number of samples in the parent node.\n",
        "𝑛\n",
        "𝑗\n",
        "n\n",
        "j\n",
        "​\n",
        " : Number of samples in child node\n",
        "𝑗\n",
        "j.\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑗\n",
        "Entropy\n",
        "child\n",
        "j\n",
        "​\n",
        "\n",
        "​\n",
        " : The entropy of child node\n",
        "𝑗\n",
        "j.\n",
        "How It Works:\n",
        "Before Splitting:\n",
        "\n",
        "Compute the entropy of the parent node (\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "Entropy\n",
        "parent\n",
        "​\n",
        " ).\n",
        "This quantifies the uncertainty in the target variable for the entire dataset.\n",
        "After Splitting:\n",
        "\n",
        "For each potential split:\n",
        "Divide the data into child nodes based on the feature's values.\n",
        "Compute the entropy for each child node (\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑗\n",
        "Entropy\n",
        "child\n",
        "j\n",
        "​\n",
        "\n",
        "​\n",
        " ).\n",
        "Calculate the weighted average of these entropies, based on the proportion of samples in each child node.\n",
        "Information Gain:\n",
        "\n",
        "Subtract the weighted average entropy of the child nodes from the entropy of the parent node.\n",
        "Higher Information Gain indicates a better split, as it leads to purer child nodes (less uncertainty).\n",
        "Key Intuition:\n",
        "A high Information Gain means the split has greatly reduced uncertainty, creating child nodes that are closer to being pure (all samples in a node belong to the same class).\n",
        "Features with the highest Information Gain are chosen for splitting at each step of building the tree.\n",
        "Example:\n",
        "Suppose a dataset has 10 samples:\n",
        "\n",
        "6\n",
        "6 belong to Class A.\n",
        "4\n",
        "4 belong to Class B.\n",
        "1. Compute\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "Entropy\n",
        "parent\n",
        "​\n",
        " :\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "=\n",
        "−\n",
        "(\n",
        "6\n",
        "10\n",
        "log\n",
        "⁡\n",
        "2\n",
        "6\n",
        "10\n",
        "+\n",
        "4\n",
        "10\n",
        "log\n",
        "⁡\n",
        "2\n",
        "4\n",
        "10\n",
        ")\n",
        "Entropy\n",
        "parent\n",
        "​\n",
        " =−(\n",
        "10\n",
        "6\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        "\n",
        "10\n",
        "6\n",
        "​\n",
        " +\n",
        "10\n",
        "4\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        "\n",
        "10\n",
        "4\n",
        "​\n",
        " )\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "=\n",
        "−\n",
        "(\n",
        "0.6\n",
        "⋅\n",
        "−\n",
        "0.737\n",
        "+\n",
        "0.4\n",
        "⋅\n",
        "−\n",
        "1.322\n",
        ")\n",
        "=\n",
        "0.971\n",
        "Entropy\n",
        "parent\n",
        "​\n",
        " =−(0.6⋅−0.737+0.4⋅−1.322)=0.971\n",
        "2. After Splitting:\n",
        "Split the dataset into two child nodes based on a feature:\n",
        "\n",
        "Child Node 1:\n",
        "4\n",
        "4 samples (\n",
        "3\n",
        "3 Class A,\n",
        "1\n",
        "1 Class B).\n",
        "Child Node 2:\n",
        "6\n",
        "6 samples (\n",
        "3\n",
        "3 Class A,\n",
        "3\n",
        "3 Class B).\n",
        "Compute\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "1\n",
        "Entropy\n",
        "child\n",
        "1\n",
        "​\n",
        "\n",
        "​\n",
        " :\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "1\n",
        "=\n",
        "−\n",
        "(\n",
        "3\n",
        "4\n",
        "log\n",
        "⁡\n",
        "2\n",
        "3\n",
        "4\n",
        "+\n",
        "1\n",
        "4\n",
        "log\n",
        "⁡\n",
        "2\n",
        "1\n",
        "4\n",
        ")\n",
        "=\n",
        "0.811\n",
        "Entropy\n",
        "child\n",
        "1\n",
        "​\n",
        "\n",
        "​\n",
        " =−(\n",
        "4\n",
        "3\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        "\n",
        "4\n",
        "3\n",
        "​\n",
        " +\n",
        "4\n",
        "1\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        "\n",
        "4\n",
        "1\n",
        "​\n",
        " )=0.811\n",
        "Compute\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "2\n",
        "Entropy\n",
        "child\n",
        "2\n",
        "​\n",
        "\n",
        "​\n",
        " :\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "2\n",
        "=\n",
        "−\n",
        "(\n",
        "3\n",
        "6\n",
        "log\n",
        "⁡\n",
        "2\n",
        "3\n",
        "6\n",
        "+\n",
        "3\n",
        "6\n",
        "log\n",
        "⁡\n",
        "2\n",
        "3\n",
        "6\n",
        ")\n",
        "=\n",
        "1.000\n",
        "Entropy\n",
        "child\n",
        "2\n",
        "​\n",
        "\n",
        "​\n",
        " =−(\n",
        "6\n",
        "3\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        "\n",
        "6\n",
        "3\n",
        "​\n",
        " +\n",
        "6\n",
        "3\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        "\n",
        "6\n",
        "3\n",
        "​\n",
        " )=1.000\n",
        "3. Compute Weighted Average Entropy of Child Nodes:\n",
        "𝑊\n",
        "𝑒\n",
        "𝑖\n",
        "𝑔\n",
        "ℎ\n",
        "𝑡\n",
        "𝑒\n",
        "𝑑\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "4\n",
        "10\n",
        "⋅\n",
        "0.811\n",
        "+\n",
        "6\n",
        "10\n",
        "⋅\n",
        "1.000\n",
        "=\n",
        "0.911\n",
        "Weighted Entropy=\n",
        "10\n",
        "4\n",
        "​\n",
        " ⋅0.811+\n",
        "10\n",
        "6\n",
        "​\n",
        " ⋅1.000=0.911\n",
        "4. Compute Information Gain:\n",
        "𝐼\n",
        "𝑛\n",
        "𝑓\n",
        "𝑜\n",
        "𝑟\n",
        "𝑚\n",
        "𝑎\n",
        "𝑡\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "\n",
        "𝐺\n",
        "𝑎\n",
        "𝑖\n",
        "𝑛\n",
        "=\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "−\n",
        "𝑊\n",
        "𝑒\n",
        "𝑖\n",
        "𝑔\n",
        "ℎ\n",
        "𝑡\n",
        "𝑒\n",
        "𝑑\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "0.971\n",
        "−\n",
        "0.911\n",
        "=\n",
        "0.060\n",
        "Information Gain=Entropy\n",
        "parent\n",
        "​\n",
        " −Weighted Entropy=0.971−0.911=0.060\n",
        "Usage in Decision Trees:\n",
        "At each step of building the tree, the feature with the highest Information Gain is selected to split the data.\n",
        "This ensures that the tree grows in a way that reduces uncertainty about the target variable most effectively."
      ],
      "metadata": {
        "id": "aqdUF-8IkSJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Gini Impurity and Entropy are both impurity measures used in Decision Trees to evaluate the quality of a split. While they serve the same purpose (to determine the \"purity\" of nodes), they differ in how they calculate impurity and in their characteristics.\n",
        "\n",
        "Key Differences Between Gini Impurity and Entropy\n",
        "Aspect\tGini Impurity\tEntropy\n",
        "Formula\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝐶\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini=1−∑\n",
        "i=1\n",
        "C\n",
        "​\n",
        " p\n",
        "i\n",
        "2\n",
        "​\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝐶\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "Entropy=−∑\n",
        "i=1\n",
        "C\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )\n",
        "Interpretation\tMeasures the probability of misclassification if a sample is randomly assigned a class.\tMeasures the uncertainty or randomness in the class distribution.\n",
        "Range\n",
        "0\n",
        "≤\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "≤\n",
        "𝐶\n",
        "−\n",
        "1\n",
        "𝐶\n",
        "0≤Gini≤\n",
        "C\n",
        "C−1\n",
        "​\n",
        "\n",
        "0\n",
        "≤\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "≤\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝐶\n",
        ")\n",
        "0≤Entropy≤log\n",
        "2\n",
        "​\n",
        " (C)\n",
        "Value at Purity\tGini = 0 (when all samples belong to one class).\tEntropy = 0 (when all samples belong to one class).\n",
        "Value at Maximum Impurity\tGini approaches\n",
        "1\n",
        "−\n",
        "1\n",
        "𝐶\n",
        "1−\n",
        "C\n",
        "1\n",
        "​\n",
        "  when all classes are evenly distributed.\tEntropy is maximum (\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝐶\n",
        ")\n",
        "log\n",
        "2\n",
        "​\n",
        " (C)) when all classes are evenly distributed.\n",
        "Sensitivity to Class Distribution\tSlightly less sensitive to small changes in class probabilities.\tMore sensitive to small changes in class probabilities.\n",
        "Computational Complexity\tEasier to compute since it avoids logarithmic calculations.\tMore computationally expensive due to the logarithm.\n",
        "Use Case Preference\tCommonly used in CART (Classification and Regression Trees) due to faster computation.\tCommonly used in ID3, C4.5, and C5.0 algorithms for classification tasks.\n",
        "Detailed Comparison\n",
        "Behavior with Pure Nodes:\n",
        "\n",
        "When a node contains samples of only one class, both Gini and Entropy reach their minimum value (0).\n",
        "Behavior with Impure Nodes:\n",
        "\n",
        "For a binary classification problem:\n",
        "Gini Impurity is maximized when both classes are equally likely (\n",
        "𝑝\n",
        "1\n",
        "=\n",
        "𝑝\n",
        "2\n",
        "=\n",
        "0.5\n",
        "p\n",
        "1\n",
        "​\n",
        " =p\n",
        "2\n",
        "​\n",
        " =0.5), resulting in\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "0.5\n",
        "Gini=0.5.\n",
        "Entropy is also maximized when both classes are equally likely, resulting in\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "Entropy=1.\n",
        "Impact of Class Probability Changes:\n",
        "\n",
        "Gini Impurity is less sensitive to small changes in class probabilities, making it slightly more robust.\n",
        "Entropy is more sensitive, meaning it penalizes impurity more heavily, especially when class probabilities are closer to each other.\n",
        "Computational Efficiency:\n",
        "\n",
        "Gini Impurity is computationally faster as it avoids logarithmic calculations.\n",
        "Entropy involves\n",
        "log\n",
        "⁡\n",
        "2\n",
        "log\n",
        "2\n",
        "​\n",
        "  calculations, which can add computational overhead.\n",
        "When to Use Gini Impurity vs. Entropy\n",
        "Gini Impurity:\n",
        "\n",
        "Often used in practical applications (e.g., Scikit-learn uses Gini as the default criterion for Decision Trees).\n",
        "Preferred when speed is a priority, especially for large datasets.\n",
        "Entropy:\n",
        "\n",
        "Used when interpretability of the \"information gain\" is more important.\n",
        "Preferred in cases where understanding the reduction in randomness or uncertainty is critical.\n",
        "Example Comparison:\n",
        "Suppose a node contains:\n",
        "\n",
        "40 samples of Class A.\n",
        "60 samples of Class B.\n",
        "Probabilities:\n",
        "𝑝\n",
        "𝐴\n",
        "=\n",
        "0.4\n",
        ",\n",
        "𝑝\n",
        "𝐵\n",
        "=\n",
        "0.6\n",
        "p\n",
        "A\n",
        "​\n",
        " =0.4,p\n",
        "B\n",
        "​\n",
        " =0.6.\n",
        "\n",
        "Gini Impurity:\n",
        "\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "−\n",
        "(\n",
        "0.4\n",
        "2\n",
        "+\n",
        "0.6\n",
        "2\n",
        ")\n",
        "=\n",
        "1\n",
        "−\n",
        "(\n",
        "0.16\n",
        "+\n",
        "0.36\n",
        ")\n",
        "=\n",
        "0.48\n",
        "Gini=1−(0.4\n",
        "2\n",
        " +0.6\n",
        "2\n",
        " )=1−(0.16+0.36)=0.48\n",
        "Entropy:\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "(\n",
        "0.4\n",
        "⋅\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "0.4\n",
        ")\n",
        "+\n",
        "0.6\n",
        "⋅\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "0.6\n",
        ")\n",
        ")\n",
        "Entropy=−(0.4⋅log\n",
        "2\n",
        "​\n",
        " (0.4)+0.6⋅log\n",
        "2\n",
        "​\n",
        " (0.6))\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "=\n",
        "−\n",
        "(\n",
        "0.4\n",
        "⋅\n",
        "−\n",
        "1.322\n",
        "+\n",
        "0.6\n",
        "⋅\n",
        "−\n",
        "0.737\n",
        ")\n",
        "=\n",
        "0.5288\n",
        "+\n",
        "0.4422\n",
        "=\n",
        "0.971\n",
        "Entropy=−(0.4⋅−1.322+0.6⋅−0.737)=0.5288+0.4422=0.971\n",
        "Both measures indicate impurity, but the values are on different scales."
      ],
      "metadata": {
        "id": "Fieo48LvkiE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The mathematical foundation of Decision Trees lies in recursive partitioning, where the dataset is split into subsets based on conditions that maximize some measure of \"purity\" (e.g., Gini Impurity, Entropy, or Variance Reduction). Here’s the detailed explanation:\n",
        "\n",
        "1. Objective of Decision Trees\n",
        "The goal is to divide the dataset into smaller and smaller subsets (nodes) such that:\n",
        "\n",
        "For classification: Each node contains samples that mostly belong to a single class.\n",
        "For regression: Each node minimizes the variance of the target values.\n",
        "At each step, the tree identifies the feature and threshold (or category) that results in the best split.\n",
        "\n",
        "2. Steps of Decision Tree Construction\n",
        "Step 1: Select the Best Feature to Split\n",
        "The algorithm evaluates all available features and their possible split points (for numerical features) or categories (for categorical features). The \"best\" split is the one that maximizes the Information Gain or minimizes the Impurity.\n",
        "\n",
        "For a feature\n",
        "𝑋\n",
        "X:\n",
        "\n",
        "For classification: Use measures like Entropy, Gini Impurity, or Information Gain.\n",
        "For regression: Use measures like Mean Squared Error (MSE) or Variance Reduction.\n",
        "Step 2: Splitting the Data\n",
        "The dataset is divided into two or more child nodes based on the split decision. For example:\n",
        "\n",
        "For a numerical feature\n",
        "𝑋\n",
        "X: Use a threshold\n",
        "𝑡\n",
        "t such that\n",
        "𝑋\n",
        "<\n",
        "𝑡\n",
        "X<t goes to one child node and\n",
        "𝑋\n",
        "≥\n",
        "𝑡\n",
        "X≥t to another.\n",
        "For a categorical feature: Assign categories to different child nodes.\n",
        "Step 3: Stopping Criteria\n",
        "The recursion stops when one of the following conditions is met:\n",
        "\n",
        "A node becomes pure (all samples belong to one class for classification).\n",
        "A maximum tree depth is reached.\n",
        "A minimum number of samples per node is reached.\n",
        "The reduction in impurity becomes negligible.\n",
        "Mathematical Explanation\n",
        "Classification Trees\n",
        "For a dataset\n",
        "𝐷\n",
        "D with\n",
        "𝑁\n",
        "N samples and\n",
        "𝐶\n",
        "C classes:\n",
        "\n",
        "Entropy:\n",
        "\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝐷\n",
        ")\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝐶\n",
        "𝑝\n",
        "𝑖\n",
        "⋅\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "Entropy(D)=−\n",
        "i=1\n",
        "∑\n",
        "C\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        " ⋅log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )\n",
        "Where\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "​\n",
        "  is the proportion of samples in class\n",
        "𝑖\n",
        "i.\n",
        "\n",
        "Gini Impurity:\n",
        "\n",
        "𝐺\n",
        "𝑖\n",
        "𝑛\n",
        "𝑖\n",
        "(\n",
        "𝐷\n",
        ")\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝐶\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini(D)=1−\n",
        "i=1\n",
        "∑\n",
        "C\n",
        "​\n",
        " p\n",
        "i\n",
        "2\n",
        "​\n",
        "\n",
        "Information Gain:\n",
        "\n",
        "𝐼\n",
        "𝐺\n",
        "=\n",
        "𝐼\n",
        "𝑚\n",
        "𝑝\n",
        "𝑢\n",
        "𝑟\n",
        "𝑖\n",
        "𝑡\n",
        "𝑦\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "−\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "𝑛\n",
        "𝑗\n",
        "𝑛\n",
        "⋅\n",
        "𝐼\n",
        "𝑚\n",
        "𝑝\n",
        "𝑢\n",
        "𝑟\n",
        "𝑖\n",
        "𝑡\n",
        "𝑦\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑗\n",
        "IG=Impurity\n",
        "parent\n",
        "​\n",
        " −\n",
        "j=1\n",
        "∑\n",
        "k\n",
        "​\n",
        "\n",
        "n\n",
        "n\n",
        "j\n",
        "​\n",
        "\n",
        "​\n",
        " ⋅Impurity\n",
        "child\n",
        "j\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝐼\n",
        "𝑚\n",
        "𝑝\n",
        "𝑢\n",
        "𝑟\n",
        "𝑖\n",
        "𝑡\n",
        "𝑦\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "Impurity\n",
        "parent\n",
        "​\n",
        " : Impurity before the split.\n",
        "𝐼\n",
        "𝑚\n",
        "𝑝\n",
        "𝑢\n",
        "𝑟\n",
        "𝑖\n",
        "𝑡\n",
        "𝑦\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑗\n",
        "Impurity\n",
        "child\n",
        "j\n",
        "​\n",
        "\n",
        "​\n",
        " : Impurity in child node\n",
        "𝑗\n",
        "j.\n",
        "𝑛\n",
        "n: Total samples in the parent node.\n",
        "𝑛\n",
        "𝑗\n",
        "n\n",
        "j\n",
        "​\n",
        " : Samples in child node\n",
        "𝑗\n",
        "j.\n",
        "The algorithm selects the split that maximizes\n",
        "𝐼\n",
        "𝐺\n",
        "IG.\n",
        "\n",
        "Regression Trees\n",
        "For a dataset\n",
        "𝐷\n",
        "D with\n",
        "𝑁\n",
        "N samples and target values\n",
        "{\n",
        "𝑦\n",
        "1\n",
        ",\n",
        "𝑦\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "𝑦\n",
        "𝑁\n",
        "}\n",
        "{y\n",
        "1\n",
        "​\n",
        " ,y\n",
        "2\n",
        "​\n",
        " ,...,y\n",
        "N\n",
        "​\n",
        " }:\n",
        "\n",
        "Mean Squared Error (MSE):\n",
        "\n",
        "𝑀\n",
        "𝑆\n",
        "𝐸\n",
        "(\n",
        "𝐷\n",
        ")\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        "−\n",
        "𝑦\n",
        "ˉ\n",
        ")\n",
        "2\n",
        "MSE(D)=\n",
        "N\n",
        "1\n",
        "​\n",
        "\n",
        "i=1\n",
        "∑\n",
        "N\n",
        "​\n",
        " (y\n",
        "i\n",
        "​\n",
        " −\n",
        "y\n",
        "ˉ\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "Where\n",
        "𝑦\n",
        "ˉ\n",
        "y\n",
        "ˉ\n",
        "​\n",
        "  is the mean of target values in\n",
        "𝐷\n",
        "D.\n",
        "\n",
        "Variance Reduction:\n",
        "\n",
        "𝑉\n",
        "𝑎\n",
        "𝑟\n",
        "𝑖\n",
        "𝑎\n",
        "𝑛\n",
        "𝑐\n",
        "𝑒\n",
        "\n",
        "𝑅\n",
        "𝑒\n",
        "𝑑\n",
        "𝑢\n",
        "𝑐\n",
        "𝑡\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "=\n",
        "𝑉\n",
        "𝑎\n",
        "𝑟\n",
        "𝑖\n",
        "𝑎\n",
        "𝑛\n",
        "𝑐\n",
        "𝑒\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "−\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "𝑛\n",
        "𝑗\n",
        "𝑛\n",
        "⋅\n",
        "𝑉\n",
        "𝑎\n",
        "𝑟\n",
        "𝑖\n",
        "𝑎\n",
        "𝑛\n",
        "𝑐\n",
        "𝑒\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑗\n",
        "Variance Reduction=Variance\n",
        "parent\n",
        "​\n",
        " −\n",
        "j=1\n",
        "∑\n",
        "k\n",
        "​\n",
        "\n",
        "n\n",
        "n\n",
        "j\n",
        "​\n",
        "\n",
        "​\n",
        " ⋅Variance\n",
        "child\n",
        "j\n",
        "​\n",
        "\n",
        "​\n",
        "\n",
        "The algorithm selects the split that minimizes MSE or maximizes Variance Reduction.\n",
        "\n",
        "Tree Prediction\n",
        "Classification: Assign a class label to a new sample by traversing the tree based on feature values until reaching a leaf node. The leaf node predicts the majority class.\n",
        "Regression: Predict the target value as the average (or weighted average) of the target values in the leaf node.\n",
        "Mathematical Properties\n",
        "Recursive Partitioning: Each split divides the feature space into smaller hyperrectangles. For\n",
        "𝑑\n",
        "d-dimensional data, the splits form a hierarchical partition of\n",
        "𝑅\n",
        "𝑑\n",
        "R\n",
        "d\n",
        " .\n",
        "\n",
        "Non-Parametric Nature: Decision Trees are non-parametric, meaning they do not assume any underlying distribution of the data.\n",
        "\n",
        "Optimization:\n",
        "\n",
        "The splitting process optimizes a greedy algorithm: it makes the best split at each step without considering future splits.\n",
        "Computational Complexity:\n",
        "\n",
        "Splitting: Evaluating all features and thresholds is\n",
        "𝑂\n",
        "(\n",
        "𝑛\n",
        "⋅\n",
        "𝑚\n",
        ")\n",
        "O(n⋅m), where\n",
        "𝑛\n",
        "n is the number of samples and\n",
        "𝑚\n",
        "m is the number of features.\n",
        "Prediction: Traversing a tree is\n",
        "𝑂\n",
        "(\n",
        "tree depth\n",
        ")\n",
        "O(tree depth)."
      ],
      "metadata": {
        "id": "tUFa4OaHkxot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Pre-Pruning, also known as early stopping, is a technique used in Decision Trees to limit their growth during construction. The main objective is to prevent the tree from becoming overly complex and overfitting the training data. Pre-pruning imposes constraints or conditions on the tree growth, stopping it from further splitting when certain criteria are met.\n",
        "\n",
        "How Pre-Pruning Works\n",
        "During the tree-building process, the algorithm evaluates whether a split should occur at a node. If a pre-defined stopping condition is met, the splitting process halts, and the node becomes a leaf node, predicting the majority class (for classification) or the mean value (for regression) of the samples in that node.\n",
        "\n",
        "Common Pre-Pruning Criteria\n",
        "Pre-pruning uses thresholds to decide when to stop splitting. Some common criteria include:\n",
        "\n",
        "Maximum Depth:\n",
        "\n",
        "The tree is allowed to grow only up to a specified depth (\n",
        "𝑚\n",
        "𝑎\n",
        "𝑥\n",
        "_\n",
        "𝑑\n",
        "𝑒\n",
        "𝑝\n",
        "𝑡\n",
        "ℎ\n",
        "max_depth).\n",
        "Limits the number of splits from the root to any leaf.\n",
        "Minimum Number of Samples per Split:\n",
        "\n",
        "Splitting is allowed only if the number of samples in a node is greater than a threshold (\n",
        "𝑚\n",
        "𝑖\n",
        "𝑛\n",
        "_\n",
        "𝑠\n",
        "𝑎\n",
        "𝑚\n",
        "𝑝\n",
        "𝑙\n",
        "𝑒\n",
        "𝑠\n",
        "_\n",
        "𝑠\n",
        "𝑝\n",
        "𝑙\n",
        "𝑖\n",
        "𝑡\n",
        "min_samples_split).\n",
        "Prevents splitting nodes with too few samples.\n",
        "Minimum Number of Samples in a Leaf Node:\n",
        "\n",
        "Ensures that each leaf node contains at least a minimum number of samples (\n",
        "𝑚\n",
        "𝑖\n",
        "𝑛\n",
        "_\n",
        "𝑠\n",
        "𝑎\n",
        "𝑚\n",
        "𝑝\n",
        "𝑙\n",
        "𝑒\n",
        "𝑠\n",
        "_\n",
        "𝑙\n",
        "𝑒\n",
        "𝑎\n",
        "𝑓\n",
        "min_samples_leaf).\n",
        "Minimum Information Gain:\n",
        "\n",
        "Splitting occurs only if the Information Gain or Reduction in Impurity exceeds a certain threshold (\n",
        "𝑚\n",
        "𝑖\n",
        "𝑛\n",
        "_\n",
        "𝑖\n",
        "𝑚\n",
        "𝑝\n",
        "𝑢\n",
        "𝑟\n",
        "𝑖\n",
        "𝑡\n",
        "𝑦\n",
        "_\n",
        "𝑑\n",
        "𝑒\n",
        "𝑐\n",
        "𝑟\n",
        "𝑒\n",
        "𝑎\n",
        "𝑠\n",
        "𝑒\n",
        "min_impurity_decrease).\n",
        "Prevents splits that do not significantly improve the tree.\n",
        "Maximum Number of Leaf Nodes:\n",
        "\n",
        "Limits the total number of leaf nodes in the tree (\n",
        "𝑚\n",
        "𝑎\n",
        "𝑥\n",
        "_\n",
        "𝑙\n",
        "𝑒\n",
        "𝑎\n",
        "𝑓\n",
        "_\n",
        "𝑛\n",
        "𝑜\n",
        "𝑑\n",
        "𝑒\n",
        "𝑠\n",
        "max_leaf_nodes).\n",
        "Maximum Features to Consider:\n",
        "\n",
        "Restricts the number of features evaluated for splitting at each node (\n",
        "𝑚\n",
        "𝑎\n",
        "𝑥\n",
        "_\n",
        "𝑓\n",
        "𝑒\n",
        "𝑎\n",
        "𝑡\n",
        "𝑢\n",
        "𝑟\n",
        "𝑒\n",
        "𝑠\n",
        "max_features).\n",
        "Advantages of Pre-Pruning\n",
        "Prevents Overfitting:\n",
        "\n",
        "Stops the tree from growing too complex, which reduces the likelihood of overfitting to training data.\n",
        "Reduces Computational Cost:\n",
        "\n",
        "Stops unnecessary splits early, reducing both training time and model size.\n",
        "Simplifies the Model:\n",
        "\n",
        "Results in smaller, more interpretable trees.\n",
        "Disadvantages of Pre-Pruning\n",
        "Risk of Underfitting:\n",
        "\n",
        "By halting the growth too early, the tree might not capture the full complexity of the data.\n",
        "Important splits might be missed, reducing the model's accuracy.\n",
        "Hard to Tune:\n",
        "\n",
        "Finding the optimal pre-pruning parameters (e.g.,\n",
        "𝑚\n",
        "𝑖\n",
        "𝑛\n",
        "_\n",
        "𝑠\n",
        "𝑎\n",
        "𝑚\n",
        "𝑝\n",
        "𝑙\n",
        "𝑒\n",
        "𝑠\n",
        "_\n",
        "𝑠\n",
        "𝑝\n",
        "𝑙\n",
        "𝑖\n",
        "𝑡\n",
        "min_samples_split,\n",
        "𝑚\n",
        "𝑎\n",
        "𝑥\n",
        "_\n",
        "𝑑\n",
        "𝑒\n",
        "𝑝\n",
        "𝑡\n",
        "ℎ\n",
        "max_depth) can require extensive experimentation and validation.\n",
        "Example of Pre-Pruning\n",
        "Let’s say we have a Decision Tree for classifying whether a song is \"Popular\" or \"Not Popular\" based on features like Duration and Artist Popularity.\n",
        "\n",
        "Dataset Size: 500 samples.\n",
        "\n",
        "Pre-Pruning Conditions:\n",
        "\n",
        "𝑚\n",
        "𝑎\n",
        "𝑥\n",
        "_\n",
        "𝑑\n",
        "𝑒\n",
        "𝑝\n",
        "𝑡\n",
        "ℎ\n",
        "=\n",
        "5\n",
        "max_depth=5: The tree can only grow up to 5 levels.\n",
        "𝑚\n",
        "𝑖\n",
        "𝑛\n",
        "_\n",
        "𝑠\n",
        "𝑎\n",
        "𝑚\n",
        "𝑝\n",
        "𝑙\n",
        "𝑒\n",
        "𝑠\n",
        "_\n",
        "𝑙\n",
        "𝑒\n",
        "𝑎\n",
        "𝑓\n",
        "=\n",
        "10\n",
        "min_samples_leaf=10: Each leaf must have at least 10 samples.\n",
        "During tree growth:\n",
        "\n",
        "At a node with only 8 samples, the split is not allowed because it violates\n",
        "𝑚\n",
        "𝑖\n",
        "𝑛\n",
        "_\n",
        "𝑠\n",
        "𝑎\n",
        "𝑚\n",
        "𝑝\n",
        "𝑙\n",
        "𝑒\n",
        "𝑠\n",
        "_\n",
        "𝑙\n",
        "𝑒\n",
        "𝑎\n",
        "𝑓\n",
        "min_samples_leaf.\n",
        "If the tree reaches a depth of 5, no further splitting occurs regardless of the data.\n",
        "Comparison with Post-Pruning\n",
        "Aspect\tPre-Pruning\tPost-Pruning\n",
        "Timing\tStops tree growth during construction.\tRemoves unnecessary branches after full tree growth.\n",
        "Objective\tPrevents overfitting early.\tSimplifies an overfitted tree.\n",
        "Computational Cost\tLower, as tree growth is restricted.\tHigher, as a fully grown tree is pruned.\n",
        "Risk\tMay underfit the data by halting too early.\tMay overfit the training data before pruning."
      ],
      "metadata": {
        "id": "l2_B_M5FlD7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Post-Pruning, also called Pruning or Reduced Error Pruning, is a technique used in Decision Trees to simplify the model by removing unnecessary branches after the tree has been fully grown. The primary goal is to reduce overfitting and improve the tree's generalization ability on unseen data.\n",
        "\n",
        "How Post-Pruning Works\n",
        "Grow the Tree Fully:\n",
        "\n",
        "A Decision Tree is first grown to its maximum depth, potentially leading to a very complex and overfitted tree.\n",
        "Prune Back:\n",
        "\n",
        "Starting from the leaf nodes, branches (subtrees) are removed or replaced if doing so results in improved performance or does not significantly degrade it.\n",
        "Criteria for Pruning:\n",
        "\n",
        "A subtree or branch is pruned if:\n",
        "It does not improve validation accuracy.\n",
        "It does not sufficiently reduce impurity (e.g., Gini, Entropy).\n",
        "It does not improve some other evaluation metric (e.g., cross-validation performance).\n",
        "Leaf Node Replacement:\n",
        "\n",
        "When a branch is pruned, the node becomes a leaf, predicting the majority class (classification) or the average value (regression) of the samples it contains.\n",
        "Types of Post-Pruning Techniques\n",
        "Cost Complexity Pruning (CCP):\n",
        "\n",
        "Prunes branches based on a trade-off between tree complexity (number of leaf nodes) and performance.\n",
        "A cost complexity parameter\n",
        "𝛼\n",
        "α controls how much penalty is applied for tree size:\n",
        "𝑅\n",
        "𝛼\n",
        "(\n",
        "𝑇\n",
        ")\n",
        "=\n",
        "𝑅\n",
        "(\n",
        "𝑇\n",
        ")\n",
        "+\n",
        "𝛼\n",
        "⋅\n",
        "∣\n",
        "𝑇\n",
        "∣\n",
        "R\n",
        "α\n",
        "​\n",
        " (T)=R(T)+α⋅∣T∣\n",
        "Where:\n",
        "𝑅\n",
        "(\n",
        "𝑇\n",
        ")\n",
        "R(T): Empirical error of tree\n",
        "𝑇\n",
        "T (e.g., misclassification error or MSE).\n",
        "∣\n",
        "𝑇\n",
        "∣\n",
        "∣T∣: Number of leaf nodes in tree\n",
        "𝑇\n",
        "T.\n",
        "𝛼\n",
        "α: Regularization parameter (higher\n",
        "𝛼\n",
        "α leads to more pruning).\n",
        "Reduced Error Pruning:\n",
        "\n",
        "Directly evaluates the impact of removing branches on a validation set.\n",
        "A branch is pruned if removing it does not reduce the accuracy on the validation set.\n",
        "Minimum Error Pruning:\n",
        "\n",
        "Prunes the tree to minimize the classification error (for classification tasks) or prediction error (for regression tasks).\n",
        "Advantages of Post-Pruning\n",
        "Improves Generalization:\n",
        "\n",
        "Reduces overfitting by simplifying the tree and removing noise-induced branches.\n",
        "Better Control Over Complexity:\n",
        "\n",
        "Allows the tree to fully explore the data initially and then removes unnecessary complexity.\n",
        "Flexibility:\n",
        "\n",
        "Works with any fully grown tree and can be combined with various metrics for evaluation.\n",
        "Disadvantages of Post-Pruning\n",
        "Computational Overhead:\n",
        "\n",
        "Requires growing the tree fully first, which can be computationally expensive, especially for large datasets.\n",
        "Dependency on Validation Data:\n",
        "\n",
        "Pruning decisions often rely on a validation set, reducing the effective size of the training data.\n",
        "Example of Post-Pruning\n",
        "Step-by-Step Explanation\n",
        "Step 1: Fully grow a Decision Tree, splitting the data until all leaf nodes are pure or the minimum node size is reached.\n",
        "Step 2: Use a validation set to evaluate the performance of the tree.\n",
        "Step 3: Start pruning:\n",
        "Begin with leaf nodes and prune branches if merging them improves or does not degrade validation accuracy.\n",
        "Step 4: Repeat pruning until the tree's performance stops improving or reaches the desired complexity.\n",
        "Comparison Between Pre-Pruning and Post-Pruning\n",
        "Aspect\tPre-Pruning\tPost-Pruning\n",
        "Timing\tStops tree growth early during construction.\tSimplifies a fully grown tree by removing branches.\n",
        "Overfitting\tPrevents overfitting during construction.\tRemoves overfitting after full growth.\n",
        "Underfitting Risk\tHigher risk, as stopping early might miss important splits.\tLower risk, as the tree is initially fully grown.\n",
        "Computation Cost\tLower, as it avoids growing a large tree.\tHigher, as it requires building a full tree first.\n"
      ],
      "metadata": {
        "id": "YMUDmJjolTnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Pre-Pruning and Post-Pruning are both methods to control the growth of a Decision Tree and prevent overfitting, but they differ in their timing, approach, and impact on the tree. Here's a detailed comparison:\n",
        "\n",
        "Key Differences\n",
        "Aspect\tPre-Pruning\tPost-Pruning\n",
        "Timing\tApplied during the construction of the tree.\tApplied after the tree is fully grown.\n",
        "Approach\tStops the tree growth early by imposing constraints.\tPrunes branches from a fully grown tree.\n",
        "Control Criteria\tUses conditions like maximum depth, minimum samples, or minimum impurity improvement to stop splits.\tUses validation data or cost-complexity criteria to prune subtrees.\n",
        "Complexity of Tree\tProduces a smaller tree directly by limiting growth.\tProduces a larger initial tree, then simplifies it.\n",
        "Overfitting Risk\tReduces the risk of overfitting early but may underfit if stopped too soon.\tAllows the tree to overfit first, then reduces overfitting.\n",
        "Computational Cost\tLower, as unnecessary splits are avoided upfront.\tHigher, as the tree must first be fully constructed.\n",
        "Dependency on Data\tDoes not require a fully grown tree or additional pruning logic.\tRelies on validation data or specific pruning algorithms.\n",
        "Ease of Interpretation\tOften easier to control as constraints are applied during growth.\tPruned trees are simplified post-construction, requiring additional steps.\n",
        "Advantages and Disadvantages\n",
        "Feature\tPre-Pruning\tPost-Pruning\n",
        "Advantages\t- Reduces training time and model size.\n",
        "- Prevents overfitting early.\n",
        "- Simple to implement.\t- Produces a tree that fully explores the data.\n",
        "- More robust and flexible.\n",
        "- Better generalization due to validation-based pruning.\n",
        "Disadvantages\t- Can lead to underfitting if the constraints are too strict.\n",
        "- May miss important splits.\t- Higher computational cost due to growing and then pruning.\n",
        "- Requires additional data (e.g., validation set).\n",
        "When to Use Each\n",
        "Pre-Pruning:\n",
        "\n",
        "Use when computational efficiency is critical, or when you want to restrict tree complexity upfront (e.g., real-time systems).\n",
        "Useful for datasets where overfitting is less of a concern or when the structure of the tree needs to be simple.\n",
        "Post-Pruning:\n",
        "\n",
        "Use when you want a more accurate model and are okay with additional computational cost.\n",
        "Suitable when you have validation data to evaluate and prune the tree for better generalization.\n",
        "Example Scenario\n",
        "Pre-Pruning:\n",
        "You set\n",
        "𝑚\n",
        "𝑎\n",
        "𝑥\n",
        "_\n",
        "𝑑\n",
        "𝑒\n",
        "𝑝\n",
        "𝑡\n",
        "ℎ\n",
        "=\n",
        "5\n",
        "max_depth=5 and\n",
        "𝑚\n",
        "𝑖\n",
        "𝑛\n",
        "_\n",
        "𝑠\n",
        "𝑎\n",
        "𝑚\n",
        "𝑝\n",
        "𝑙\n",
        "𝑒\n",
        "𝑠\n",
        "_\n",
        "𝑠\n",
        "𝑝\n",
        "𝑙\n",
        "𝑖\n",
        "𝑡\n",
        "=\n",
        "10\n",
        "min_samples_split=10 while building a Decision Tree. The tree stops growing at depth 5 or when a node has fewer than 10 samples, ensuring that the tree is simple and computationally efficient.\n",
        "Post-Pruning:\n",
        "You grow the tree fully with no constraints. Afterward, you use validation data and prune unnecessary branches that do not improve accuracy, creating a simpler and better-generalized model.\n",
        "Conclusion\n",
        "Pre-Pruning: Stops unnecessary splits early, but risks underfitting.\n",
        "Post-Pruning: Optimizes a fully grown tree by removing redundant complexity, but is computationally expensive."
      ],
      "metadata": {
        "id": "1xlhqmcblrDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A Decision Tree Regressor is a type of Decision Tree model used for regression tasks, where the goal is to predict a continuous numeric value (e.g., price, temperature, sales, etc.). Unlike a Decision Tree for classification, which assigns samples to discrete categories, a Decision Tree Regressor predicts the target as a numeric value based on the input features.\n",
        "\n",
        "How a Decision Tree Regressor Works\n",
        "Recursive Splitting:\n",
        "\n",
        "The dataset is recursively split into subsets based on input features to minimize a specific loss function (e.g., mean squared error or mean absolute error).\n",
        "At each node, the algorithm identifies the feature and split point that best reduces the impurity (e.g., variance or error) of the target variable.\n",
        "Leaf Nodes:\n",
        "\n",
        "Once splitting stops (based on stopping criteria such as tree depth, minimum samples per split, or minimum impurity), the remaining data points in a leaf node are used to compute the predicted value.\n",
        "The predicted value is typically the mean (or median) of the target values in the leaf node.\n",
        "Prediction:\n",
        "\n",
        "For a new input sample, the tree is traversed from the root to a leaf based on the feature values of the sample.\n",
        "The prediction for the sample is the value of the leaf node reached.\n",
        "Splitting Criteria in Decision Tree Regression\n",
        "Instead of using metrics like Gini Impurity or Entropy (used in classification trees), regression trees use metrics to minimize the variability of the target values in a subset. Common metrics include:\n",
        "\n",
        "Mean Squared Error (MSE):\n",
        "\n",
        "Measures the variance within the subsets:\n",
        "𝑀\n",
        "𝑆\n",
        "𝐸\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "(\n",
        "𝑦\n",
        "𝑖\n",
        "−\n",
        "𝑦\n",
        "ˉ\n",
        ")\n",
        "2\n",
        "MSE=\n",
        "N\n",
        "1\n",
        "​\n",
        "\n",
        "i=1\n",
        "∑\n",
        "N\n",
        "​\n",
        " (y\n",
        "i\n",
        "​\n",
        " −\n",
        "y\n",
        "ˉ\n",
        "​\n",
        " )\n",
        "2\n",
        "\n",
        "Where\n",
        "𝑦\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "​\n",
        "  are the target values,\n",
        "𝑦\n",
        "ˉ\n",
        "y\n",
        "ˉ\n",
        "​\n",
        "  is the mean target value in the subset, and\n",
        "𝑁\n",
        "N is the number of samples.\n",
        "Mean Absolute Error (MAE):\n",
        "\n",
        "Measures the average absolute deviation:\n",
        "𝑀\n",
        "𝐴\n",
        "𝐸\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "∣\n",
        "𝑦\n",
        "𝑖\n",
        "−\n",
        "𝑦\n",
        "ˉ\n",
        "∣\n",
        "MAE=\n",
        "N\n",
        "1\n",
        "​\n",
        "\n",
        "i=1\n",
        "∑\n",
        "N\n",
        "​\n",
        " ∣y\n",
        "i\n",
        "​\n",
        " −\n",
        "y\n",
        "ˉ\n",
        "​\n",
        " ∣\n",
        "Reduction in Variance:\n",
        "\n",
        "The algorithm chooses splits that maximize the reduction in variance of the target values after splitting.\n",
        "Stopping Criteria\n",
        "To avoid overfitting, Decision Tree Regressors rely on constraints such as:\n",
        "\n",
        "Maximum Depth (\n",
        "𝑚\n",
        "𝑎\n",
        "𝑥\n",
        "_\n",
        "𝑑\n",
        "𝑒\n",
        "𝑝\n",
        "𝑡\n",
        "ℎ\n",
        "max_depth):\n",
        "\n",
        "Limits the depth of the tree.\n",
        "Minimum Samples per Split (\n",
        "𝑚\n",
        "𝑖\n",
        "𝑛\n",
        "_\n",
        "𝑠\n",
        "𝑎\n",
        "𝑚\n",
        "𝑝\n",
        "𝑙\n",
        "𝑒\n",
        "𝑠\n",
        "_\n",
        "𝑠\n",
        "𝑝\n",
        "𝑙\n",
        "𝑖\n",
        "𝑡\n",
        "min_samples_split):\n",
        "\n",
        "Requires a minimum number of samples to consider a split.\n",
        "Minimum Samples per Leaf (\n",
        "𝑚\n",
        "𝑖\n",
        "𝑛\n",
        "_\n",
        "𝑠\n",
        "𝑎\n",
        "𝑚\n",
        "𝑝\n",
        "𝑙\n",
        "𝑒\n",
        "𝑠\n",
        "_\n",
        "𝑙\n",
        "𝑒\n",
        "𝑎\n",
        "𝑓\n",
        "min_samples_leaf):\n",
        "\n",
        "Ensures leaf nodes have at least a minimum number of samples.\n",
        "Minimum Impurity Decrease (\n",
        "𝑚\n",
        "𝑖\n",
        "𝑛\n",
        "_\n",
        "𝑖\n",
        "𝑚\n",
        "𝑝\n",
        "𝑢\n",
        "𝑟\n",
        "𝑖\n",
        "𝑡\n",
        "𝑦\n",
        "_\n",
        "𝑑\n",
        "𝑒\n",
        "𝑐\n",
        "𝑟\n",
        "𝑒\n",
        "𝑎\n",
        "𝑠\n",
        "𝑒\n",
        "min_impurity_decrease):\n",
        "\n",
        "Splits occur only if the reduction in impurity exceeds a threshold.\n",
        "Advantages of Decision Tree Regressors\n",
        "Interpretable:\n",
        "\n",
        "Easy to understand and visualize, as decisions are made in a hierarchical structure.\n",
        "Non-linear Relationships:\n",
        "\n",
        "Can model complex, non-linear relationships between features and the target variable.\n",
        "No Feature Scaling Required:\n",
        "\n",
        "Works without the need for normalization or standardization of input features.\n",
        "Handles Mixed Data:\n",
        "\n",
        "Can work with both numerical and categorical features.\n",
        "Disadvantages of Decision Tree Regressors\n",
        "Overfitting:\n",
        "\n",
        "Fully grown trees tend to overfit the training data unless pruned or constrained.\n",
        "Instability:\n",
        "\n",
        "Small changes in the data can lead to entirely different splits and trees.\n",
        "Lack of Smooth Predictions:\n",
        "\n",
        "Predictions are constant within regions, leading to abrupt changes at split boundaries.\n",
        "Bias Toward Features with Many Values:\n",
        "\n",
        "Features with a high number of unique values may dominate splits."
      ],
      "metadata": {
        "id": "9aWLtDInl98j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Advantages and Disadvantages of Decision Trees\n",
        "Advantages\n",
        "Simplicity and Interpretability:\n",
        "\n",
        "Decision Trees are easy to understand and interpret, even for non-technical users.\n",
        "The hierarchical structure clearly shows how decisions are made, making it simple to visualize.\n",
        "No Need for Feature Scaling:\n",
        "\n",
        "Decision Trees do not require normalization or standardization of data, as they are not affected by the magnitude of feature values.\n",
        "Handles Mixed Data:\n",
        "\n",
        "Decision Trees can handle both numerical and categorical data without any special preprocessing.\n",
        "Non-linear Relationships:\n",
        "\n",
        "They can model non-linear and complex relationships between features and the target variable.\n",
        "Feature Importance:\n",
        "\n",
        "Decision Trees can identify the most significant features used in decision-making, helping with feature selection.\n",
        "Works Well with Small Data:\n",
        "\n",
        "Suitable for small to medium-sized datasets and can achieve good results without extensive computation.\n",
        "Versatility:\n",
        "\n",
        "Can be used for both classification and regression tasks.\n",
        "No Assumptions About Data:\n",
        "\n",
        "Unlike linear models, Decision Trees make no assumptions about the distribution of the data or the relationship between input features.\n",
        "Disadvantages\n",
        "Overfitting:\n",
        "\n",
        "Decision Trees tend to overfit the training data, especially when grown to full depth without pruning.\n",
        "Instability:\n",
        "\n",
        "Small changes in the training data can result in completely different trees due to the greedy splitting algorithm.\n",
        "Bias Toward Features with Many Values:\n",
        "\n",
        "Features with more unique values (e.g., continuous variables) can dominate the splits, potentially overlooking categorical features.\n",
        "Lack of Generalization:\n",
        "\n",
        "Fully grown trees often do not generalize well to unseen data unless pruned or constrained (e.g., by limiting depth).\n",
        "Greedy Algorithm Limitations:\n",
        "\n",
        "Splits are determined using a greedy algorithm, which may not result in the globally optimal tree.\n",
        "Limited Prediction Smoothness:\n",
        "\n",
        "For regression tasks, Decision Trees predict constant values within split regions, leading to abrupt changes and less smooth predictions.\n",
        "Computational Cost for Large Trees:\n",
        "\n",
        "Training a fully grown tree on a large dataset can be computationally expensive.\n",
        "Curse of Dimensionality:\n",
        "\n",
        "Performance can degrade when the dataset has many irrelevant features or high dimensionality, as the tree might split on unimportant features.\n",
        "Summary Table\n",
        "Advantages\tDisadvantages\n",
        "Simple to understand and interpret\tProne to overfitting without pruning\n",
        "No need for feature scaling\tUnstable to small changes in data\n",
        "Handles both numerical and categorical data\tBias toward features with many unique values\n",
        "Can model non-linear relationships\tGreedy algorithm may not find global optimum\n",
        "Identifies feature importance\tPredictions are not smooth in regression\n",
        "Works well with small datasets\tComputationally expensive for large trees\n",
        "When to Use Decision Trees\n",
        "Use Decision Trees when:\n",
        "Interpretability is important.\n",
        "You have mixed types of data (categorical and numerical).\n",
        "The dataset is small to medium in size.\n",
        "You want a quick, baseline model for classification or regression.\n",
        "Alternative Models\n",
        "If Decision Trees' disadvantages (e.g., overfitting, instability) are problematic, consider:\n",
        "\n",
        "Random Forests:\n",
        "Reduces overfitting and instability by averaging multiple Decision Trees.\n",
        "Gradient Boosting Machines (GBMs):\n",
        "Builds trees sequentially to optimize performance and reduce error.\n",
        "Linear Models:\n",
        "Suitable for simpler tasks where relationships are linear.\n",
        "Support Vector Machines (SVMs):\n",
        "Good for high-dimensional data but requires feature scaling."
      ],
      "metadata": {
        "id": "rUvXDSxvmOE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Decision Trees have mechanisms to handle missing values during both training and prediction phases. Here's how they handle missing data effectively:\n",
        "\n",
        "1. During Training\n",
        "a) Ignore Missing Values in Splitting:\n",
        "When evaluating a potential split at a node, the Decision Tree can ignore data points with missing values for the splitting feature.\n",
        "Splitting criteria (e.g., Gini Impurity or Entropy for classification, or variance reduction for regression) are calculated only on samples with non-missing values for the feature being evaluated.\n",
        "b) Surrogate Splits:\n",
        "A surrogate split is a backup splitting rule used when the primary splitting feature has missing values.\n",
        "The algorithm identifies alternative features (surrogates) that are most correlated with the primary split and uses them to assign samples with missing values to one of the child nodes.\n",
        "This technique ensures that samples with missing values are still assigned a path in the tree.\n",
        "c) Imputation:\n",
        "Some implementations may impute missing values during training using strategies such as:\n",
        "Mean or median imputation (for numerical data).\n",
        "Mode imputation (for categorical data).\n",
        "Custom imputation methods, such as using domain knowledge.\n",
        "2. During Prediction\n",
        "a) Follow the Majority Path:\n",
        "When encountering a missing value for a splitting feature during prediction, the algorithm can follow the branch with the majority of the training samples from that node.\n",
        "This is a simple and commonly used heuristic.\n",
        "b) Weighted Distribution:\n",
        "Another approach is to distribute the prediction across all possible branches, weighted by the proportion of samples that went down each branch during training.\n",
        "This approach avoids making a hard decision and considers all potential outcomes.\n",
        "c) Surrogate Splits:\n",
        "Similar to training, surrogate splits can be used during prediction. If the primary feature's value is missing, the decision is made based on the surrogate feature.\n",
        "Example Handling in Scikit-learn\n",
        "Scikit-learn's DecisionTreeClassifier and DecisionTreeRegressor do not natively handle missing values (e.g., NaNs). Instead, you need to preprocess the data to handle missing values, for instance:\n",
        "\n",
        "Impute missing values:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Example dataset with missing values\n",
        "X = [[1, 2], [3, None], [7, 6], [None, 5]]\n",
        "y = [0, 1, 0, 1]\n",
        "\n",
        "# Impute missing values with mean\n",
        "imputer = SimpleImputer(strategy=\"mean\")\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# Train a Decision Tree\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_imputed, y)\n",
        "Use libraries with native missing value support:\n",
        "\n",
        "Libraries like XGBoost, LightGBM, and CatBoost have built-in mechanisms to handle missing values without imputation.\n",
        "Advantages of Decision Trees in Handling Missing Values\n",
        "Flexibility:\n",
        "\n",
        "Decision Trees can effectively handle missing values through surrogate splits or ignoring missing data points for splitting.\n",
        "No Need for Extensive Preprocessing:\n",
        "\n",
        "Some implementations can work directly with datasets containing missing values.\n",
        "Localized Handling:\n",
        "\n",
        "Missing values are handled node by node, reducing the impact on the overall model.\n",
        "Limitations\n",
        "Dependent on Implementation:\n",
        "\n",
        "Not all Decision Tree implementations (e.g., in Scikit-learn) natively support missing values.\n",
        "Preprocessing is required in such cases.\n",
        "Accuracy Trade-off:\n",
        "\n",
        "Ignoring missing values or relying on surrogate splits can lead to slight reductions in accuracy, especially if a large proportion of data is missing."
      ],
      "metadata": {
        "id": "2LbRGqkgmnXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Decision Trees can handle categorical features naturally, as they don't rely on numerical properties like distance or scale. Here's how Decision Trees deal with categorical data:\n",
        "\n",
        "1. Splitting on Categorical Features\n",
        "When a feature is categorical (e.g., \"Color\" with values like \"Red,\" \"Green,\" \"Blue\"), the Decision Tree splits based on categories rather than numerical thresholds.\n",
        "\n",
        "a) Binary Splitting (Two Groups)\n",
        "The Decision Tree splits the feature into two groups:\n",
        "For example, if the \"Color\" feature has values\n",
        "{\n",
        "𝑅\n",
        "𝑒\n",
        "𝑑\n",
        ",\n",
        "𝐺\n",
        "𝑟\n",
        "𝑒\n",
        "𝑒\n",
        "𝑛\n",
        ",\n",
        "𝐵\n",
        "𝑙\n",
        "𝑢\n",
        "𝑒\n",
        "}\n",
        "{Red,Green,Blue}, the algorithm may decide on a split like:\n",
        "Group 1:\n",
        "{\n",
        "𝑅\n",
        "𝑒\n",
        "𝑑\n",
        ",\n",
        "𝐺\n",
        "𝑟\n",
        "𝑒\n",
        "𝑒\n",
        "𝑛\n",
        "}\n",
        "{Red,Green}\n",
        "Group 2:\n",
        "{\n",
        "𝐵\n",
        "𝑙\n",
        "𝑢\n",
        "𝑒\n",
        "}\n",
        "{Blue}\n",
        "The algorithm determines the best grouping by calculating impurity (e.g., Gini Impurity, Entropy) for all possible splits and selecting the one that minimizes impurity.\n",
        "b) Multiway Splitting (One Group Per Category)\n",
        "The tree can also split the data into multiple branches, one for each unique category.\n",
        "For example:\n",
        "If \"Color\" has three categories (\n",
        "{\n",
        "𝑅\n",
        "𝑒\n",
        "𝑑\n",
        ",\n",
        "𝐺\n",
        "𝑟\n",
        "𝑒\n",
        "𝑒\n",
        "𝑛\n",
        ",\n",
        "𝐵\n",
        "𝑙\n",
        "𝑢\n",
        "𝑒\n",
        "}\n",
        "{Red,Green,Blue}), the node splits into three child nodes:\n",
        "One for\n",
        "𝐶\n",
        "𝑜\n",
        "𝑙\n",
        "𝑜\n",
        "𝑟\n",
        "=\n",
        "𝑅\n",
        "𝑒\n",
        "𝑑\n",
        "Color=Red\n",
        "One for\n",
        "𝐶\n",
        "𝑜\n",
        "𝑙\n",
        "𝑜\n",
        "𝑟\n",
        "=\n",
        "𝐺\n",
        "𝑟\n",
        "𝑒\n",
        "𝑒\n",
        "𝑛\n",
        "Color=Green\n",
        "One for\n",
        "𝐶\n",
        "𝑜\n",
        "𝑙\n",
        "𝑜\n",
        "𝑟\n",
        "=\n",
        "𝐵\n",
        "𝑙\n",
        "𝑢\n",
        "𝑒\n",
        "Color=Blue\n",
        "Multiway splits are common in Decision Tree implementations like Scikit-learn.\n",
        "2. Encoding Categorical Features\n",
        "Some Decision Tree implementations (e.g., Scikit-learn) require categorical features to be encoded numerically before training. Encoding methods include:\n",
        "\n",
        "a) One-Hot Encoding\n",
        "Converts each category into a separate binary feature (e.g., \"Color\" with values \"Red,\" \"Green,\" \"Blue\" becomes three features:\n",
        "𝐶\n",
        "𝑜\n",
        "𝑙\n",
        "𝑜\n",
        "𝑟\n",
        "_\n",
        "𝑅\n",
        "𝑒\n",
        "𝑑\n",
        ",\n",
        "𝐶\n",
        "𝑜\n",
        "𝑙\n",
        "𝑜\n",
        "𝑟\n",
        "_\n",
        "𝐺\n",
        "𝑟\n",
        "𝑒\n",
        "𝑒\n",
        "𝑛\n",
        ",\n",
        "𝐶\n",
        "𝑜\n",
        "𝑙\n",
        "𝑜\n",
        "𝑟\n",
        "_\n",
        "𝐵\n",
        "𝑙\n",
        "𝑢\n",
        "𝑒\n",
        "Color_Red,Color_Green,Color_Blue).\n",
        "Example:\n",
        "Original Feature: \"Color\"\n",
        "→\n",
        "[Red, Green, Blue]\n",
        "Original Feature: \"Color\"→[Red, Green, Blue]\n",
        "One-Hot Encoded:\n",
        "→\n",
        "[\n",
        "1\n",
        ",\n",
        "0\n",
        ",\n",
        "0\n",
        "]\n",
        "\n",
        "(\n",
        "Red\n",
        ")\n",
        ",\n",
        "\n",
        "[\n",
        "0\n",
        ",\n",
        "1\n",
        ",\n",
        "0\n",
        "]\n",
        "\n",
        "(\n",
        "Green\n",
        ")\n",
        ",\n",
        "\n",
        "[\n",
        "0\n",
        ",\n",
        "0\n",
        ",\n",
        "1\n",
        "]\n",
        "\n",
        "(\n",
        "Blue\n",
        ")\n",
        "One-Hot Encoded:→[1,0,0] (Red), [0,1,0] (Green), [0,0,1] (Blue)\n",
        "b) Label Encoding\n",
        "Assigns a unique integer to each category (e.g., \"Red\" → 0, \"Green\" → 1, \"Blue\" → 2).\n",
        "Example:\n",
        "Original Feature: \"Color\"\n",
        "→\n",
        "[Red, Green, Blue]\n",
        "Original Feature: \"Color\"→[Red, Green, Blue]\n",
        "Label Encoded:\n",
        "→\n",
        "[\n",
        "0\n",
        ",\n",
        "1\n",
        ",\n",
        "2\n",
        "]\n",
        "Label Encoded:→[0,1,2]\n",
        "Note: This approach can introduce unintended ordinal relationships between categories (e.g.,\n",
        "𝐵\n",
        "𝑙\n",
        "𝑢\n",
        "𝑒\n",
        ">\n",
        "𝐺\n",
        "𝑟\n",
        "𝑒\n",
        "𝑒\n",
        "𝑛\n",
        ">\n",
        "𝑅\n",
        "𝑒\n",
        "𝑑\n",
        "Blue>Green>Red), which may not align with the actual data.\n",
        "c) Native Support for Categorical Features\n",
        "Some libraries like CatBoost and LightGBM can handle categorical features natively without requiring encoding.\n",
        "3. Handling Mixed Features\n",
        "Decision Trees can handle datasets with both categorical and numerical features. Each feature is evaluated independently during splitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "Plc0Z8r0nbts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Decision Trees are widely used across industries because of their interpretability and versatility. Below are some real-world applications of Decision Trees:\n",
        "\n",
        "1. Healthcare\n",
        "Disease Diagnosis:\n",
        "\n",
        "Predict whether a patient has a certain disease based on symptoms, medical history, or test results.\n",
        "Example: Classifying whether a tumor is malignant or benign.\n",
        "Treatment Recommendations:\n",
        "\n",
        "Suggest personalized treatment plans based on patient characteristics and medical conditions.\n",
        "Risk Assessment:\n",
        "\n",
        "Estimate the likelihood of a patient developing a chronic condition (e.g., diabetes, heart disease).\n",
        "2. Finance\n",
        "Credit Scoring:\n",
        "\n",
        "Assess the creditworthiness of an individual based on factors like income, debt, and repayment history.\n",
        "Fraud Detection:\n",
        "\n",
        "Identify suspicious transactions or patterns that indicate fraudulent activity.\n",
        "Loan Approval:\n",
        "\n",
        "Determine whether a loan application should be approved based on financial metrics and applicant profiles.\n",
        "Investment Decisions:\n",
        "\n",
        "Predict stock price trends or portfolio risks.\n",
        "3. Marketing and Customer Analytics\n",
        "Customer Segmentation:\n",
        "\n",
        "Group customers based on purchasing behavior, preferences, or demographics to create targeted marketing campaigns.\n",
        "Churn Prediction:\n",
        "\n",
        "Identify customers who are likely to stop using a service and implement retention strategies.\n",
        "Recommendation Systems:\n",
        "\n",
        "Suggest products or services to customers based on past behavior and preferences.\n",
        "Sales Forecasting:\n",
        "\n",
        "Predict future sales for products using historical data.\n",
        "4. E-commerce\n",
        "Dynamic Pricing:\n",
        "\n",
        "Predict optimal product pricing based on demand, competition, and customer behavior.\n",
        "Product Categorization:\n",
        "\n",
        "Classify products into categories based on features and descriptions.\n",
        "Personalized Shopping Experiences:\n",
        "\n",
        "Create user-specific recommendations or discounts.\n",
        "5. Manufacturing\n",
        "Quality Control:\n",
        "\n",
        "Predict whether a product will meet quality standards based on production parameters.\n",
        "Predictive Maintenance:\n",
        "\n",
        "Estimate when equipment is likely to fail and schedule maintenance accordingly.\n",
        "Supply Chain Optimization:\n",
        "\n",
        "Predict demand for raw materials or finished goods to minimize waste and maximize efficiency.\n",
        "6. Education\n",
        "Student Performance Prediction:\n",
        "\n",
        "Predict whether a student will pass or fail based on factors like attendance, test scores, and engagement.\n",
        "Personalized Learning:\n",
        "\n",
        "Recommend tailored learning paths based on a student's strengths and weaknesses.\n",
        "Dropout Risk Assessment:\n",
        "\n",
        "Identify students at risk of dropping out and suggest intervention strategies.\n",
        "7. Retail\n",
        "Inventory Management:\n",
        "\n",
        "Predict demand for products to optimize stock levels and reduce waste.\n",
        "Customer Behavior Analysis:\n",
        "\n",
        "Understand purchasing patterns to optimize product placement and marketing efforts.\n",
        "Store Location Analysis:\n",
        "\n",
        "Determine the best locations for new stores based on demographic and geographic data.\n",
        "8. Energy and Utilities\n",
        "Energy Consumption Forecasting:\n",
        "\n",
        "Predict energy usage for efficient grid management.\n",
        "Fault Detection:\n",
        "\n",
        "Identify faults in power grids or equipment.\n",
        "Renewable Energy Optimization:\n",
        "\n",
        "Predict the efficiency of solar panels or wind turbines based on environmental data.\n",
        "9. Agriculture\n",
        "Crop Yield Prediction:\n",
        "\n",
        "Forecast crop yields based on weather conditions, soil quality, and planting techniques.\n",
        "Pest and Disease Detection:\n",
        "\n",
        "Classify whether crops are affected by specific pests or diseases.\n",
        "Precision Farming:\n",
        "\n",
        "Recommend optimal planting and irrigation strategies for higher productivity.\n",
        "10. Transportation\n",
        "Traffic Prediction:\n",
        "\n",
        "Predict traffic patterns based on historical data and real-time inputs.\n",
        "Route Optimization:\n",
        "\n",
        "Recommend the best routes for deliveries or personal travel.\n",
        "Autonomous Vehicles:\n",
        "\n",
        "Make decisions like obstacle detection and lane changes.\n",
        "11. Entertainment\n",
        "Recommendation Systems:\n",
        "\n",
        "Suggest movies, music, or TV shows based on user preferences (e.g., Spotify, Netflix).\n",
        "Content Categorization:\n",
        "\n",
        "Classify content into genres or themes for better organization.\n",
        "12. Government and Public Policy\n",
        "Policy Impact Analysis:\n",
        "\n",
        "Predict the effects of new policies based on historical data.\n",
        "Fraud Detection in Tax Systems:\n",
        "\n",
        "Identify fraudulent tax filings or benefit claims.\n",
        "Crime Prediction and Prevention:\n",
        "\n",
        "Analyze crime patterns to allocate resources effectively.\n",
        "13. Environmental Science\n",
        "Weather Prediction:\n",
        "\n",
        "Predict temperature, rainfall, or storms using environmental data.\n",
        "Wildlife Conservation:\n",
        "\n",
        "Classify species or predict habitats under threat based on ecological data.\n",
        "Pollution Monitoring:\n",
        "\n",
        "Predict pollution levels and recommend mitigation strategies.\n",
        "14. Human Resources\n",
        "Employee Attrition Prediction:\n",
        "\n",
        "Identify employees likely to leave the organization based on performance reviews and engagement levels.\n",
        "Hiring Decisions:\n",
        "\n",
        "Evaluate candidates for specific roles based on past data and interview performance.\n"
      ],
      "metadata": {
        "id": "HGlMt7qVntnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Practical"
      ],
      "metadata": {
        "id": "Ea93vTq5n9v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1. Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "IUVZcv49oAXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "2. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier using Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print the feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "w44mYscdoPIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "3.Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier using Entropy as the splitting criterion\n",
        "clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "VKuyhWuvocjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4.Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE)?\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data  # Features\n",
        "y = housing.target  # Labels (median house value)\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate the Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "AzZtgpweo1vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5. Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Train a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Export the Decision Tree to DOT format\n",
        "dot_data = export_graphviz(\n",
        "    clf,\n",
        "    out_file=None,  # Output as a string\n",
        "    feature_names=iris.feature_names,\n",
        "    class_names=iris.target_names,\n",
        "    filled=True,  # Color nodes based on the class they represent\n",
        "    rounded=True,  # Rounded corners for better readability\n",
        "    special_characters=True\n",
        ")\n",
        "\n",
        "# Visualize the Decision Tree using Graphviz\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"decision_tree\")  # Save as a PDF file\n",
        "graph.view()  # Open the visualization in your default viewer\n"
      ],
      "metadata": {
        "id": "_QDrpL-XpEOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6.Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its\n",
        "accuracy with a fully grown tree?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree with a maximum depth of 3\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "\n",
        "# Train a fully grown Decision Tree\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with both models\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "\n",
        "# Calculate accuracy for both models\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print the accuracy results\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_limited:.2f}\")\n",
        "print(f\"Accuracy with fully grown tree: {accuracy_full:.2f}\")\n"
      ],
      "metadata": {
        "id": "KXwyYEP6pPUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7.Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its\n",
        "accuracy with a default tree?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with min_samples_split=5\n",
        "clf_custom = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "clf_custom.fit(X_train, y_train)\n",
        "\n",
        "# Train a default Decision Tree Classifier\n",
        "clf_default = DecisionTreeClassifier(random_state=42)\n",
        "clf_default.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions with both models\n",
        "y_pred_custom = clf_custom.predict(X_test)\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "\n",
        "# Calculate accuracy for both models\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Print the accuracy results\n",
        "print(f\"Accuracy with min_samples_split=5: {accuracy_custom:.2f}\")\n",
        "print(f\"Accuracy with default tree: {accuracy_default:.2f}\")\n"
      ],
      "metadata": {
        "id": "i7h7N93uprCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "8.Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its\n",
        "accuracy with unscaled data?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier without feature scaling\n",
        "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Apply feature scaling (standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a Decision Tree Classifier with scaled features\n",
        "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(f\"Accuracy without scaling: {accuracy_unscaled:.2f}\")\n",
        "print(f\"Accuracy with scaling: {accuracy_scaled:.2f}\")\n"
      ],
      "metadata": {
        "id": "Gl-UQiEqp42s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "9.Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass\n",
        "classification?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "base_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Apply One-vs-Rest (OvR) strategy\n",
        "ovr_classifier = OneVsRestClassifier(base_classifier)\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = ovr_classifier.predict(X_test)\n",
        "\n",
        "# Calculat\n"
      ],
      "metadata": {
        "id": "iPzG_W_GqJ7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Display feature importance scores\n",
        "print(\"Feature Importance Scores:\")\n",
        "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "jNrYP6QIqKyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "11.Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance\n",
        "with an unrestricted tree?\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor with max_depth=5\n",
        "reg_limited = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "reg_limited.fit(X_train, y_train)\n",
        "\n",
        "# Train an unrestricted Decision Tree Regressor\n",
        "reg_full = DecisionTreeRegressor(random_state=42)\n",
        "reg_full.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_limited = reg_limited.predict(X_test)\n",
        "y_pred_full = reg_full.predict(X_test)\n",
        "\n",
        "# Evaluate performance using Mean Squared Error\n",
        "mse_limited = mean_squared_error(y_test, y_pred_limited)\n",
        "mse_full = mean_squar\n"
      ],
      "metadata": {
        "id": "KzSeDEXLqKrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "12.Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and\n",
        "visualize its effect on accuracy?\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier without pruning to get effective alphas for pruning\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas  # Effective alphas\n",
        "impurities = path.impurities  # Total impurity at each alpha\n",
        "\n",
        "# Train and evaluate a tree for each alpha\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    train_accuracies.append(clf.score(X_train, y_train))\n",
        "    test_accuracies.append(clf.score(X_test, y_test))\n",
        "\n",
        "# Plot the effect of CCP on accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(ccp_alphas, train_accuracies, label=\"Train Accuracy\", marker=\"o\")\n",
        "plt.plot(ccp_alphas, test_accuracies, label=\"Test Accuracy\", marker=\"o\")\n",
        "plt.xlabel(\"CCP Alpha\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of Cost Complexity Pruning (CCP) on Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MPpqERrJqKnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "13.Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,\n",
        "Recall, and F1-Score?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
        "recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
        "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "\n",
        "# Display the results\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "# Detailed Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "id": "pZ6p6lzNqKmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred, labels=clf.classes_)\n",
        "\n",
        "# Visualize the\n"
      ],
      "metadata": {
        "id": "SfcECVToqKiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "14.Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values\n",
        "for max_depth and min_samples_split.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    \"max_depth\": [2, 3, 4, 5, 6, None],\n",
        "    \"min_samples_split\": [2, 5, 10, 20],\n",
        "}\n",
        "\n",
        "# Initialize a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=\"accuracy\", cv=5, verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the best estimator\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "id": "9QUpoYVhqKgm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}