{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAwQOVbZyIyy"
      },
      "outputs": [],
      "source": [
        "Theoretical\n",
        "1. What is Logistic Regression, and how does it differ from Linear Regression.\n",
        "Logistic Regression and Linear Regression are both supervised learning algorithms used for different types of prediction tasks. Here's an overview of each and their key differences:\n",
        "\n",
        "1. Logistic Regression\n",
        "Purpose: Used for classification tasks, particularly for binary classification (e.g., Yes/No, 0/1).\n",
        "\n",
        "Output: Produces probabilities that map to a class label, typically between 0 and 1.\n",
        "\n",
        "Equation: Uses the logistic (sigmoid) function to transform the linear equation into a probability:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        ")\n",
        "P(y=1∣x)=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "Where\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        "  is the intercept,\n",
        "𝛽\n",
        "1\n",
        ",\n",
        "𝛽\n",
        "2\n",
        ",\n",
        "…\n",
        "β\n",
        "1\n",
        "​\n",
        " ,β\n",
        "2\n",
        "​\n",
        " ,… are the coefficients, and\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        "…\n",
        "x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,… are the input features.\n",
        "\n",
        "Key Features:\n",
        "\n",
        "Maps predictions to probabilities using the sigmoid function.\n",
        "Outputs a probability threshold (e.g., 0.5) to decide class labels.\n",
        "Can be extended to multi-class classification using techniques like one-vs-rest (OvR) or softmax (for multinomial logistic regression).\n",
        "2. Linear Regression\n",
        "Purpose: Used for regression tasks, predicting continuous outcomes (e.g., house prices, sales, etc.).\n",
        "\n",
        "Output: Predicts a continuous value.\n",
        "\n",
        "Equation: A straight-line equation:\n",
        "\n",
        "𝑦\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "\n",
        "Where\n",
        "𝑦\n",
        "y is the target variable,\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        "  is the intercept, and\n",
        "𝛽\n",
        "1\n",
        ",\n",
        "𝛽\n",
        "2\n",
        ",\n",
        "…\n",
        "β\n",
        "1\n",
        "​\n",
        " ,β\n",
        "2\n",
        "​\n",
        " ,… are the coefficients.\n",
        "\n",
        "Key Features:\n",
        "\n",
        "Models the linear relationship between input variables and the target variable.\n",
        "Does not include probability-based outputs or classification thresholds.\n",
        "Key Differences\n",
        "Aspect\tLogistic Regression\tLinear Regression\n",
        "Goal\tClassification (e.g., Yes/No, Spam/Not Spam)\tRegression (e.g., predicting a numerical value)\n",
        "Output\tProbability values (0 to 1)\tContinuous values\n",
        "Equation\tSigmoid function applied to linear equation\tLinear equation directly\n",
        "Loss Function\tLog-loss (cross-entropy)\tMean Squared Error (MSE)\n",
        "Assumption on Data\tAssumes a relationship between features and class probability\tAssumes a linear relationship between features and output\n",
        "Use Cases\tBinary/multi-class classification (e.g., spam detection)\tPredicting continuous outcomes (e.g., stock prices)\n",
        "Example:\n",
        "Linear Regression: Predicting the price of a house based on size and location.\n",
        "Logistic Regression: Predicting whether an email is spam or not based on its content.\n",
        "\n",
        "The mathematical equation for Logistic Regression is derived from the logistic (sigmoid) function, which maps any real-valued number to a probability between 0 and 1. Here's the equation:\n",
        "\n",
        "1. Logistic Regression Hypothesis\n",
        "The logistic regression model predicts the probability\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "P(y=1∣x) as:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        ")\n",
        "P(y=1∣x)=\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "P(y=1∣x): The probability that the target\n",
        "𝑦\n",
        "y belongs to class 1 given the input\n",
        "𝑥\n",
        "x.\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        " : The intercept (bias term).\n",
        "𝛽\n",
        "1\n",
        ",\n",
        "𝛽\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝛽\n",
        "𝑛\n",
        "β\n",
        "1\n",
        "​\n",
        " ,β\n",
        "2\n",
        "​\n",
        " ,…,β\n",
        "n\n",
        "​\n",
        " : The coefficients (weights) of the features\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        "x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,…,x\n",
        "n\n",
        "​\n",
        " .\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        "x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,…,x\n",
        "n\n",
        "​\n",
        " : The feature variables.\n",
        "𝑒\n",
        "e: The base of the natural logarithm.\n",
        "2. Logistic Function\n",
        "The logistic regression equation can also be written using the logit function, which is the natural logarithm of the odds:\n",
        "\n",
        "logit\n",
        "(\n",
        "𝑃\n",
        ")\n",
        "=\n",
        "ln\n",
        "⁡\n",
        "(\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "1\n",
        "−\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "logit(P)=ln(\n",
        "1−P(y=1∣x)\n",
        "P(y=1∣x)\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "1\n",
        "−\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "1−P(y=1∣x)\n",
        "P(y=1∣x)\n",
        "​\n",
        "  is the odds of\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1 occurring.\n",
        "3. Decision Rule\n",
        "To classify an observation, logistic regression typically uses a threshold:\n",
        "\n",
        "If\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        ">\n",
        "0.5\n",
        "P(y=1∣x)>0.5, predict\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1.\n",
        "If\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "≤\n",
        "0.5\n",
        "P(y=1∣x)≤0.5, predict\n",
        "𝑦\n",
        "=\n",
        "0\n",
        "y=0.\n",
        "This threshold can be adjusted depending on the problem (e.g., in imbalanced datasets).\n",
        "\n",
        "3. Why do we use the Sigmoid function in Logistic Regression.\n",
        "The sigmoid function is used in Logistic Regression because it maps any real-valued number to a value between 0 and 1, which is ideal for modeling probabilities. Here's a detailed explanation of why the sigmoid function is crucial in Logistic Regression:\n",
        "\n",
        "1. Maps Outputs to a Probability Range\n",
        "In Logistic Regression, we want the output to represent a probability\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "P(y=1∣x), which must lie in the range [0, 1].\n",
        "The sigmoid function:\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "𝑧\n",
        "σ(z)=\n",
        "1+e\n",
        "−z\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "ensures that the output is always between 0 and 1, regardless of the value of\n",
        "𝑧\n",
        "z (the linear combination of inputs).\n",
        "2. Interpretable Probabilities\n",
        "The sigmoid function provides an interpretable probability for binary classification.\n",
        "A value close to 1 means a high probability of belonging to class 1.\n",
        "A value close to 0 means a high probability of belonging to class 0.\n",
        "This enables decision-making using a simple threshold (e.g.,\n",
        "𝑃\n",
        ">\n",
        "0.5\n",
        "P>0.5).\n",
        "3. Handles Non-Linearity\n",
        "Logistic Regression applies the sigmoid function to the linear equation\n",
        "𝑧\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "z=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " , effectively introducing non-linearity into the model.\n",
        "Without the sigmoid, the output of the linear equation could be any real number, which cannot be interpreted as a probability.\n",
        "4. Smooth Gradient for Optimization\n",
        "The sigmoid function is differentiable, which makes it suitable for optimization using gradient descent.\n",
        "During model training, the derivative of the sigmoid function helps compute gradients for adjusting the model's weights.\n",
        "5. Connects to the Logit Function\n",
        "The sigmoid function is mathematically tied to the logit function, which models the log-odds of the probability:\n",
        "ln\n",
        "⁡\n",
        "(\n",
        "𝑃\n",
        "1\n",
        "−\n",
        "𝑃\n",
        ")\n",
        "=\n",
        "𝑧\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "ln(\n",
        "1−P\n",
        "P\n",
        "​\n",
        " )=z=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "\n",
        "The sigmoid function transforms the linear log-odds into a probability, making it the perfect choice for classification tasks.\n",
        "Visual Intuition\n",
        "The sigmoid function has an \"S-shaped\" curve:\n",
        "\n",
        "For large negative inputs (\n",
        "𝑧\n",
        "z),\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "≈\n",
        "0\n",
        "σ(z)≈0.\n",
        "For large positive inputs (\n",
        "𝑧\n",
        "z),\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "≈\n",
        "1\n",
        "σ(z)≈1.\n",
        "For inputs near 0, the sigmoid function produces probabilities around 0.5.\n",
        "This shape is ideal for modeling binary outcomes, where we want to distinguish between two classes.\n",
        "\n",
        "The cost function in Logistic Regression is designed to measure how well the predicted probabilities match the actual labels. Unlike Linear Regression, which uses Mean Squared Error (MSE), Logistic Regression uses the log-loss or logarithmic loss function, also called the binary cross-entropy loss for binary classification.\n",
        "\n",
        "1. Why Not Use Mean Squared Error (MSE)?\n",
        "Using MSE in Logistic Regression leads to a non-convex cost function, making optimization harder and convergence unreliable.\n",
        "Instead, the log-loss function ensures a convex cost function, allowing gradient descent to find the global minimum.\n",
        "2. Logistic Regression Cost Function\n",
        "The cost function for Logistic Regression is derived from the likelihood function, which models the probability of the observed data. It is given as:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑦\n",
        "^\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "^\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "]\n",
        "J(β)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "\n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " log(\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "(i)\n",
        " )+(1−y\n",
        "(i)\n",
        " )log(1−\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "(i)\n",
        " )]\n",
        "Where:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "J(β): The cost function.\n",
        "𝑚\n",
        "m: The number of training examples.\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "y\n",
        "(i)\n",
        " : The true label for the\n",
        "𝑖\n",
        "i-th training example (\n",
        "0\n",
        "0 or\n",
        "1\n",
        "1).\n",
        "𝑦\n",
        "^\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "(i)\n",
        " : The predicted probability for the\n",
        "𝑖\n",
        "i-th training example, given by the sigmoid function:\n",
        "𝑦\n",
        "^\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "𝑒\n",
        "−\n",
        "(\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "(i)\n",
        " =\n",
        "1+e\n",
        "−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "(i)\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "(i)\n",
        "​\n",
        " )\n",
        "\n",
        "1\n",
        "​\n",
        "\n",
        "3. Interpretation of the Cost Function\n",
        "The cost function penalizes the model more for incorrect predictions with high confidence:\n",
        "If\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1 and\n",
        "𝑦\n",
        "^\n",
        "y\n",
        "^\n",
        "​\n",
        "  is close to 0, the cost is very high (since\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑦\n",
        "^\n",
        ")\n",
        "log(\n",
        "y\n",
        "^\n",
        "​\n",
        " ) is large and negative).\n",
        "If\n",
        "𝑦\n",
        "=\n",
        "0\n",
        "y=0 and\n",
        "𝑦\n",
        "^\n",
        "y\n",
        "^\n",
        "​\n",
        "  is close to 1, the cost is similarly very high.\n",
        "Correct predictions (e.g.,\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1 and\n",
        "𝑦\n",
        "^\n",
        "≈\n",
        "1\n",
        "y\n",
        "^\n",
        "​\n",
        " ≈1) result in a cost close to 0.\n",
        "4. Simplified for One Training Example\n",
        "For a single training example, the cost function simplifies to:\n",
        "\n",
        "Cost\n",
        "(\n",
        "𝑦\n",
        "^\n",
        ",\n",
        "𝑦\n",
        ")\n",
        "=\n",
        "−\n",
        "[\n",
        "𝑦\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑦\n",
        "^\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "^\n",
        ")\n",
        "]\n",
        "Cost(\n",
        "y\n",
        "^\n",
        "​\n",
        " ,y)=−[ylog(\n",
        "y\n",
        "^\n",
        "​\n",
        " )+(1−y)log(1−\n",
        "y\n",
        "^\n",
        "​\n",
        " )]\n",
        "This is:\n",
        "\n",
        "−\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑦\n",
        "^\n",
        ")\n",
        "−log(\n",
        "y\n",
        "^\n",
        "​\n",
        " ) if\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1,\n",
        "−\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "^\n",
        ")\n",
        "−log(1−\n",
        "y\n",
        "^\n",
        "​\n",
        " ) if\n",
        "𝑦\n",
        "=\n",
        "0\n",
        "y=0.\n",
        "5. Convexity of the Cost Function\n",
        "The log-loss cost function is convex, meaning it has a single global minimum. This makes optimization using gradient descent efficient and reliable.\n",
        "6. Why Use Logarithms?\n",
        "Logarithms turn multiplication in the likelihood into addition, simplifying computation.\n",
        "The logarithm amplifies small probability errors, penalizing confident but incorrect predictions more heavily.\n",
        "\n",
        "Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty term to the cost function. It discourages the model from relying too heavily on any particular feature or fitting the noise in the training data.\n",
        "\n",
        "1. What is Regularization?\n",
        "Regularization adds a term to the cost function to penalize large model weights (coefficients). By doing this, the model learns simpler patterns that generalize better to unseen data. The modified cost function becomes:\n",
        "\n",
        "𝐽\n",
        "regularized\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "=\n",
        "𝐽\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "+\n",
        "𝜆\n",
        "⋅\n",
        "Penalty\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "J\n",
        "regularized\n",
        "​\n",
        " (β)=J(β)+λ⋅Penalty(β)\n",
        "Where:\n",
        "\n",
        "𝐽\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "J(β): Original cost function.\n",
        "𝜆\n",
        "λ: Regularization strength (hyperparameter controlling the penalty).\n",
        "Penalty\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "Penalty(β): The penalty term based on the coefficients (\n",
        "𝛽\n",
        "β).\n",
        "2. Types of Regularization\n",
        "There are two main types of regularization commonly used in Logistic Regression:\n",
        "\n",
        "a. L1 Regularization (Lasso)\n",
        "Adds the sum of the absolute values of the coefficients as a penalty:\n",
        "Penalty\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "=\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "Penalty(β)=\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣β\n",
        "j\n",
        "​\n",
        " ∣\n",
        "Encourages sparsity by shrinking some coefficients to exactly 0, effectively performing feature selection.\n",
        "b. L2 Regularization (Ridge)\n",
        "Adds the sum of the squared values of the coefficients as a penalty:\n",
        "Penalty\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "=\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "Penalty(β)=\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "Shrinks the coefficients but does not set them to 0, leading to a smoother, more stable model.\n",
        "3. Why is Regularization Needed?\n",
        "a. Prevents Overfitting\n",
        "In the absence of regularization, Logistic Regression can fit the training data too closely, especially if the dataset has many features or is noisy.\n",
        "Overfitting results in poor performance on unseen (test) data because the model captures noise rather than underlying patterns.\n",
        "b. Handles High-Dimensional Data\n",
        "When there are many features relative to the number of training examples, the model can assign high weights to irrelevant features. Regularization controls this by constraining the coefficients.\n",
        "c. Improves Generalization\n",
        "By adding a penalty, the model is forced to focus on the most important features, improving its ability to generalize to new data.\n",
        "d. Feature Selection (L1 Regularization)\n",
        "L1 regularization helps in automatically selecting the most relevant features by reducing some coefficients to zero, simplifying the model.\n",
        "4. Cost Functions with Regularization\n",
        "For L2 Regularization (Ridge), the regularized cost function becomes:\n",
        "\n",
        "𝐽\n",
        "regularized\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑦\n",
        "^\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "^\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "]\n",
        "+\n",
        "𝜆\n",
        "2\n",
        "𝑚\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "J\n",
        "regularized\n",
        "​\n",
        " (β)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "\n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " log(\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "(i)\n",
        " )+(1−y\n",
        "(i)\n",
        " )log(1−\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "(i)\n",
        " )]+\n",
        "2m\n",
        "λ\n",
        "​\n",
        "\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "For L1 Regularization (Lasso), the regularized cost function becomes:\n",
        "\n",
        "𝐽\n",
        "regularized\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑦\n",
        "^\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "^\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "]\n",
        "+\n",
        "𝜆\n",
        "𝑚\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "J\n",
        "regularized\n",
        "​\n",
        " (β)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "\n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " log(\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "(i)\n",
        " )+(1−y\n",
        "(i)\n",
        " )log(1−\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "(i)\n",
        " )]+\n",
        "m\n",
        "λ\n",
        "​\n",
        "\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣β\n",
        "j\n",
        "​\n",
        " ∣\n",
        "5. Impact of Regularization Strength (\n",
        "𝜆\n",
        "λ):\n",
        "Small\n",
        "𝜆\n",
        "λ: Minimal penalty, allowing the model to fit the data more closely.\n",
        "Large\n",
        "𝜆\n",
        "λ: Strong penalty, forcing the coefficients to shrink more and reducing model complexity.\n",
        "Optimal\n",
        "𝜆\n",
        "λ: Chosen via techniques like cross-validation to balance underfitting and overfitting.\n",
        "6. When to Use Regularization?\n",
        "When there are many features (high dimensionality).\n",
        "When the training accuracy is high, but the test accuracy is low (overfitting).\n",
        "When the dataset has noisy or irrelevant features.\n",
        "\n",
        "Lasso, Ridge, and Elastic Net are regularization techniques used to improve the performance and generalizability of regression models by adding a penalty to the cost function. While all three aim to prevent overfitting, they differ in how they penalize model coefficients.\n",
        "\n",
        "1. Lasso Regression (L1 Regularization)\n",
        "Penalty: L1 regularization adds the absolute value of the coefficients to the cost function:\n",
        "Penalty\n",
        "=\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "Penalty=λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣β\n",
        "j\n",
        "​\n",
        " ∣\n",
        "Effect on Coefficients:\n",
        "Encourages sparsity by shrinking some coefficients to exactly 0, effectively performing feature selection.\n",
        "Produces simpler models by eliminating less important features.\n",
        "Cost Function:\n",
        "𝐽\n",
        "lasso\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "=\n",
        "MSE\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "J\n",
        "lasso\n",
        "​\n",
        " (β)=MSE+λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣β\n",
        "j\n",
        "​\n",
        " ∣\n",
        "When to Use:\n",
        "When you suspect many features are irrelevant or redundant.\n",
        "For automatic feature selection.\n",
        "Limitation:\n",
        "May struggle when features are highly correlated, as it arbitrarily selects one feature over others.\n",
        "2. Ridge Regression (L2 Regularization)\n",
        "Penalty: L2 regularization adds the squared value of the coefficients to the cost function:\n",
        "Penalty\n",
        "=\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "Penalty=λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "Effect on Coefficients:\n",
        "Shrinks all coefficients toward zero but does not reduce them to exactly zero.\n",
        "Retains all features but reduces the magnitude of less important ones.\n",
        "Cost Function:\n",
        "𝐽\n",
        "ridge\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "=\n",
        "MSE\n",
        "+\n",
        "𝜆\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "J\n",
        "ridge\n",
        "​\n",
        " (β)=MSE+λ\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "When to Use:\n",
        "When you suspect all features are relevant, but their importance varies.\n",
        "When features are highly correlated, as it tends to distribute the weights more evenly among correlated features.\n",
        "Limitation:\n",
        "Does not perform feature selection, so all features remain in the model.\n",
        "3. Elastic Net Regression\n",
        "Penalty: Elastic Net combines both L1 and L2 regularization:\n",
        "Penalty\n",
        "=\n",
        "𝜆\n",
        "1\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "+\n",
        "𝜆\n",
        "2\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "Penalty=λ\n",
        "1\n",
        "​\n",
        "\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣β\n",
        "j\n",
        "​\n",
        " ∣+λ\n",
        "2\n",
        "​\n",
        "\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "Where:\n",
        "𝜆\n",
        "1\n",
        "λ\n",
        "1\n",
        "​\n",
        "  controls the L1 penalty (lasso).\n",
        "𝜆\n",
        "2\n",
        "λ\n",
        "2\n",
        "​\n",
        "  controls the L2 penalty (ridge).\n",
        "Effect on Coefficients:\n",
        "Combines the strengths of both Lasso (feature selection) and Ridge (handles correlated features).\n",
        "Shrinks some coefficients to exactly 0 (like Lasso) while also shrinking others proportionally (like Ridge).\n",
        "Cost Function:\n",
        "𝐽\n",
        "elastic_net\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "=\n",
        "MSE\n",
        "+\n",
        "𝜆\n",
        "1\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "+\n",
        "𝜆\n",
        "2\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "J\n",
        "elastic_net\n",
        "​\n",
        " (β)=MSE+λ\n",
        "1\n",
        "​\n",
        "\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣β\n",
        "j\n",
        "​\n",
        " ∣+λ\n",
        "2\n",
        "​\n",
        "\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "When to Use:\n",
        "When you have a mix of irrelevant features and multicollinearity (correlated features).\n",
        "When Lasso alone struggles due to high correlation among features.\n",
        "Limitation:\n",
        "Requires tuning two hyperparameters (\n",
        "𝜆\n",
        "1\n",
        "λ\n",
        "1\n",
        "​\n",
        "  and\n",
        "𝜆\n",
        "2\n",
        "λ\n",
        "2\n",
        "​\n",
        " ), which can be computationally intensive.\n",
        "Key Differences\n",
        "Aspect\tLasso Regression\tRidge Regression\tElastic Net Regression\n",
        "Penalty Term\t( \\sum\t\\beta_j\t) (L1)\n",
        "Feature Selection\tYes (some coefficients = 0)\tNo (all coefficients shrink)\tYes (some coefficients = 0, others shrink)\n",
        "Effect on Correlated Features\tSelects one feature and ignores others\tDistributes weights among them\tBalances selection and distribution\n",
        "Sparsity\tHigh (very sparse)\tLow (no sparsity)\tMedium (depends on mix of L1 and L2)\n",
        "4. When to Use Each\n",
        "Lasso: When feature selection is important, or you suspect only a few features are truly important.\n",
        "Ridge: When all features are relevant but need regularization to reduce multicollinearity or prevent overfitting.\n",
        "Elastic Net: When there are both irrelevant features and correlated features, making it a more balanced choice.\n",
        "\n",
        "We should use Elastic Net instead of Lasso or Ridge when the dataset has a mix of irrelevant features (that need to be removed) and highly correlated features (where weight distribution is important). Elastic Net combines the strengths of both Lasso (feature selection) and Ridge (handling multicollinearity) and is particularly effective in the following scenarios:\n",
        "\n",
        "1. When Features are Highly Correlated\n",
        "Why Lasso Fails:\n",
        "Lasso tends to select only one feature from a group of highly correlated features and ignores the rest, which may lead to unstable results when the choice of the selected feature is arbitrary.\n",
        "Why Elastic Net Works:\n",
        "Elastic Net distributes the weights across correlated features, balancing the impact of feature selection (L1) with regularization (L2).\n",
        "2. When There are Many Irrelevant Features\n",
        "Why Ridge Fails:\n",
        "Ridge shrinks all coefficients but doesn’t eliminate any irrelevant features (coefficients are never exactly 0).\n",
        "Why Elastic Net Works:\n",
        "The L1 component of Elastic Net allows irrelevant features to have their coefficients reduced to exactly 0, effectively removing them from the model.\n",
        "3. When Lasso Alone is Too Aggressive\n",
        "Why Lasso Fails:\n",
        "In some cases, Lasso may be too aggressive in shrinking coefficients to zero, which could result in the loss of important features, especially if they are weakly correlated with the target variable.\n",
        "Why Elastic Net Works:\n",
        "Elastic Net balances feature selection with shrinkage, ensuring weak but relevant features are not completely ignored.\n",
        "4. When the Number of Features (p) > Number of Observations (n)\n",
        "Why Lasso Fails:\n",
        "In high-dimensional datasets (where\n",
        "𝑝\n",
        ">\n",
        "𝑛\n",
        "p>n), Lasso may select at most\n",
        "𝑛\n",
        "n features due to its nature, which might not be sufficient if more features are relevant.\n",
        "Why Elastic Net Works:\n",
        "Elastic Net can select more than\n",
        "𝑛\n",
        "n features by combining the benefits of both L1 and L2 penalties.\n",
        "5. When You Want a More Balanced Model\n",
        "Elastic Net provides flexibility by allowing you to adjust the balance between L1 (Lasso) and L2 (Ridge) regularization via the mixing parameter\n",
        "𝛼\n",
        "α:\n",
        "Penalty\n",
        "=\n",
        "𝛼\n",
        "⋅\n",
        "∑\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝛼\n",
        ")\n",
        "⋅\n",
        "∑\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "Penalty=α⋅∑∣β\n",
        "j\n",
        "​\n",
        " ∣+(1−α)⋅∑β\n",
        "j\n",
        "2\n",
        "​\n",
        "\n",
        "If\n",
        "𝛼\n",
        "=\n",
        "1\n",
        "α=1, Elastic Net becomes Lasso.\n",
        "If\n",
        "𝛼\n",
        "=\n",
        "0\n",
        "α=0, Elastic Net becomes Ridge.\n",
        "Choosing an intermediate\n",
        "𝛼\n",
        "α gives a balance between feature selection and coefficient shrinkage.\n",
        "6. When Model Interpretability and Robustness are Important\n",
        "Elastic Net is more robust and interpretable than Lasso or Ridge alone, especially in complex datasets with multicollinearity and many irrelevant features.\n",
        "Practical Scenarios for Elastic Net\n",
        "Genomics Data: High-dimensional datasets with many irrelevant and correlated features.\n",
        "Financial Data: Where multiple economic indicators are highly correlated, but irrelevant features also exist.\n",
        "Marketing Data: Where customer behavior data often has both irrelevant features and strong correlations among predictors.\n",
        "When Not to Use Elastic Net\n",
        "If there is no multicollinearity and the number of features is small, Ridge or Lasso alone might be sufficient.\n",
        "If feature selection is the primary goal (e.g., sparse models), Lasso may be more appropriate.\n",
        "If you believe all features are relevant but need regularization, Ridge is simpler and faster to use.\n",
        "\n",
        "\n",
        "The regularization parameter (\n",
        "𝜆\n",
        "λ) in Logistic Regression controls the strength of the penalty applied to the model's coefficients. Its value has a significant impact on the behavior and performance of the model, as it determines the trade-off between fitting the training data and preventing overfitting (or underfitting).\n",
        "\n",
        "1. Role of\n",
        "𝜆\n",
        "λ in Regularization\n",
        "Regularization Term: The regularization term in Logistic Regression cost function is multiplied by\n",
        "𝜆\n",
        "λ:\n",
        "𝐽\n",
        "regularized\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑦\n",
        "^\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "^\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "]\n",
        "+\n",
        "𝜆\n",
        "2\n",
        "𝑚\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "𝛽\n",
        "𝑗\n",
        "2\n",
        "(\n",
        "L2\n",
        ")\n",
        "J\n",
        "regularized\n",
        "​\n",
        " (β)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "\n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " log(\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "(i)\n",
        " )+(1−y\n",
        "(i)\n",
        " )log(1−\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "(i)\n",
        " )]+\n",
        "2m\n",
        "λ\n",
        "​\n",
        "\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " β\n",
        "j\n",
        "2\n",
        "​\n",
        " (L2)\n",
        "or:\n",
        "𝐽\n",
        "regularized\n",
        "(\n",
        "𝛽\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑚\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑚\n",
        "[\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑦\n",
        "^\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "^\n",
        "(\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "]\n",
        "+\n",
        "𝜆\n",
        "𝑚\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝑛\n",
        "∣\n",
        "𝛽\n",
        "𝑗\n",
        "∣\n",
        "(\n",
        "L1\n",
        ")\n",
        "J\n",
        "regularized\n",
        "​\n",
        " (β)=−\n",
        "m\n",
        "1\n",
        "​\n",
        "\n",
        "i=1\n",
        "∑\n",
        "m\n",
        "​\n",
        " [y\n",
        "(i)\n",
        " log(\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "(i)\n",
        " )+(1−y\n",
        "(i)\n",
        " )log(1−\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "(i)\n",
        " )]+\n",
        "m\n",
        "λ\n",
        "​\n",
        "\n",
        "j=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " ∣β\n",
        "j\n",
        "​\n",
        " ∣(L1)\n",
        "Here:\n",
        "Large\n",
        "𝜆\n",
        "λ: Strong penalty on large coefficients.\n",
        "Small\n",
        "𝜆\n",
        "λ: Weak penalty, allowing the model to fit more closely to the training data.\n",
        "2. Impact of\n",
        "𝜆\n",
        "λ\n",
        "a. Small\n",
        "𝜆\n",
        "λ (Weak Regularization)\n",
        "Effect:\n",
        "Regularization term has minimal influence.\n",
        "The model fits the training data more closely, capturing more of the data's variance.\n",
        "Coefficients may grow large, leading to potential overfitting.\n",
        "Result:\n",
        "High accuracy on training data but poor generalization to test data.\n",
        "b. Large\n",
        "𝜆\n",
        "λ (Strong Regularization)\n",
        "Effect:\n",
        "Strong penalty on the coefficients, forcing them to shrink closer to zero.\n",
        "Simplifies the model by reducing the influence of less relevant features.\n",
        "May result in underfitting, where the model cannot capture the true patterns in the data.\n",
        "Result:\n",
        "Better generalization to test data (up to a point), but poor performance if\n",
        "𝜆\n",
        "λ is too large.\n",
        "c. Optimal\n",
        "𝜆\n",
        "λ\n",
        "Effect:\n",
        "Balances bias and variance.\n",
        "Prevents both overfitting and underfitting, resulting in good generalization performance.\n",
        "How to Find It:\n",
        "Use techniques like cross-validation to tune\n",
        "𝜆\n",
        "λ for the best performance on validation data.\n",
        "3. Intuitive Impact of\n",
        "𝜆\n",
        "λ on Coefficients\n",
        "For L1 Regularization (Lasso):\n",
        "A large\n",
        "𝜆\n",
        "λ drives some coefficients to exactly zero, effectively removing irrelevant features.\n",
        "A small\n",
        "𝜆\n",
        "λ allows all features to contribute to the model.\n",
        "For L2 Regularization (Ridge):\n",
        "A large\n",
        "𝜆\n",
        "λ reduces the magnitude of all coefficients but does not set any to zero.\n",
        "A small\n",
        "𝜆\n",
        "λ allows the coefficients to grow larger.\n",
        "4. Bias-Variance Tradeoff\n",
        "The value of\n",
        "𝜆\n",
        "λ directly impacts the bias-variance tradeoff:\n",
        "\n",
        "Small\n",
        "𝜆\n",
        "λ:\n",
        "Low Bias, High Variance: Model fits training data too closely, including noise.\n",
        "Large\n",
        "𝜆\n",
        "λ:\n",
        "High Bias, Low Variance: Model becomes too simplistic and may miss important patterns.\n",
        "Optimal\n",
        "𝜆\n",
        "λ:\n",
        "Achieves the best tradeoff, minimizing test error.\n",
        "5. Practical Considerations\n",
        "Default Values: Many libraries like scikit-learn use a default regularization parameter, which is often a reasonable starting point.\n",
        "Hyperparameter Tuning: Use methods like grid search, random search, or Bayesian optimization to find the best\n",
        "𝜆\n",
        "λ.\n",
        "Scale of Features: Always normalize or standardize features before applying regularization since\n",
        "𝜆\n",
        "λ affects the magnitude of coefficients.\n",
        "\n",
        "9. What are the key assumptions of Logistic Regression.\n",
        "Logistic Regression, while a robust and commonly used method, relies on several key assumptions to perform effectively. Understanding these assumptions is important for ensuring the model's validity and interpretability.\n",
        "\n",
        "1. The Relationship Between Features and the Log-Odds is Linear\n",
        "Logistic Regression assumes that the log-odds (logarithm of the odds of the dependent variable being 1) is a linear combination of the input features:\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑃\n",
        "1\n",
        "−\n",
        "𝑃\n",
        ")\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "log(\n",
        "1−P\n",
        "P\n",
        "​\n",
        " )=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        "\n",
        "What This Means:\n",
        "The relationship between the predictors (\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        " ) and the log-odds of the outcome is linear, even though the relationship between predictors and the probability is non-linear (due to the sigmoid function).\n",
        "Violation Impact:\n",
        "If this assumption does not hold, the model may underperform. Transformation of variables or feature engineering might be needed.\n",
        "2. The Dependent Variable is Binary\n",
        "Logistic Regression is designed for binary classification problems, where the dependent variable has only two possible outcomes (e.g., 0 or 1, True or False, Yes or No).\n",
        "What This Means:\n",
        "For multi-class problems, Logistic Regression can be extended using multinomial logistic regression or one-vs-rest approaches.\n",
        "Violation Impact:\n",
        "If the dependent variable is not binary (and multinomial logistic regression is not applied), the model will fail to produce meaningful results.\n",
        "3. Independence of Observations\n",
        "The observations in the dataset should be independent of each other.\n",
        "What This Means:\n",
        "No individual observation should influence or depend on another (e.g., repeated measures or time-series data).\n",
        "Violation Impact:\n",
        "Correlated observations (e.g., clustering or hierarchical data) can lead to underestimated standard errors and incorrect inferences. Techniques like mixed-effects models may be needed in such cases.\n",
        "4. No Multicollinearity Among Predictors\n",
        "Logistic Regression assumes that the predictors are not highly correlated with one another.\n",
        "What This Means:\n",
        "High multicollinearity (strong correlation between features) can make it difficult to estimate coefficients reliably.\n",
        "Violation Impact:\n",
        "High multicollinearity can lead to unstable estimates and inflate the variance of the coefficient estimates. Techniques like removing correlated features, PCA, or Ridge/Elastic Net regularization can help.\n",
        "5. The Dataset is Sufficiently Large\n",
        "Logistic Regression assumes that the dataset has enough observations to provide reliable estimates of the coefficients.\n",
        "What This Means:\n",
        "A rule of thumb is to have at least 10 events per predictor variable (e.g., at least 10 instances of each class for every feature in the model).\n",
        "Violation Impact:\n",
        "A small dataset or imbalanced classes can lead to overfitting and unreliable estimates. Data augmentation, resampling (e.g., SMOTE), or regularization may be needed.\n",
        "6. Linearity in the Logit for Continuous Variables\n",
        "Continuous predictors should have a linear relationship with the log-odds of the dependent variable.\n",
        "What This Means:\n",
        "If the relationship is non-linear, the model may perform poorly. In such cases, you might transform the variables (e.g., using polynomials, logarithms, or splines).\n",
        "Violation Impact:\n",
        "Misrepresentation of the feature relationship may result in biased predictions.\n",
        "7. Independence of Errors\n",
        "Logistic Regression assumes that the errors (residuals) are independent and uncorrelated.\n",
        "What This Means:\n",
        "The outcome of one observation should not influence the errors of another.\n",
        "Violation Impact:\n",
        "Correlated errors can bias coefficient estimates and confidence intervals. Adjustments like adding random effects (e.g., in mixed models) may be required.\n",
        "8. The Dependent Variable is Properly Coded\n",
        "The dependent variable should be coded as 0 and 1 (binary coding) for correct interpretation of probabilities.\n",
        "What This Means:\n",
        "The model interprets\n",
        "𝑦\n",
        "=\n",
        "0\n",
        "y=0 as the negative class and\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1 as the positive class.\n",
        "Violation Impact:\n",
        "Improper coding can lead to incorrect predictions or errors during model fitting.\n",
        "9. Outliers and Influential Points\n",
        "Logistic Regression can be sensitive to outliers and influential data points in the feature space.\n",
        "What This Means:\n",
        "Outliers can disproportionately affect coefficient estimates and reduce model performance.\n",
        "Violation Impact:\n",
        "Regularization, robust methods, or data preprocessing steps (e.g., removing or transforming outliers) can mitigate this issue.\n",
        "10. Balanced Classes\n",
        "Logistic Regression performs better when the classes in the dependent variable are balanced (roughly equal number of 0s and 1s).\n",
        "What This Means:\n",
        "If one class dominates, the model might predict only the dominant class.\n",
        "Violation Impact:\n",
        "Class imbalance can be addressed using techniques like oversampling, undersampling, or class-weight adjustments.\n",
        "\n",
        "1. Decision Trees\n",
        "How It Works:\n",
        "Decision Trees split the data into subsets based on feature values, creating a tree-like structure to make decisions.\n",
        "Advantages:\n",
        "Non-linear relationships.\n",
        "Handles categorical and continuous data.\n",
        "Easy to interpret.\n",
        "Disadvantages:\n",
        "Prone to overfitting (mitigated by pruning or ensemble methods like Random Forest).\n",
        "2. Random Forest\n",
        "How It Works:\n",
        "An ensemble method combining multiple decision trees using bagging (Bootstrap Aggregation).\n",
        "Advantages:\n",
        "Reduces overfitting compared to a single decision tree.\n",
        "Handles high-dimensional datasets well.\n",
        "Robust to missing data and noisy features.\n",
        "Disadvantages:\n",
        "Less interpretable than Logistic Regression or a single Decision Tree.\n",
        "3. Gradient Boosting Machines (GBMs)\n",
        "How It Works:\n",
        "Sequentially builds models to correct the errors of previous models, optimizing for accuracy.\n",
        "Includes implementations like XGBoost, LightGBM, and CatBoost.\n",
        "Advantages:\n",
        "High predictive accuracy.\n",
        "Effective for imbalanced datasets and large feature spaces.\n",
        "Disadvantages:\n",
        "Computationally expensive and sensitive to hyperparameters.\n",
        "4. Support Vector Machines (SVM)\n",
        "How It Works:\n",
        "Finds a hyperplane that maximally separates classes in a high-dimensional feature space.\n",
        "Advantages:\n",
        "Works well for non-linear boundaries with kernels (e.g., RBF kernel).\n",
        "Effective for high-dimensional spaces.\n",
        "Disadvantages:\n",
        "Requires careful tuning of kernel and regularization parameters.\n",
        "Computationally expensive for large datasets.\n",
        "5. k-Nearest Neighbors (k-NN)\n",
        "How It Works:\n",
        "Classifies based on the majority class of the\n",
        "𝑘\n",
        "k nearest data points in feature space.\n",
        "Advantages:\n",
        "Simple and intuitive.\n",
        "No assumptions about data distribution.\n",
        "Disadvantages:\n",
        "Sensitive to noise and scaling.\n",
        "Computationally expensive for large datasets.\n",
        "6. Naive Bayes\n",
        "How It Works:\n",
        "Based on Bayes' Theorem, assumes features are conditionally independent given the target class.\n",
        "Advantages:\n",
        "Computationally efficient.\n",
        "Works well with small datasets and categorical features.\n",
        "Disadvantages:\n",
        "Assumption of feature independence is often unrealistic.\n",
        "7. Neural Networks\n",
        "How It Works:\n",
        "Consists of layers of interconnected nodes that learn hierarchical representations of features.\n",
        "Advantages:\n",
        "Handles complex, non-linear relationships.\n",
        "Scalable for large datasets.\n",
        "Disadvantages:\n",
        "Requires large datasets to perform well.\n",
        "Computationally intensive and harder to interpret.\n",
        "8. Linear Discriminant Analysis (LDA)\n",
        "How It Works:\n",
        "Projects data onto a lower-dimensional space to maximize the separation between classes.\n",
        "Advantages:\n",
        "Works well for linearly separable data.\n",
        "Robust to small sample sizes.\n",
        "Disadvantages:\n",
        "Assumes normal distribution of features and equal class covariance.\n",
        "9. Quadratic Discriminant Analysis (QDA)\n",
        "How It Works:\n",
        "Similar to LDA but allows for different covariance matrices for each class.\n",
        "Advantages:\n",
        "Works well when class boundaries are quadratic.\n",
        "Disadvantages:\n",
        "Requires more data to estimate the covariance matrices.\n",
        "10. Ensemble Methods\n",
        "Combine multiple models to improve prediction accuracy and robustness:\n",
        "Bagging (e.g., Random Forest): Reduces variance by averaging predictions.\n",
        "Boosting (e.g., XGBoost, AdaBoost): Focuses on reducing bias and optimizing for hard-to-classify data.\n",
        "Stacking: Combines predictions from different models using a meta-model.\n",
        "11. Probabilistic Models\n",
        "Bayesian Logistic Regression:\n",
        "A probabilistic extension of Logistic Regression that incorporates prior distributions over parameters.\n",
        "Gaussian Processes:\n",
        "Non-parametric methods providing uncertainty estimates along with predictions.\n",
        "12. Rule-Based Algorithms\n",
        "E.g., RuleFit or RIPPER:\n",
        "Builds interpretable models based on rules extracted from data.\n",
        "When to Choose an Alternative\n",
        "Scenario\tRecommended Alternative\n",
        "Non-linear relationships\tSVM, Neural Networks, Decision Trees, Random Forest, GBMs.\n",
        "High-dimensional data\tSVM, Random Forest, Gradient Boosting, Neural Networks.\n",
        "Imbalanced datasets\tGradient Boosting (e.g., XGBoost), Random Forest, SVM with class weights.\n",
        "Small datasets\tNaive Bayes, LDA, k-NN.\n",
        "Need for interpretability\tDecision Trees, LDA, Rule-Based Models.\n",
        "High multicollinearity in features\tRidge Regression, Elastic Net, Random Forest.\n",
        "Large datasets\tGradient Boosting, Neural Networks, Random Forest.\n",
        "\n",
        "Classification evaluation metrics help us measure the performance of a classification model. These metrics are essential to understand how well the model is predicting and to identify areas of improvement. The choice of metrics depends on the problem type (binary, multiclass, or imbalanced datasets).\n",
        "\n",
        "Key Classification Evaluation Metrics\n",
        "1. Accuracy\n",
        "Definition: The ratio of correctly predicted instances to the total number of instances.\n",
        "Accuracy\n",
        "=\n",
        "TP\n",
        "+\n",
        "TN\n",
        "TP\n",
        "+\n",
        "TN\n",
        "+\n",
        "FP\n",
        "+\n",
        "FN\n",
        "Accuracy=\n",
        "TP+TN+FP+FN\n",
        "TP+TN\n",
        "​\n",
        "\n",
        "When to Use:\n",
        "Useful when classes are balanced.\n",
        "Limitations:\n",
        "Can be misleading for imbalanced datasets (e.g., if one class dominates).\n",
        "2. Precision (Positive Predictive Value)\n",
        "Definition: The proportion of correctly predicted positive instances out of all predicted positive instances.\n",
        "Precision\n",
        "=\n",
        "TP\n",
        "TP\n",
        "+\n",
        "FP\n",
        "Precision=\n",
        "TP+FP\n",
        "TP\n",
        "​\n",
        "\n",
        "When to Use:\n",
        "Important in cases where false positives are costly (e.g., spam detection).\n",
        "3. Recall (Sensitivity/True Positive Rate)\n",
        "Definition: The proportion of correctly predicted positive instances out of all actual positive instances.\n",
        "Recall\n",
        "=\n",
        "TP\n",
        "TP\n",
        "+\n",
        "FN\n",
        "Recall=\n",
        "TP+FN\n",
        "TP\n",
        "​\n",
        "\n",
        "When to Use:\n",
        "Important in cases where false negatives are costly (e.g., disease detection).\n",
        "4. F1-Score\n",
        "Definition: The harmonic mean of precision and recall.\n",
        "F1-Score\n",
        "=\n",
        "2\n",
        "×\n",
        "Precision\n",
        "×\n",
        "Recall\n",
        "Precision\n",
        "+\n",
        "Recall\n",
        "F1-Score=2×\n",
        "Precision+Recall\n",
        "Precision×Recall\n",
        "​\n",
        "\n",
        "When to Use:\n",
        "Useful for imbalanced datasets.\n",
        "Balances precision and recall when one metric alone is insufficient.\n",
        "5. Specificity (True Negative Rate)\n",
        "Definition: The proportion of correctly predicted negative instances out of all actual negative instances.\n",
        "Specificity\n",
        "=\n",
        "TN\n",
        "TN\n",
        "+\n",
        "FP\n",
        "Specificity=\n",
        "TN+FP\n",
        "TN\n",
        "​\n",
        "\n",
        "When to Use:\n",
        "Important when identifying negatives accurately is crucial.\n",
        "6. ROC-AUC (Receiver Operating Characteristic - Area Under Curve)\n",
        "Definition: Measures the ability of the model to distinguish between classes. It plots the True Positive Rate (Recall) against the False Positive Rate (1 - Specificity).\n",
        "AUC: The area under the ROC curve.\n",
        "Higher AUC values indicate better model performance.\n",
        "When to Use:\n",
        "Good for binary classification and assessing ranking ability.\n",
        "7. Precision-Recall (PR) Curve and AUC\n",
        "Definition: Plots precision against recall at various thresholds.\n",
        "PR-AUC: Area under the Precision-Recall curve.\n",
        "When to Use:\n",
        "Useful for imbalanced datasets.\n",
        "8. Log Loss (Logarithmic Loss)\n",
        "Definition: Measures the uncertainty of predicted probabilities for the true class.\n",
        "Log Loss\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "[\n",
        "𝑦\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "]\n",
        "Log Loss=−\n",
        "N\n",
        "1\n",
        "​\n",
        "\n",
        "i=1\n",
        "∑\n",
        "N\n",
        "​\n",
        " [y\n",
        "i\n",
        "​\n",
        " log(\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "i\n",
        "​\n",
        " )+(1−y\n",
        "i\n",
        "​\n",
        " )log(1−\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "i\n",
        "​\n",
        " )]\n",
        "When to Use:\n",
        "Evaluates probabilistic models.\n",
        "9. Matthews Correlation Coefficient (MCC)\n",
        "Definition: A balanced measure that accounts for true/false positives and negatives.\n",
        "MCC\n",
        "=\n",
        "TP\n",
        "⋅\n",
        "TN\n",
        "−\n",
        "FP\n",
        "⋅\n",
        "FN\n",
        "(\n",
        "TP\n",
        "+\n",
        "FP\n",
        ")\n",
        "(\n",
        "TP\n",
        "+\n",
        "FN\n",
        ")\n",
        "(\n",
        "TN\n",
        "+\n",
        "FP\n",
        ")\n",
        "(\n",
        "TN\n",
        "+\n",
        "FN\n",
        ")\n",
        "MCC=\n",
        "(TP+FP)(TP+FN)(TN+FP)(TN+FN)\n",
        "​\n",
        "\n",
        "TP⋅TN−FP⋅FN\n",
        "​\n",
        "\n",
        "When to Use:\n",
        "Especially useful for imbalanced datasets.\n",
        "10. Cohen's Kappa\n",
        "Definition: Measures the agreement between actual and predicted classes, considering chance.\n",
        "Kappa\n",
        "=\n",
        "Observed Accuracy\n",
        "−\n",
        "Expected Accuracy\n",
        "1\n",
        "−\n",
        "Expected Accuracy\n",
        "Kappa=\n",
        "1−Expected Accuracy\n",
        "Observed Accuracy−Expected Accuracy\n",
        "​\n",
        "\n",
        "When to Use:\n",
        "Evaluates model performance beyond random guessing.\n",
        "11. Balanced Accuracy\n",
        "Definition: The average of recall for each class.\n",
        "Balanced Accuracy\n",
        "=\n",
        "Sensitivity\n",
        "+\n",
        "Specificity\n",
        "2\n",
        "Balanced Accuracy=\n",
        "2\n",
        "Sensitivity+Specificity\n",
        "​\n",
        "\n",
        "When to Use:\n",
        "Useful for imbalanced datasets.\n",
        "12. Confusion Matrix\n",
        "Definition: A matrix that provides detailed insights into the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
        "Structure:\n",
        "[\n",
        "TN\n",
        "FP\n",
        "FN\n",
        "TP\n",
        "]\n",
        "[\n",
        "TN\n",
        "FN\n",
        "​\n",
        "\n",
        "FP\n",
        "TP\n",
        "​\n",
        " ]\n",
        "When to Use:\n",
        "To gain detailed insights into model errors and predictions.\n",
        "Summary Table of Metrics\n",
        "Metric\tBest Use Case\tKey Strength\n",
        "Accuracy\tBalanced datasets\tSimple to understand.\n",
        "Precision\tFalse positives are costly\tFocuses on positive predictions.\n",
        "Recall\tFalse negatives are costly\tCaptures all positives.\n",
        "F1-Score\tImbalanced datasets\tBalances precision and recall.\n",
        "Specificity\tIdentifying true negatives\tFocuses on negatives.\n",
        "ROC-AUC\tBinary classification, ranking tasks\tMeasures model’s ability to rank.\n",
        "PR-AUC\tImbalanced datasets\tFocuses on precision and recall.\n",
        "Log Loss\tProbabilistic models\tPenalizes incorrect probabilities.\n",
        "MCC\tImbalanced datasets\tBalanced evaluation of all outcomes.\n",
        "Cohen's Kappa\tAgreement assessment beyond chance\tAdjusts for random guessing.\n",
        "Balanced Accuracy\tImbalanced datasets\tBalances sensitivity and specificity.\n",
        "Confusion Matrix\tDetailed performance analysis\tOffers granular insights.\n",
        "\n",
        "Class imbalance can significantly affect the performance of Logistic Regression, as well as the interpretability and reliability of its predictions. Here's an explanation of how and why class imbalance causes issues, along with some solutions:\n",
        "\n",
        "Impact of Class Imbalance on Logistic Regression\n",
        "Biased Decision Boundary\n",
        "\n",
        "Logistic Regression seeks to maximize the overall likelihood or minimize the cost function. In class-imbalanced datasets, the model can achieve high accuracy by prioritizing the majority class, often ignoring the minority class.\n",
        "This results in a biased decision boundary, which favors the majority class, leading to poor predictions for the minority class.\n",
        "Poor Recall for the Minority Class\n",
        "\n",
        "The minority class may be treated as noise, leading to very low recall (i.e., many false negatives).\n",
        "While precision for the majority class might be high, recall for the minority class suffers significantly.\n",
        "Misleading Accuracy\n",
        "\n",
        "With a highly imbalanced dataset, accuracy becomes an unreliable metric. For example:\n",
        "If 95% of instances belong to one class, a model predicting only the majority class will achieve 95% accuracy, despite failing to identify any instances of the minority class.\n",
        "Underestimation of Minority Class Probabilities\n",
        "\n",
        "Logistic Regression outputs probabilities for each class. Due to class imbalance, the predicted probabilities for the minority class may be systematically underestimated, making it harder to classify them correctly.\n",
        "Inconsistent Calibration\n",
        "\n",
        "The predicted probabilities might not reflect the true likelihood of belonging to the minority class. This reduces the model's reliability in real-world applications where calibrated probabilities are essential.\n",
        "How to Handle Class Imbalance in Logistic Regression\n",
        "1. Re-sampling Techniques\n",
        "Oversampling the Minority Class:\n",
        "Duplicate or synthetically generate samples from the minority class (e.g., using SMOTE – Synthetic Minority Oversampling Technique).\n",
        "Undersampling the Majority Class:\n",
        "Randomly remove samples from the majority class to balance the dataset.\n",
        "Hybrid Approach:\n",
        "Combine oversampling and undersampling to create a balanced dataset.\n",
        "2. Class Weights\n",
        "Assign higher weights to the minority class and lower weights to the majority class in the Logistic Regression cost function.\n",
        "In scikit-learn, this can be achieved by setting class_weight=\"balanced\" or providing custom weights.\n",
        "This approach ensures that the model penalizes misclassifications of the minority class more heavily.\n",
        "3. Threshold Tuning\n",
        "Logistic Regression typically uses a default threshold of 0.5 for classification. For imbalanced datasets:\n",
        "Adjust the decision threshold to favor the minority class by lowering it (e.g., 0.3 or 0.2).\n",
        "Use metrics like the ROC curve or Precision-Recall curve to determine the optimal threshold.\n",
        "4. Evaluation with Appropriate Metrics\n",
        "Use metrics that focus on class-specific performance, such as:\n",
        "Precision, Recall, and F1-Score for the minority class.\n",
        "ROC-AUC or PR-AUC (better for imbalanced datasets).\n",
        "Avoid relying solely on accuracy.\n",
        "5. Ensemble Techniques\n",
        "Use ensemble models (e.g., Random Forest, Gradient Boosting) with class weights or sampling techniques to handle imbalance more effectively.\n",
        "These methods are often more robust to imbalance than Logistic Regression.\n",
        "6. Anomaly Detection Perspective\n",
        "In cases of extreme imbalance, treat the minority class as an anomaly and use anomaly detection techniques instead of traditional classification.\n",
        "Practical Example of Class Imbalance\n",
        "Suppose a dataset contains 98% instances of Class 0 and 2% of Class 1. A Logistic Regression model trained without addressing the imbalance might classify almost all data points as Class 0. This results in:\n",
        "\n",
        "High accuracy (~98%), but:\n",
        "Precision for Class 1: Low.\n",
        "Recall for Class 1: Near zero.\n",
        "By addressing imbalance (e.g., using re-sampling or class weights), the model can achieve better recall and F1-score for Class 1, even if the overall accuracy decreases.\n",
        "\n",
        "13. What is Hyperparameter Tuning in Logistic Regression.\n",
        "Hyperparameter Tuning in Logistic Regression\n",
        "Hyperparameter tuning refers to the process of optimizing the hyperparameters of a machine learning model to achieve the best possible performance. For Logistic Regression, hyperparameters are settings that are not learned from the data but need to be configured before training.\n",
        "\n",
        "Common Hyperparameters in Logistic Regression\n",
        "Here are the key hyperparameters in Logistic Regression that can be tuned:\n",
        "\n",
        "1. Regularization Parameter (\n",
        "𝜆\n",
        "λ or\n",
        "𝐶\n",
        "C)\n",
        "What It Controls:\n",
        "The strength of the regularization applied to the model.\n",
        "In scikit-learn,\n",
        "𝐶\n",
        "C is used instead of\n",
        "𝜆\n",
        "λ, where:\n",
        "𝐶\n",
        "=\n",
        "1\n",
        "𝜆\n",
        "C=\n",
        "λ\n",
        "1\n",
        "​\n",
        "\n",
        "A smaller\n",
        "𝐶\n",
        "C (or larger\n",
        "𝜆\n",
        "λ) applies stronger regularization.\n",
        "A larger\n",
        "𝐶\n",
        "C (or smaller\n",
        "𝜆\n",
        "λ) applies weaker regularization.\n",
        "Effect:\n",
        "Prevents overfitting by penalizing large coefficients in the Logistic Regression model.\n",
        "Striking a balance between underfitting and overfitting is crucial.\n",
        "2. Regularization Type (Penalty)\n",
        "What It Controls:\n",
        "The type of regularization applied to the model:\n",
        "L1 Regularization (Lasso): Shrinks some coefficients to exactly zero, performing feature selection.\n",
        "L2 Regularization (Ridge): Shrinks coefficients but does not set them to zero.\n",
        "Elastic Net: Combines both L1 and L2 regularization.\n",
        "Effect:\n",
        "The choice of penalty affects how the model handles multicollinearity and feature selection.\n",
        "3. Solver\n",
        "What It Controls:\n",
        "The optimization algorithm used to minimize the cost function.\n",
        "Common solvers in scikit-learn:\n",
        "lbfgs: Robust and works well for most problems (default for small datasets).\n",
        "saga: Suitable for large datasets and supports both L1 and L2 regularization.\n",
        "liblinear: Best for small datasets, supports L1 and L2.\n",
        "newton-cg: Suitable for large datasets but slower.\n",
        "Effect:\n",
        "Affects the convergence speed and ability to handle large datasets or specific regularization methods.\n",
        "4. Maximum Number of Iterations (max_iter)\n",
        "What It Controls:\n",
        "The maximum number of iterations allowed for the optimization solver.\n",
        "Effect:\n",
        "If the model fails to converge, increasing max_iter can help the solver find the optimal solution.\n",
        "5. Class Weight\n",
        "What It Controls:\n",
        "The weights assigned to classes in the cost function.\n",
        "Options:\n",
        "balanced: Automatically adjusts weights inversely proportional to class frequencies.\n",
        "Custom weights for each class (e.g., {0: 1, 1: 5}).\n",
        "Effect:\n",
        "Addresses class imbalance by giving more importance to the minority class.\n",
        "6. Fit Intercept\n",
        "What It Controls:\n",
        "Whether to include an intercept (\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        " ) in the model.\n",
        "Effect:\n",
        "Helps adjust the decision boundary based on whether data is centered or not.\n",
        "7. Tolerance (tol)\n",
        "What It Controls:\n",
        "The tolerance for stopping criteria during optimization.\n",
        "Effect:\n",
        "A smaller tol value leads to more precise convergence but may increase computation time.\n",
        "Why Hyperparameter Tuning is Important\n",
        "Improves Model Performance:\n",
        "Helps achieve better accuracy, precision, recall, F1-score, or other evaluation metrics.\n",
        "Reduces Overfitting/Underfitting:\n",
        "Ensures the model generalizes well to unseen data.\n",
        "Balances Computational Cost:\n",
        "Finds the best trade-off between computation time and performance.\n",
        "\n",
        "Logistic Regression solvers are optimization algorithms used to minimize the cost function (log-loss) during training. The choice of solver affects the speed, convergence, and ability to handle specific scenarios like large datasets or particular types of regularization.\n",
        "\n",
        "Different Solvers in Logistic Regression\n",
        "liblinear\n",
        "\n",
        "Based on Coordinate Descent.\n",
        "Suitable for small to medium-sized datasets.\n",
        "Supports both L1 (Lasso) and L2 (Ridge) regularization.\n",
        "Works well for sparse datasets.\n",
        "Does not support multiclass classification with multi_class='multinomial' (only supports one-vs-rest).\n",
        "Use When:\n",
        "\n",
        "Dataset is small or sparse.\n",
        "L1 regularization (Lasso) is required.\n",
        "lbfgs (Limited-memory Broyden–Fletcher–Goldfarb–Shanno)\n",
        "\n",
        "An optimization algorithm from the Quasi-Newton family.\n",
        "Suitable for large datasets.\n",
        "Supports only L2 regularization.\n",
        "Efficient and robust for most cases.\n",
        "Supports multiclass classification with multi_class='multinomial'.\n",
        "Use When:\n",
        "\n",
        "Dataset is large or dense.\n",
        "Multiclass classification is required.\n",
        "saga\n",
        "\n",
        "A variant of Stochastic Gradient Descent (SGD).\n",
        "Suitable for very large datasets (especially sparse datasets).\n",
        "Supports both L1 and L2 regularization and Elastic Net regularization.\n",
        "Handles multiclass classification with multi_class='multinomial'.\n",
        "Use When:\n",
        "\n",
        "Dataset is very large or sparse.\n",
        "Elastic Net regularization is required.\n",
        "newton-cg (Newton-Conjugate Gradient)\n",
        "\n",
        "A second-order optimization algorithm using the Newton-Raphson method.\n",
        "Supports only L2 regularization.\n",
        "More computationally expensive than lbfgs but can handle large datasets.\n",
        "Suitable for multiclass classification with multi_class='multinomial'.\n",
        "Use When:\n",
        "\n",
        "Dataset is large, and convergence needs to be precise.\n",
        "sag (Stochastic Average Gradient Descent)\n",
        "\n",
        "A faster version of Gradient Descent.\n",
        "Suitable for large datasets (especially dense datasets).\n",
        "Supports only L2 regularization.\n",
        "Requires normalized features for good performance.\n",
        "Use When:\n",
        "\n",
        "Dataset is large and dense.\n",
        "Only L2 regularization is needed.\n",
        "Comparison Table\n",
        "Solver\tRegularization Supported\tDataset Size\tSupports Sparse Data\tMulticlass Support\tComments\n",
        "liblinear\tL1, L2\tSmall to Medium\tYes\tOne-vs-Rest Only\tGreat for L1 and small datasets\n",
        "lbfgs\tL2\tMedium to Large\tNo\tMultinomial and One-vs-Rest\tDefault for most problems\n",
        "saga\tL1, L2, Elastic Net\tVery Large\tYes\tMultinomial and One-vs-Rest\tBest for very large datasets and Elastic Net\n",
        "newton-cg\tL2\tLarge\tNo\tMultinomial and One-vs-Rest\tPrecise but computationally expensive\n",
        "sag\tL2\tLarge\tNo\tOne-vs-Rest Only\tFast for dense datasets with normalized features\n",
        "Which Solver Should Be Used?\n",
        "The choice of solver depends on:\n",
        "\n",
        "Dataset Size:\n",
        "\n",
        "Small to medium datasets: liblinear.\n",
        "Large datasets: lbfgs, newton-cg, saga, or sag.\n",
        "Regularization Type:\n",
        "\n",
        "L1 Regularization: Use liblinear or saga.\n",
        "L2 Regularization: Use any solver.\n",
        "Elastic Net Regularization: Use saga.\n",
        "Multiclass Classification:\n",
        "\n",
        "Multinomial Logistic Regression: Use lbfgs, saga, or newton-cg.\n",
        "One-vs-Rest: Any solver works.\n",
        "Data Sparsity:\n",
        "\n",
        "Sparse Data: Use liblinear or saga.\n",
        "Speed and Convergence:\n",
        "\n",
        "Fast and efficient: lbfgs or sag.\n",
        "More precise but slower: newton-cg.\n",
        "\n",
        "\n",
        "Logistic Regression solvers are optimization algorithms used to minimize the cost function (log-loss) during training. The choice of solver affects the speed, convergence, and ability to handle specific scenarios like large datasets or particular types of regularization.\n",
        "\n",
        "Different Solvers in Logistic Regression\n",
        "liblinear\n",
        "\n",
        "Based on Coordinate Descent.\n",
        "Suitable for small to medium-sized datasets.\n",
        "Supports both L1 (Lasso) and L2 (Ridge) regularization.\n",
        "Works well for sparse datasets.\n",
        "Does not support multiclass classification with multi_class='multinomial' (only supports one-vs-rest).\n",
        "Use When:\n",
        "\n",
        "Dataset is small or sparse.\n",
        "L1 regularization (Lasso) is required.\n",
        "lbfgs (Limited-memory Broyden–Fletcher–Goldfarb–Shanno)\n",
        "\n",
        "An optimization algorithm from the Quasi-Newton family.\n",
        "Suitable for large datasets.\n",
        "Supports only L2 regularization.\n",
        "Efficient and robust for most cases.\n",
        "Supports multiclass classification with multi_class='multinomial'.\n",
        "Use When:\n",
        "\n",
        "Dataset is large or dense.\n",
        "Multiclass classification is required.\n",
        "saga\n",
        "\n",
        "A variant of Stochastic Gradient Descent (SGD).\n",
        "Suitable for very large datasets (especially sparse datasets).\n",
        "Supports both L1 and L2 regularization and Elastic Net regularization.\n",
        "Handles multiclass classification with multi_class='multinomial'.\n",
        "Use When:\n",
        "\n",
        "Dataset is very large or sparse.\n",
        "Elastic Net regularization is required.\n",
        "newton-cg (Newton-Conjugate Gradient)\n",
        "\n",
        "A second-order optimization algorithm using the Newton-Raphson method.\n",
        "Supports only L2 regularization.\n",
        "More computationally expensive than lbfgs but can handle large datasets.\n",
        "Suitable for multiclass classification with multi_class='multinomial'.\n",
        "Use When:\n",
        "\n",
        "Dataset is large, and convergence needs to be precise.\n",
        "sag (Stochastic Average Gradient Descent)\n",
        "\n",
        "A faster version of Gradient Descent.\n",
        "Suitable for large datasets (especially dense datasets).\n",
        "Supports only L2 regularization.\n",
        "Requires normalized features for good performance.\n",
        "Use When:\n",
        "\n",
        "Dataset is large and dense.\n",
        "Only L2 regularization is needed.\n",
        "Comparison Table\n",
        "Solver\tRegularization Supported\tDataset Size\tSupports Sparse Data\tMulticlass Support\tComments\n",
        "liblinear\tL1, L2\tSmall to Medium\tYes\tOne-vs-Rest Only\tGreat for L1 and small datasets\n",
        "lbfgs\tL2\tMedium to Large\tNo\tMultinomial and One-vs-Rest\tDefault for most problems\n",
        "saga\tL1, L2, Elastic Net\tVery Large\tYes\tMultinomial and One-vs-Rest\tBest for very large datasets and Elastic Net\n",
        "newton-cg\tL2\tLarge\tNo\tMultinomial and One-vs-Rest\tPrecise but computationally expensive\n",
        "sag\tL2\tLarge\tNo\tOne-vs-Rest Only\tFast for dense datasets with normalized features\n",
        "Which Solver Should Be Used?\n",
        "The choice of solver depends on:\n",
        "\n",
        "Dataset Size:\n",
        "\n",
        "Small to medium datasets: liblinear.\n",
        "Large datasets: lbfgs, newton-cg, saga, or sag.\n",
        "Regularization Type:\n",
        "\n",
        "L1 Regularization: Use liblinear or saga.\n",
        "L2 Regularization: Use any solver.\n",
        "Elastic Net Regularization: Use saga.\n",
        "Multiclass Classification:\n",
        "\n",
        "Multinomial Logistic Regression: Use lbfgs, saga, or newton-cg.\n",
        "One-vs-Rest: Any solver works.\n",
        "Data Sparsity:\n",
        "\n",
        "Sparse Data: Use liblinear or saga.\n",
        "Speed and Convergence:\n",
        "\n",
        "Fast and efficient: lbfgs or sag.\n",
        "More precise but slower: newton-cg.\n",
        "\n",
        "15. How is Logistic Regression extended for multiclass classification.\n",
        "Logistic Regression is inherently a binary classification algorithm. However, it can be extended to handle multiclass classification problems using two main strategies: One-vs-Rest (OvR) and Multinomial Logistic Regression (Softmax Regression). Here’s how each approach works:\n",
        "\n",
        "1. One-vs-Rest (OvR) Approach\n",
        "Also known as One-vs-All (OvA).\n",
        "This strategy involves training one binary Logistic Regression model for each class.\n",
        "Each model predicts whether an instance belongs to a specific class or not.\n",
        "The class with the highest probability among all the binary classifiers is chosen as the predicted class.\n",
        "Steps:\n",
        "For a dataset with\n",
        "𝐾\n",
        "K classes, train\n",
        "𝐾\n",
        "K separate binary Logistic Regression models:\n",
        "Model 1: Class 1 vs. All other classes.\n",
        "Model 2: Class 2 vs. All other classes.\n",
        "...\n",
        "Model\n",
        "𝐾\n",
        "K: Class\n",
        "𝐾\n",
        "K vs. All other classes.\n",
        "During prediction, calculate the probabilities for all\n",
        "𝐾\n",
        "K models.\n",
        "Assign the instance to the class with the highest predicted probability.\n",
        "Key Features:\n",
        "Works with any binary classifier (not just Logistic Regression).\n",
        "Computationally simpler for small datasets.\n",
        "Advantages:\n",
        "Easy to implement and interpret.\n",
        "Works well for datasets where classes are not strongly correlated.\n",
        "Disadvantages:\n",
        "May not perform well if the classes are highly imbalanced or correlated.\n",
        "Requires\n",
        "𝐾\n",
        "K separate models, increasing computational cost.\n",
        "2. Multinomial Logistic Regression (Softmax Regression)\n",
        "Directly extends Logistic Regression to handle multiple classes in a single model.\n",
        "Instead of learning\n",
        "𝐾\n",
        "K binary classifiers, it learns\n",
        "𝐾\n",
        "K sets of parameters in one model.\n",
        "Uses the Softmax function to calculate probabilities for each class.\n",
        "Mathematics:\n",
        "For\n",
        "𝐾\n",
        "K classes and feature vector\n",
        "𝑥\n",
        "x:\n",
        "\n",
        "The model predicts the probability of class\n",
        "𝑘\n",
        "k as:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑘\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "𝜃\n",
        "𝑘\n",
        "𝑇\n",
        "𝑥\n",
        ")\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝐾\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "𝜃\n",
        "𝑗\n",
        "𝑇\n",
        "𝑥\n",
        ")\n",
        "P(y=k∣x)=\n",
        "∑\n",
        "j=1\n",
        "K\n",
        "​\n",
        " exp(θ\n",
        "j\n",
        "T\n",
        "​\n",
        " x)\n",
        "exp(θ\n",
        "k\n",
        "T\n",
        "​\n",
        " x)\n",
        "​\n",
        "\n",
        "Where:\n",
        "\n",
        "𝜃\n",
        "𝑘\n",
        "θ\n",
        "k\n",
        "​\n",
        "  are the coefficients for class\n",
        "𝑘\n",
        "k.\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "⋅\n",
        ")\n",
        "exp(⋅) is the exponential function.\n",
        "The predicted class is the one with the highest probability:\n",
        "\n",
        "𝑦\n",
        "^\n",
        "=\n",
        "arg\n",
        "⁡\n",
        "max\n",
        "⁡\n",
        "𝑘\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑘\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "y\n",
        "^\n",
        "​\n",
        " =arg\n",
        "k\n",
        "max\n",
        "​\n",
        " P(y=k∣x)\n",
        "Key Features:\n",
        "A single cost function is minimized for all classes:\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "𝑘\n",
        "=\n",
        "1\n",
        "𝐾\n",
        "𝑦\n",
        "𝑖\n",
        ",\n",
        "𝑘\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑘\n",
        "∣\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        ")\n",
        "J(θ)=−\n",
        "N\n",
        "1\n",
        "​\n",
        "\n",
        "i=1\n",
        "∑\n",
        "N\n",
        "​\n",
        "\n",
        "k=1\n",
        "∑\n",
        "K\n",
        "​\n",
        " y\n",
        "i,k\n",
        "​\n",
        " log(P(y=k∣x\n",
        "i\n",
        "​\n",
        " ))\n",
        "𝑦\n",
        "𝑖\n",
        ",\n",
        "𝑘\n",
        "y\n",
        "i,k\n",
        "​\n",
        "  is a binary indicator (1 if class\n",
        "𝑘\n",
        "k, else 0).\n",
        "Advantages:\n",
        "All classes are considered simultaneously, leading to a more cohesive model.\n",
        "More robust when classes are highly correlated.\n",
        "Disadvantages:\n",
        "Computationally expensive for very large datasets with many classes.\n",
        "Requires a solver that supports multinomial loss (e.g., lbfgs, saga, or newton-cg).\n",
        "3. Comparison of OvR and Multinomial Logistic Regression\n",
        "Feature\tOne-vs-Rest (OvR)\tMultinomial Logistic Regression\n",
        "Training Approach\tTrains\n",
        "𝐾\n",
        "K binary classifiers\tTrains a single model for all classes\n",
        "Computational Cost\tHigher (separate models per class)\tLower (single model)\n",
        "Performance\tMay perform worse for correlated classes\tConsiders correlations among classes\n",
        "Interpretability\tEasier (each binary model is interpretable)\tHarder (more complex probability distribution)\n",
        "Implementation\tSimple\tRequires solvers supporting multinomial loss\n",
        "4. Multiclass Logistic Regression in Python\n",
        "Using One-vs-Rest:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train Logistic Regression with One-vs-Rest (default)\n",
        "ovr_model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "ovr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = ovr_model.predict(X_test)\n",
        "Using Multinomial Logistic Regression:\n",
        "\n",
        "python\n",
        "Edit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train Logistic Regression with Multinomial Strategy\n",
        "multinomial_model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "multinomial_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = multinomial_model.predict(X_test)\n",
        "5. Which Approach Should You Use?\n",
        "One-vs-Rest (OvR):\n",
        "\n",
        "Suitable for small datasets or when you need interpretability for individual classes.\n",
        "Can be used with any solver, including liblinear, which only supports binary classification.\n",
        "Multinomial Logistic Regression:\n",
        "\n",
        "Preferred for large datasets or when class relationships are important.\n",
        "Use solvers like lbfgs, saga, or newton-cg.\n",
        "\n",
        "Advantages and Disadvantages of Logistic Regression\n",
        "Logistic Regression is one of the most widely used machine learning algorithms for binary and multiclass classification problems. While it has several benefits, it also comes with limitations. Here's an overview:\n",
        "\n",
        "Advantages of Logistic Regression\n",
        "Simple to Implement and Interpret\n",
        "\n",
        "Logistic Regression is easy to implement and interpret due to its simple probabilistic foundation.\n",
        "The coefficients provide insights into feature importance and direction (positive or negative impact on the outcome).\n",
        "Probabilistic Outputs\n",
        "\n",
        "Outputs probabilities, which can be useful for decision-making and threshold adjustments.\n",
        "Works Well with Linearly Separable Data\n",
        "\n",
        "Logistic Regression performs well when the relationship between features and the target variable is linear.\n",
        "Efficient and Fast\n",
        "\n",
        "Training is computationally efficient, especially for small to medium-sized datasets.\n",
        "Handles Multiclass Classification\n",
        "\n",
        "Can be extended to multiclass classification problems using strategies like One-vs-Rest or Softmax Regression.\n",
        "Regularization Support\n",
        "\n",
        "Logistic Regression supports L1 (Lasso), L2 (Ridge), and Elastic Net regularization, which helps in handling multicollinearity and overfitting.\n",
        "Less Prone to Overfitting\n",
        "\n",
        "With proper regularization, Logistic Regression is less prone to overfitting compared to more complex models.\n",
        "Robust to Small Noise\n",
        "\n",
        "Performs well even when the dataset contains small amounts of noise or irrelevant features.\n",
        "Feature Engineering\n",
        "\n",
        "Works effectively with manually engineered features, making it suitable for scenarios where domain knowledge plays a significant role.\n",
        "Disadvantages of Logistic Regression\n",
        "Assumes Linear Relationship\n",
        "\n",
        "Logistic Regression assumes a linear relationship between the independent variables and the log-odds of the target variable. It may fail when the data is not linearly separable.\n",
        "Limited to Linearity\n",
        "\n",
        "It cannot capture complex relationships in the data unless feature transformations or interaction terms are introduced.\n",
        "Requires Well-Scaled Features\n",
        "\n",
        "Logistic Regression is sensitive to feature scaling, so normalization or standardization is often required for optimal performance.\n",
        "Poor Performance with Nonlinear Data\n",
        "\n",
        "Performs poorly on datasets with nonlinear decision boundaries unless the data is transformed (e.g., using polynomial features).\n",
        "Sensitive to Outliers\n",
        "\n",
        "Logistic Regression can be influenced by outliers, which may distort the results.\n",
        "Class Imbalance Issues\n",
        "\n",
        "Struggles with highly imbalanced datasets where the majority class dominates. Probabilities and decision boundaries may become biased.\n",
        "Feature Independence Assumption\n",
        "\n",
        "Assumes that the independent variables are not highly correlated (no multicollinearity). This can lead to misleading coefficients in the presence of multicollinearity.\n",
        "Limited Interpretability in Multiclass Problems\n",
        "\n",
        "While straightforward for binary classification, interpreting the coefficients in multiclass classification (e.g., using the multinomial approach) can be more complex.\n",
        "Overfitting with High-Dimensional Data\n",
        "\n",
        "Logistic Regression can overfit if there are too many features compared to the number of observations, particularly without proper regularization.\n",
        "Not Suitable for Complex Problems\n",
        "\n",
        "Logistic Regression is not ideal for tasks requiring non-linear relationships or complex decision boundaries, where models like Decision Trees, Random Forests, or Neural Networks are better suited.\n",
        "\n",
        "\n",
        "1. Healthcare\n",
        "Disease Diagnosis: Predicting whether a patient has a particular disease (e.g., diabetes, heart disease) based on features like age, BMI, blood pressure, and medical history.\n",
        "Medical Treatment Outcomes: Estimating the probability of a patient responding to a specific treatment.\n",
        "Readmission Prediction: Determining the likelihood of patients being readmitted to the hospital within a certain time frame.\n",
        "2. Finance\n",
        "Credit Risk Assessment: Predicting whether a customer will default on a loan based on features like credit history, income, and debt-to-income ratio.\n",
        "Fraud Detection: Identifying fraudulent transactions by analyzing transaction patterns and behavior.\n",
        "Customer Retention: Estimating the probability of a customer churning or leaving a financial service.\n",
        "3. Marketing and Sales\n",
        "Customer Segmentation: Categorizing customers into groups (e.g., likely to buy vs. not likely to buy) based on their browsing or purchasing behavior.\n",
        "Email Campaign Success: Predicting whether a customer will open or click on an email campaign.\n",
        "Lead Scoring: Determining the likelihood of a lead converting into a customer.\n",
        "4. Human Resources\n",
        "Employee Retention: Predicting whether an employee is likely to leave the company based on features like salary, job satisfaction, and work experience.\n",
        "Hiring Decisions: Screening candidates for a job role based on their qualifications and past performance.\n",
        "5. E-Commerce\n",
        "Purchase Prediction: Estimating whether a user will complete a purchase after adding items to their cart.\n",
        "Recommendation Systems: Predicting whether a user will engage with a recommended product.\n",
        "Customer Feedback Sentiment Analysis: Classifying feedback as positive or negative.\n",
        "6. Social Media\n",
        "Spam Detection: Identifying whether a message or post is spam or not.\n",
        "User Engagement Prediction: Predicting whether users will like, comment, or share a post.\n",
        "Fake News Detection: Classifying news articles as fake or real based on features like content, source, and metadata.\n",
        "7. Retail\n",
        "Inventory Management: Predicting whether a product will sell out within a specific time frame.\n",
        "Discount Effectiveness: Estimating the likelihood of increased sales after applying discounts or promotions.\n",
        "Store Performance: Identifying high-performing stores based on historical sales data.\n",
        "8. Education\n",
        "Student Performance Prediction: Estimating whether a student will pass or fail a course based on attendance, study habits, and test scores.\n",
        "Enrollment Prediction: Predicting whether a student will enroll in a particular program or course.\n",
        "Dropout Risk Analysis: Identifying students at risk of dropping out based on academic performance and engagement.\n",
        "9. Transportation\n",
        "Accident Prediction: Estimating the likelihood of accidents occurring based on factors like weather, traffic conditions, and road features.\n",
        "Rideshare Demand Prediction: Predicting whether a user will book a ride during specific times or conditions.\n",
        "On-Time Arrival: Determining whether a flight or delivery will arrive on time.\n",
        "10. Insurance\n",
        "Claims Prediction: Estimating whether a customer is likely to file an insurance claim based on demographic and policy data.\n",
        "Fraudulent Claims Detection: Identifying whether an insurance claim is legitimate or fraudulent.\n",
        "Policy Renewal Likelihood: Predicting whether customers will renew their insurance policies.\n",
        "11. Government and Policy\n",
        "Crime Prediction: Estimating the likelihood of criminal activities in certain areas based on historical data.\n",
        "Election Outcomes: Predicting the probability of a candidate winning based on polling data.\n",
        "Public Health Campaign Effectiveness: Estimating the likelihood of people adopting behaviors promoted by health campaigns (e.g., vaccination uptake).\n",
        "12. Energy and Utilities\n",
        "Customer Demand Prediction: Estimating whether customers will exceed a certain energy consumption threshold.\n",
        "Renewable Energy Adoption: Predicting the likelihood of customers switching to renewable energy sources.\n",
        "Equipment Failure: Predicting whether a piece of equipment will fail based on usage and environmental conditions.\n",
        "13. Legal and Compliance\n",
        "Legal Case Outcomes: Predicting the likelihood of a favorable or unfavorable outcome in legal cases.\n",
        "Regulatory Compliance: Identifying entities at risk of non-compliance with laws and regulations.\n",
        "14. Technology\n",
        "Churn Prediction in Software Products: Estimating whether a user will stop using a product or service.\n",
        "Bug Detection: Classifying whether a reported issue in software is likely to be a bug.\n",
        "Spam Filter in Emails: Predicting whether an incoming email is spam or legitimate.\n",
        "\n",
        "The main difference between Softmax Regression and Logistic Regression lies in the type of problems they are designed to solve and the methods they use for classification. Here's a detailed comparison:\n",
        "\n",
        "1. Definition\n",
        "Logistic Regression:\n",
        "\n",
        "A binary classification algorithm that predicts the probability of a data point belonging to one of two classes.\n",
        "Outputs probabilities using the sigmoid function, which maps predictions to the range [0, 1].\n",
        "Softmax Regression:\n",
        "\n",
        "A multiclass classification algorithm that extends Logistic Regression to handle multiple classes.\n",
        "Outputs a probability distribution over all possible classes using the softmax function.\n",
        "2. Number of Classes\n",
        "Logistic Regression:\n",
        "\n",
        "Suitable for binary classification tasks (e.g., predicting whether an email is spam or not).\n",
        "Outputs a single probability\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "P(y=1∣x), with\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "0\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "−\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "P(y=0∣x)=1−P(y=1∣x).\n",
        "Softmax Regression:\n",
        "\n",
        "Designed for multiclass classification tasks (e.g., classifying handwritten digits into 0–9).\n",
        "Outputs probabilities for all\n",
        "𝐾\n",
        "K classes, ensuring they sum to 1:\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑘\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "𝜃\n",
        "𝑘\n",
        "𝑇\n",
        "𝑥\n",
        ")\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝐾\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "𝜃\n",
        "𝑗\n",
        "𝑇\n",
        "𝑥\n",
        ")\n",
        "P(y=k∣x)=\n",
        "∑\n",
        "j=1\n",
        "K\n",
        "​\n",
        " exp(θ\n",
        "j\n",
        "T\n",
        "​\n",
        " x)\n",
        "exp(θ\n",
        "k\n",
        "T\n",
        "​\n",
        " x)\n",
        "​\n",
        "\n",
        "3. Mathematical Formulation\n",
        "Logistic Regression:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "𝜎\n",
        "(\n",
        "𝜃\n",
        "𝑇\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "𝜃\n",
        "𝑇\n",
        "𝑥\n",
        ")\n",
        "P(y=1∣x)=σ(θ\n",
        "T\n",
        " x)=\n",
        "1+exp(−θ\n",
        "T\n",
        " x)\n",
        "1\n",
        "​\n",
        "\n",
        "Where\n",
        "𝜎\n",
        "σ is the sigmoid function.\n",
        "\n",
        "Softmax Regression: For\n",
        "𝐾\n",
        "K classes:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑘\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "𝜃\n",
        "𝑘\n",
        "𝑇\n",
        "𝑥\n",
        ")\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝐾\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "𝜃\n",
        "𝑗\n",
        "𝑇\n",
        "𝑥\n",
        ")\n",
        "for\n",
        "𝑘\n",
        "=\n",
        "1\n",
        ",\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝐾\n",
        "P(y=k∣x)=\n",
        "∑\n",
        "j=1\n",
        "K\n",
        "​\n",
        " exp(θ\n",
        "j\n",
        "T\n",
        "​\n",
        " x)\n",
        "exp(θ\n",
        "k\n",
        "T\n",
        "​\n",
        " x)\n",
        "​\n",
        " for k=1,2,…,K\n",
        "4. Activation Function\n",
        "Logistic Regression:\n",
        "\n",
        "Uses the sigmoid function to transform predictions into probabilities:\n",
        "𝜎\n",
        "(\n",
        "𝑧\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "𝑧\n",
        ")\n",
        "σ(z)=\n",
        "1+exp(−z)\n",
        "1\n",
        "​\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "Uses the softmax function, which generalizes the sigmoid function for multiple classes:\n",
        "softmax\n",
        "(\n",
        "𝑧\n",
        "𝑖\n",
        ")\n",
        "=\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "𝑧\n",
        "𝑖\n",
        ")\n",
        "∑\n",
        "𝑗\n",
        "=\n",
        "1\n",
        "𝐾\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "𝑧\n",
        "𝑗\n",
        ")\n",
        "softmax(z\n",
        "i\n",
        "​\n",
        " )=\n",
        "∑\n",
        "j=1\n",
        "K\n",
        "​\n",
        " exp(z\n",
        "j\n",
        "​\n",
        " )\n",
        "exp(z\n",
        "i\n",
        "​\n",
        " )\n",
        "​\n",
        "\n",
        "5. Cost Function\n",
        "Logistic Regression:\n",
        "\n",
        "Uses the binary cross-entropy loss:\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "[\n",
        "𝑦\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "+\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "𝑖\n",
        ")\n",
        "log\n",
        "⁡\n",
        "(\n",
        "1\n",
        "−\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ")\n",
        "]\n",
        "J(θ)=−\n",
        "N\n",
        "1\n",
        "​\n",
        "\n",
        "i=1\n",
        "∑\n",
        "N\n",
        "​\n",
        " [y\n",
        "i\n",
        "​\n",
        " log(\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "i\n",
        "​\n",
        " )+(1−y\n",
        "i\n",
        "​\n",
        " )log(1−\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "i\n",
        "​\n",
        " )]\n",
        "Softmax Regression:\n",
        "\n",
        "Uses the categorical cross-entropy loss:\n",
        "𝐽\n",
        "(\n",
        "𝜃\n",
        ")\n",
        "=\n",
        "−\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑁\n",
        "∑\n",
        "𝑘\n",
        "=\n",
        "1\n",
        "𝐾\n",
        "𝑦\n",
        "𝑖\n",
        ",\n",
        "𝑘\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ",\n",
        "𝑘\n",
        ")\n",
        "J(θ)=−\n",
        "N\n",
        "1\n",
        "​\n",
        "\n",
        "i=1\n",
        "∑\n",
        "N\n",
        "​\n",
        "\n",
        "k=1\n",
        "∑\n",
        "K\n",
        "​\n",
        " y\n",
        "i,k\n",
        "​\n",
        " log(\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "i,k\n",
        "​\n",
        " )\n",
        "Where\n",
        "𝑦\n",
        "𝑖\n",
        ",\n",
        "𝑘\n",
        "y\n",
        "i,k\n",
        "​\n",
        "  is a binary indicator for the true class\n",
        "𝑘\n",
        "k and\n",
        "𝑦\n",
        "^\n",
        "𝑖\n",
        ",\n",
        "𝑘\n",
        "y\n",
        "^\n",
        "​\n",
        "\n",
        "i,k\n",
        "​\n",
        "  is the predicted probability for class\n",
        "𝑘\n",
        "k.\n",
        "6. Output\n",
        "Logistic Regression:\n",
        "\n",
        "Outputs a single probability for one class (e.g.,\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "P(y=1∣x)).\n",
        "Decision is made by applying a threshold (e.g., 0.5):\n",
        "𝑦\n",
        "^\n",
        "=\n",
        "{\n",
        "1\n",
        ",\n",
        "if\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        ">\n",
        "0.5\n",
        "0\n",
        ",\n",
        "otherwise\n",
        "y\n",
        "^\n",
        "​\n",
        " ={\n",
        "1,\n",
        "0,\n",
        "​\n",
        "\n",
        "if P(y=1∣x)>0.5\n",
        "otherwise\n",
        "​\n",
        "\n",
        "Softmax Regression:\n",
        "\n",
        "Outputs a vector of probabilities for all classes, such as:\n",
        "𝑦\n",
        "^\n",
        "=\n",
        "[\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        ",\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "2\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝐾\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "]\n",
        "y\n",
        "^\n",
        "​\n",
        " =[P(y=1∣x),P(y=2∣x),…,P(y=K∣x)]\n",
        "The predicted class is the one with the highest probability:\n",
        "𝑦\n",
        "^\n",
        "=\n",
        "arg\n",
        "⁡\n",
        "max\n",
        "⁡\n",
        "𝑘\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "𝑘\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "y\n",
        "^\n",
        "​\n",
        " =arg\n",
        "k\n",
        "max\n",
        "​\n",
        " P(y=k∣x)\n",
        "7. Applications\n",
        "Logistic Regression:\n",
        "\n",
        "Binary classification tasks, such as:\n",
        "Spam detection.\n",
        "Disease diagnosis (e.g., diabetes: yes/no).\n",
        "Customer churn prediction (will churn: yes/no).\n",
        "Softmax Regression:\n",
        "\n",
        "Multiclass classification tasks, such as:\n",
        "Image classification (e.g., handwritten digit classification).\n",
        "Sentiment analysis with multiple categories (positive, neutral, negative).\n",
        "Document classification (e.g., topic categorization).\n",
        "8. Example in Python\n",
        "Logistic Regression (Binary):\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Binary classification\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)  # y_train has 2 classes (0, 1)\n",
        "y_pred = model.predict(X_test)\n",
        "Softmax Regression (Multiclass):\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Multiclass classification\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "model.fit(X_train, y_train)  # y_train has multiple classes (e.g., 0, 1, 2)\n",
        "y_pred = model.predict(X_test)\n",
        "9. Summary Table\n",
        "Feature\tLogistic Regression\tSoftmax Regression\n",
        "Type of Problem\tBinary Classification\tMulticlass Classification\n",
        "Activation Function\tSigmoid\tSoftmax\n",
        "Cost Function\tBinary Cross-Entropy\tCategorical Cross-Entropy\n",
        "Output\tSingle probability for one class\tProbability distribution over all classes\n",
        "Predicted Class\tThreshold-based (e.g., 0.5)\tClass with highest probability\n",
        "Applications\tBinary tasks (spam detection, churn prediction)\tMulticlass tasks (image or document classification)\n",
        "\n",
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification.\n",
        "Choosing between One-vs-Rest (OvR) and Softmax (Multinomial) for multiclass classification depends on various factors like dataset size, complexity, interpretability, and computational requirements. Here's a detailed comparison to help make the decision:\n",
        "\n",
        "1. Definitions\n",
        "One-vs-Rest (OvR):\n",
        "\n",
        "Splits the multiclass problem into multiple binary classification problems.\n",
        "For\n",
        "𝐾\n",
        "K classes,\n",
        "𝐾\n",
        "K binary classifiers are trained, one for each class vs. the rest of the classes.\n",
        "The predicted class is the one with the highest confidence score among all classifiers.\n",
        "Softmax (Multinomial):\n",
        "\n",
        "Directly solves the multiclass classification problem by modeling the probability distribution across all classes.\n",
        "Uses the softmax function to compute probabilities for each class and predicts the class with the highest probability.\n",
        "2. Key Differences\n",
        "Feature\tOne-vs-Rest (OvR)\tSoftmax (Multinomial)\n",
        "Approach\tMultiple binary classifiers\tSingle multiclass classifier\n",
        "Complexity\tSimpler, as it trains independent models\tMore complex, models all classes together\n",
        "Computation\n",
        "𝐾\n",
        "K binary models trained independently\tSingle model optimizing for all\n",
        "𝐾\n",
        "K classes\n",
        "Output\tIndividual binary probabilities\tProbabilities for all classes (sum to 1)\n",
        "Interpretability\tEasier to interpret per-class results\tHarder to interpret due to joint optimization\n",
        "Performance\tMay perform better for imbalanced classes\tPerforms better when class relationships matter\n",
        "Scalability\tScales well for large datasets and\n",
        "𝐾\n",
        "K\tComputationally expensive for large\n",
        "𝐾\n",
        "K\n",
        "Dependencies\tBinary models are independent\tClasses are interdependent (shared optimization)\n",
        "3. When to Choose One-vs-Rest (OvR)\n",
        "Imbalanced Data:\n",
        "\n",
        "If you have imbalanced class distributions, OvR can handle this better since each binary classifier focuses on distinguishing one class from the rest.\n",
        "Scalability:\n",
        "\n",
        "When there are many classes (\n",
        "𝐾\n",
        "K) or a very large dataset, OvR is computationally more efficient as it trains independent binary models.\n",
        "Interpretability:\n",
        "\n",
        "OvR is easier to interpret because each binary classifier is independent, and you can analyze each class individually.\n",
        "Simple Class Relationships:\n",
        "\n",
        "If the relationships between classes are weak or negligible, OvR may be a better choice.\n",
        "4. When to Choose Softmax (Multinomial)\n",
        "Class Relationships:\n",
        "\n",
        "When the classes are related or mutually exclusive (e.g., predicting digits 0–9), Softmax performs better because it models the joint probabilities of all classes.\n",
        "Global Optimization:\n",
        "\n",
        "If you want a single model that optimizes over all classes simultaneously, Softmax is preferable as it ensures probabilities sum to 1.\n",
        "Small to Medium Class Count:\n",
        "\n",
        "For datasets with a manageable number of classes (\n",
        "𝐾\n",
        "K), Softmax is computationally feasible and often yields better results.\n",
        "Modern Applications:\n",
        "\n",
        "Softmax is the standard choice in neural networks and deep learning applications for multiclass classification problems.\n",
        "5. Advantages and Disadvantages\n",
        "One-vs-Rest (OvR)\n",
        "Advantages:\n",
        "\n",
        "Simple to implement and train.\n",
        "Scalable to a large number of classes or datasets.\n",
        "Robust to class imbalance in individual binary classifiers.\n",
        "Disadvantages:\n",
        "\n",
        "Each binary model is trained independently, which may lead to inconsistencies.\n",
        "Does not leverage the relationships between classes.\n",
        "Can result in ambiguous predictions when two or more classifiers predict high probabilities for different classes.\n",
        "Softmax (Multinomial)\n",
        "Advantages:\n",
        "\n",
        "Models interdependencies and relationships between classes.\n",
        "Provides a globally optimal solution with probabilities summing to 1.\n",
        "Performs better for mutually exclusive and balanced classes.\n",
        "Disadvantages:\n",
        "\n",
        "Computationally expensive for a large number of classes or datasets.\n",
        "Requires careful tuning of hyperparameters to avoid overfitting.\n",
        "Sensitive to class imbalance (may require techniques like weighted loss).\n",
        "6. Practical Considerations\n",
        "Dataset Size and Complexity:\n",
        "\n",
        "For small to medium datasets with simple relationships: OvR.\n",
        "For large or complex datasets where class relationships matter: Softmax.\n",
        "Number of Classes:\n",
        "\n",
        "Few classes: Either OvR or Softmax works.\n",
        "Many classes: OvR is preferred due to computational efficiency.\n",
        "Performance:\n",
        "\n",
        "Test both approaches using cross-validation to compare performance metrics (e.g., accuracy, F1-score).\n",
        "Tools and Libraries:\n",
        "\n",
        "Scikit-learn's LogisticRegression supports both OvR (default) and Softmax (set multi_class='multinomial').\n",
        "\n",
        "Interpreting the coefficients in Logistic Regression requires understanding their relationship with the odds of the outcome, as Logistic Regression models the log-odds rather than the direct probability. Here's how it works:\n",
        "\n",
        "1. Logistic Regression Equation\n",
        "The predicted probability for a binary outcome\n",
        "𝑦\n",
        "y (0 or 1) is given by:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "𝑧\n",
        ")\n",
        ",\n",
        "where\n",
        "𝑧\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        ".\n",
        "P(y=1∣x)=\n",
        "1+exp(−z)\n",
        "1\n",
        "​\n",
        " ,where z=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " .\n",
        "𝛽\n",
        "0\n",
        "β\n",
        "0\n",
        "​\n",
        " : The intercept term.\n",
        "𝛽\n",
        "1\n",
        ",\n",
        "𝛽\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝛽\n",
        "𝑛\n",
        "β\n",
        "1\n",
        "​\n",
        " ,β\n",
        "2\n",
        "​\n",
        " ,…,β\n",
        "n\n",
        "​\n",
        " : The coefficients for the corresponding features\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        "x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,…,x\n",
        "n\n",
        "​\n",
        " .\n",
        "2. Coefficients Represent Log-Odds\n",
        "Logistic Regression predicts the log-odds of the outcome:\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        ")\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "0\n",
        ")\n",
        ")\n",
        "=\n",
        "𝑧\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        ".\n",
        "log(\n",
        "P(y=0)\n",
        "P(y=1)\n",
        "​\n",
        " )=z=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " .\n",
        "Each coefficient (\n",
        "𝛽\n",
        "𝑖\n",
        "β\n",
        "i\n",
        "​\n",
        " ) represents the change in the log-odds of the outcome for a one-unit increase in the corresponding feature\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        " , holding all other features constant.\n",
        "3. Converting Coefficients to Odds\n",
        "To interpret the coefficients in terms of odds rather than log-odds, we exponentiate them:\n",
        "\n",
        "Odds Ratio\n",
        "=\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "𝛽\n",
        "𝑖\n",
        ")\n",
        "Odds Ratio=exp(β\n",
        "i\n",
        "​\n",
        " )\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "𝛽\n",
        "𝑖\n",
        ")\n",
        ">\n",
        "1\n",
        "exp(β\n",
        "i\n",
        "​\n",
        " )>1: A one-unit increase in\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  increases the odds of\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1.\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "𝛽\n",
        "𝑖\n",
        ")\n",
        "<\n",
        "1\n",
        "exp(β\n",
        "i\n",
        "​\n",
        " )<1: A one-unit increase in\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  decreases the odds of\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1.\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "𝛽\n",
        "𝑖\n",
        ")\n",
        "=\n",
        "1\n",
        "exp(β\n",
        "i\n",
        "​\n",
        " )=1:\n",
        "𝑥\n",
        "𝑖\n",
        "x\n",
        "i\n",
        "​\n",
        "  has no effect on the odds of\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1.\n",
        "4. Example of Coefficient Interpretation\n",
        "Suppose we have the Logistic Regression equation:\n",
        "\n",
        "log\n",
        "⁡\n",
        "(\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        ")\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "0\n",
        ")\n",
        ")\n",
        "=\n",
        "−\n",
        "1.5\n",
        "+\n",
        "0.8\n",
        "𝑥\n",
        "1\n",
        "−\n",
        "0.3\n",
        "𝑥\n",
        "2\n",
        ".\n",
        "log(\n",
        "P(y=0)\n",
        "P(y=1)\n",
        "​\n",
        " )=−1.5+0.8x\n",
        "1\n",
        "​\n",
        " −0.3x\n",
        "2\n",
        "​\n",
        " .\n",
        "Intercept (\n",
        "𝛽\n",
        "0\n",
        "=\n",
        "−\n",
        "1.5\n",
        "β\n",
        "0\n",
        "​\n",
        " =−1.5):\n",
        "\n",
        "When all features\n",
        "𝑥\n",
        "1\n",
        "x\n",
        "1\n",
        "​\n",
        "  and\n",
        "𝑥\n",
        "2\n",
        "x\n",
        "2\n",
        "​\n",
        "  are zero, the log-odds of\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1 are\n",
        "−\n",
        "1.5\n",
        "−1.5.\n",
        "Odds =\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "1.5\n",
        ")\n",
        "≈\n",
        "0.22\n",
        "exp(−1.5)≈0.22, meaning\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1 is less likely than\n",
        "𝑦\n",
        "=\n",
        "0\n",
        "y=0.\n",
        "Feature\n",
        "𝑥\n",
        "1\n",
        "x\n",
        "1\n",
        "​\n",
        "  (\n",
        "𝛽\n",
        "1\n",
        "=\n",
        "0.8\n",
        "β\n",
        "1\n",
        "​\n",
        " =0.8):\n",
        "\n",
        "A one-unit increase in\n",
        "𝑥\n",
        "1\n",
        "x\n",
        "1\n",
        "​\n",
        "  increases the log-odds of\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1 by 0.8.\n",
        "Odds Ratio =\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "0.8\n",
        ")\n",
        "≈\n",
        "2.23\n",
        "exp(0.8)≈2.23: The odds of\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1 are 2.23 times higher for each additional unit of\n",
        "𝑥\n",
        "1\n",
        "x\n",
        "1\n",
        "​\n",
        " .\n",
        "Feature\n",
        "𝑥\n",
        "2\n",
        "x\n",
        "2\n",
        "​\n",
        "  (\n",
        "𝛽\n",
        "2\n",
        "=\n",
        "−\n",
        "0.3\n",
        "β\n",
        "2\n",
        "​\n",
        " =−0.3):\n",
        "\n",
        "A one-unit increase in\n",
        "𝑥\n",
        "2\n",
        "x\n",
        "2\n",
        "​\n",
        "  decreases the log-odds of\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1 by 0.3.\n",
        "Odds Ratio =\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "0.3\n",
        ")\n",
        "≈\n",
        "0.74\n",
        "exp(−0.3)≈0.74: The odds of\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1 are reduced by 26% for each additional unit of\n",
        "𝑥\n",
        "2\n",
        "x\n",
        "2\n",
        "​\n",
        " .\n",
        "5. Interpreting Probabilities\n",
        "If you want to understand how a feature affects the probability of\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "y=1, you need to compute it explicitly:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑦\n",
        "=\n",
        "1\n",
        "∣\n",
        "𝑥\n",
        ")\n",
        "=\n",
        "1\n",
        "1\n",
        "+\n",
        "exp\n",
        "⁡\n",
        "(\n",
        "−\n",
        "(\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        ")\n",
        ")\n",
        ".\n",
        "P(y=1∣x)=\n",
        "1+exp(−(β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " ))\n",
        "1\n",
        "​\n",
        " .\n",
        "While coefficients give you insights into relative changes (log-odds or odds), the relationship between features and probabilities is nonlinear.\n",
        "\n",
        "6. Practical Considerations\n",
        "Standardized Features: If features have very different scales, standardize them to make coefficient comparisons meaningful.\n",
        "Multicollinearity: If features are highly correlated, interpretation becomes difficult because coefficients reflect combined effects.\n",
        "Significance: Check\n",
        "𝑝\n",
        "p-values or confidence intervals of coefficients to identify which features significantly influence the outcome."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Practical"
      ],
      "metadata": {
        "id": "HYe_K5jeAzvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load a dataset (for demonstration, we'll use the Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (class labels)\n",
        "\n",
        "# Convert the problem into binary classification (e.g., classify class 0 vs. others)\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Apply Logistic Regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "2.Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load a dataset (for demonstration, we'll use the Iris dataset)\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (class labels)\n",
        "\n",
        "# Convert the problem into binary classification (e.g., classify class 0 vs. others)\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Apply Logistic Regression with L1 Regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)  # L1 regularization requires solver='liblinear'\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L1 Regularization: {accuracy:.2f}\")\n",
        "\n",
        "# Step 6 (Optional): Print coefficients to see the effect of L1 regularization\n",
        "print(\"Model Coefficients (L1 Regularization):\", model.coef_)\n",
        "\n",
        "\n",
        "3.Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load a dataset (using the Iris dataset as an example)\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (class labels)\n",
        "\n",
        "# Convert the problem into binary classification (e.g., classify class 0 vs. others)\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Apply Logistic Regression with L2 Regularization\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', C=1.0)  # L2 regularization is the default penalty\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L2 Regularization: {accuracy:.2f}\")\n",
        "\n",
        "# Step 6: Print the coefficients\n",
        "print(\"Model Coefficients (L2 Regularization):\", model.coef_)\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Step 1: Load a dataset (using the Iris dataset as an example)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (class labels)\n",
        "\n",
        "# Convert the problem into binary classification (e.g., classify class 0 vs. others)\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Apply Logistic Regression with Elastic Net Regularization\n",
        "# Elastic Net requires a combination of L1 and L2 regularization (l1_ratio specifies the balance)\n",
        "model = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',  # 'saga' solver supports elastic net regularization\n",
        "    l1_ratio=0.5,  # 0.5 indicates an equal mix of L1 and L2 regularization\n",
        "    C=1.0  # Inverse of regularization strength (smaller = stronger regularization)\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with Elastic Net Regularization: {accuracy:.2f}\")\n",
        "\n",
        "# Step 6: Print the coefficients\n",
        "print(\"Model Coefficients (Elastic Net Regularization):\", model.coef_)\n",
        "\n",
        "\n",
        "5.Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Step 1: Load a dataset (using the Iris dataset as an example)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (class labels)\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Apply Logistic Regression with One-vs-Rest (OvR) for multiclass classification\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=500)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with One-vs-Rest: {accuracy:.2f}\")\n",
        "\n",
        "# Step 6 (Optional): Print the coefficients for each class\n",
        "print(\"Model Coefficients (One-vs-Rest):\")\n",
        "for i, coef in enumerate(model.coef_):\n",
        "    print(f\"Class {i} Coefficients: {coef}\")\n",
        "\n",
        "6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Step 1: Load a dataset (using the Iris dataset as an example)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (class labels)\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Define the Logistic Regression model\n",
        "model = LogisticRegression(solver='saga', max_iter=1000)  # Use 'saga' to support both L1 and L2 penalties\n",
        "\n",
        "# Step 4: Set up the hyperparameter grid for tuning\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],        # Regularization strength (smaller = stronger regularization)\n",
        "    'penalty': ['l1', 'l2']              # Regularization type\n",
        "}\n",
        "\n",
        "# Step 5: Apply GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Get the best parameters and evaluate the model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 7: Print the results\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Model Accuracy with Best Parameters: {accuracy:.2f}\")\n",
        "\n",
        "7.Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Step 1: Load a dataset (using the Iris dataset as an example)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (class labels)\n",
        "\n",
        "# Step 2: Define the Logistic Regression model\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000)\n",
        "\n",
        "# Step 3: Set up Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Step 4: Evaluate the model using cross_val_score\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Step 5: Calculate and print the average accuracy\n",
        "average_accuracy = np.mean(scores)\n",
        "print(f\"Stratified K-Fold Cross-Validation Accuracy Scores: {scores}\")\n",
        "print(f\"Average Accuracy: {average_accuracy:.2f}\")\n",
        "\n",
        "8.Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset from a CSV file\n",
        "# Replace 'dataset.csv' with the path to your CSV file\n",
        "data = pd.read_csv('dataset.csv')\n",
        "\n",
        "# Step 2: Define features (X) and target (y)\n",
        "# Replace 'target_column' with the name of the target column in your dataset\n",
        "X = data.drop(columns=['target_column'])  # Features (all columns except target)\n",
        "y = data['target_column']  # Target (class labels)\n",
        "\n",
        "# Step 3: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Apply Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)  # Increase max_iter if needed for convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "9.Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Step 1: Load a dataset (using the Iris dataset as an example)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (class labels)\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Define the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Step 4: Set up the hyperparameter distribution for tuning\n",
        "param_distributions = {\n",
        "    'C': uniform(0.01, 10),  # Randomly sample C values between 0.01 and 10\n",
        "    'penalty': ['l1', 'l2'],  # L1 and L2 regularization\n",
        "    'solver': ['liblinear', 'saga']  # Solvers compatible with L1 and L2 penalties\n",
        "}\n",
        "\n",
        "# Step 5: Apply RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=50,  # Number of random parameter combinations to try\n",
        "    scoring='accuracy',\n",
        "    cv=5,  # 5-fold cross-validation\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Get the best parameters and evaluate the model\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 7: Print the results\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Model Accuracy with Best Parameters: {accuracy:.2f}\")\n",
        "\n",
        "\n",
        "10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Step 1: Load a dataset (using the Iris dataset as an example)\n",
        "data = load_iris()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (class labels)\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Define the Logistic Regression model wrapped with One-vs-One Classifier\n",
        "model = OneVsOneClassifier(LogisticRegression(max_iter=1000, solver='lbfgs'))\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy using One-vs-One Logistic Regression: {accuracy:.2f}\")\n",
        "\n",
        "11.Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Load a dataset (using a synthetic dataset as an example)\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=1000, n_features=4, n_classes=2, random_state=42)\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Step 6: Visualize the confusion matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Print accuracy and classification report\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "12.Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Step 1: Load a dataset (using a synthetic dataset as an example)\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(n_samples=1000, n_features=4, n_classes=2, random_state=42)\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print the results\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "13.Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Step 1: Generate an imbalanced dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=4,\n",
        "    n_classes=2,\n",
        "    weights=[0.9, 0.1],  # 90% of class 0, 10% of class 1\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression without class weights\n",
        "model_no_weights = LogisticRegression(max_iter=1000)\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression with balanced class weights\n",
        "model_with_weights = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model_with_weights.fit(X_train, y_train)\n",
        "y_pred_with_weights = model_with_weights.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate both models\n",
        "print(\"Model WITHOUT Class Weights:\")\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "\n",
        "print(\"\\nModel WITH Class Weights:\")\n",
        "print(classification_report(y_test, y_pred_with_weights))\n",
        "\n",
        "# Step 6: Visualize confusion matrices\n",
        "conf_matrix_no_weights = confusion_matrix(y_test, y_pred_no_weights)\n",
        "conf_matrix_with_weights = confusion_matrix(y_test, y_pred_with_weights)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "sns.heatmap(conf_matrix_no_weights, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0])\n",
        "axes[0].set_title(\"Without Class Weights\")\n",
        "axes[0].set_xlabel(\"Predicted\")\n",
        "axes[0].set_ylabel(\"Actual\")\n",
        "\n",
        "sns.heatmap(conf_matrix_with_weights, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[1])\n",
        "axes[1].set_title(\"With Class Weights\")\n",
        "axes[1].set_xlabel(\"Predicted\")\n",
        "axes[1].set_ylabel(\"Actual\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "14.Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance.\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load the Titanic dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Preprocessing\n",
        "# Select relevant features and target variable\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "\n",
        "# Convert categorical features to numeric\n",
        "data['Sex'] = data['Sex'].map({'male': 0, 'female': 1})\n",
        "data['Embarked'] = data['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='median')  # Use median for numerical columns\n",
        "data['Age'] = imputer.fit_transform(data[['Age']])\n",
        "data['Embarked'] = imputer.fit_transform(data[['Embarked']])\n",
        "\n",
        "# Extract features and target\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Step 3: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 6: Visualize the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Not Survived', 'Survived'], yticklabels=['Not Survived', 'Survived'])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()\n",
        "\n",
        "15.Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Load a dataset (using a synthetic dataset for demonstration)\n",
        "from sklearn.datasets import make_classification\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression WITHOUT scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=1000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Step 4: Apply feature scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 5: Train Logistic Regression WITH scaling\n",
        "model_with_scaling = LogisticRegression(max_iter=1000)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "# Step 6: Print results\n",
        "print(\"Logistic Regression WITHOUT Scaling:\")\n",
        "print(f\"Accuracy: {accuracy_no_scaling:.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_no_scaling))\n",
        "\n",
        "print(\"\\nLogistic Regression WITH Scaling:\")\n",
        "print(f\"Accuracy: {accuracy_with_scaling:.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_with_scaling))\n",
        "\n",
        "16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict probabilities\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of the positive class\n",
        "\n",
        "# Step 5: Calculate the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
        "\n",
        "# Step 6: Compute ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Step 7: Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {roc_auc:.2f})\", color=\"blue\")\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"red\", label=\"Random Guess\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "17.Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train a Logistic Regression model with custom learning rate (C=0.5)\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)  # C=0.5 controls regularization\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 5: Print results\n",
        "print(f\"Model Accuracy with C=0.5: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "18.Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Generate a synthetic dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Create feature names (optional, for interpretability)\n",
        "feature_names = [f\"Feature {i+1}\" for i in range(X.shape[1])]\n",
        "\n",
        "# Step 3: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Extract feature importance (coefficients)\n",
        "coefficients = model.coef_[0]  # Coefficients for the first (and only) class\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "feature_importance['Absolute Coefficient'] = feature_importance['Coefficient'].abs()\n",
        "feature_importance = feature_importance.sort_values(by='Absolute Coefficient', ascending=False)\n",
        "\n",
        "# Step 6: Display the most important features\n",
        "print(\"Top Important Features Based on Coefficients:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Step 7: Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance['Feature'], feature_importance['Absolute Coefficient'], color='skyblue')\n",
        "plt.xlabel('Absolute Coefficient Value')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance Based on Logistic Regression Coefficients')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
        "plt.show()\n",
        "\n",
        "\n",
        "19.Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "Score.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Generate a synthetic dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print results\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Cohen's Kappa Score: {kappa:.2f}\\n\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "20.Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classificatio.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, auc, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Generate a synthetic dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict probabilities for the test set\n",
        "y_probs = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "\n",
        "# Step 5: Compute precision-recall values\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Step 6: Print performance summary\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, model.predict(X_test)))\n",
        "print(f\"Precision-Recall AUC: {pr_auc:.2f}\")\n",
        "\n",
        "# Step 7: Plot the Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f\"Precision-Recall AUC = {pr_auc:.2f}\", color=\"blue\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "21.Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Generate a synthetic dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Define solvers to test\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Step 4: Train and evaluate models with different solvers\n",
        "results = []\n",
        "for solver in solvers:\n",
        "    # Train Logistic Regression model with the current solver\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results.append((solver, accuracy))\n",
        "\n",
        "# Step 5: Display results\n",
        "print(\"Solver Comparison:\")\n",
        "for solver, accuracy in results:\n",
        "    print(f\"Solver: {solver}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "22.Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC).\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef, classification_report\n",
        "\n",
        "# Step 1: Generate a synthetic dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print results\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n",
        "\n",
        "\n",
        "23.Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Generate a synthetic dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Step 4: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 5: Train Logistic Regression on standardized data\n",
        "model_scaled = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Step 6: Print results\n",
        "print(f\"Accuracy on raw data: {accuracy_raw:.4f}\")\n",
        "print(f\"Accuracy on standardized data: {accuracy_scaled:.4f}\")\n",
        "\n",
        "23.Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Generate a synthetic dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Step 4: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 5: Train Logistic Regression on standardized data\n",
        "model_scaled = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Step 6: Print results\n",
        "print(f\"Accuracy on raw data: {accuracy_raw:.4f}\")\n",
        "print(f\"Accuracy on standardized data: {accuracy_scaled:.4f}\")\n",
        "\n",
        "24.Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Step 1: Generate a synthetic dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Define the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Step 4: Define the parameter grid for regularization strength (C)\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Step 5: Use GridSearchCV to find the optimal C using cross-validation\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Get the best parameters and evaluate the model\n",
        "best_model = grid_search.best_estimator_\n",
        "best_C = grid_search.best_params_['C']\n",
        "accuracy = best_model.score(X_test, y_test)\n",
        "\n",
        "# Step 7: Print the results\n",
        "print(f\"Optimal value of C (regularization strength): {best_C}\")\n",
        "print(f\"Accuracy of the model with optimal C: {accuracy:.4f}\")\n",
        "\n",
        "25.Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions.\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Step 1: Generate a synthetic dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Save the trained model using joblib\n",
        "model_filename = \"logistic_regression_model.joblib\"\n",
        "joblib.dump(model, model_filename)\n",
        "print(f\"Model saved to {model_filename}\")\n",
        "\n",
        "# Step 5: Load the saved model\n",
        "loaded_model = joblib.load(model_filename)\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# Step 6: Use the loaded model to make predictions\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the loaded model: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "HYK5apgaA42u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}